{"name":"深度学习","postlist":[{"title":"PyTorch实战-利用卷积神经网络完成手写数字识别","slug":"PyTorch实战-利用卷积神经网络完成手写数字识别","date":"2024-09-01T04:37:55.000Z","updated":"2024-09-01T04:57:42.611Z","comments":true,"path":"api/articles/PyTorch实战-利用卷积神经网络完成手写数字识别.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>卷积神经网络（Convolutional Neural Networks, CNNs）是一种特殊类型的神经网络，在图像和视频识别、推荐系统、图像分类、医学图像分析、自然语言处理等领域有着广泛的应用。它们能够自动从原始图像中提取特征，并通过多层网络结构学习这些特征的高级表示。本文通过手写数字识别项目带大家学习卷积神经网络。</p>\n<h1 id=\"卷积神经网络基本概念\"><a href=\"#卷积神经网络基本概念\" class=\"headerlink\" title=\"卷积神经网络基本概念\"></a>卷积神经网络基本概念</h1><h2 id=\"CNN结构\"><a href=\"#CNN结构\" class=\"headerlink\" title=\"CNN结构\"></a>CNN结构</h2><p>输入层（Input Layer）–&gt; {卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）–&gt; 卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）}(重复) –&gt; 全连接层（Fully Connected Layer）</p>\n<h2 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h2><p>在深度学习和计算机视觉中，尤其是在处理卷积神经网络（CNN）时，计算输出尺寸（Output Size）的公式非常重要。您给出的公式是卷积层输出尺寸计算的一个基本公式，但通常我们会用更直观的符号来表示它。下面是该公式转换为常用表示方法的形式：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - K}{S} \\right\\rfloor + 1 $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），通常是输出特征图的高度或宽度（假设它们是相等的，即正方形特征图）。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度或宽度。</li>\n<li>$P$ 代表填充（Padding）的大小，即在输入特征图的边界上添加的零的层数。注意，这里的 $2P$ 表示在输入特征图的两侧（或上下两侧，取决于维度）都添加了 $P$ 层的零。</li>\n<li>$K$ 代表卷积核（Kernel Size）的大小，即卷积核的高度或宽度（在正方形卷积核的情况下）。</li>\n<li>$S$ 代表步长（Stride），即卷积核在输入特征图上移动的步数。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<p>这个公式适用于计算卷积层（包括标准卷积层和转置卷积层，但转置卷积层有额外的参数和复杂性）后的输出特征图尺寸。在实际应用中，了解如何根据这些参数调整网络结构以得到期望的输出尺寸是非常重要的。</p>\n<h2 id=\"激活函数（Activation-Function）\"><a href=\"#激活函数（Activation-Function）\" class=\"headerlink\" title=\"激活函数（Activation Function）\"></a>激活函数（Activation Function）</h2><p>激活函数用于在卷积层（以及其他类型的神经网络层）之后引入非线性。常见的激活函数包括ReLU（Rectified Linear Unit，修正线性单元）、sigmoid和tanh等。ReLU因其简单性和减少梯度消失问题的能力而在CNN中广泛使用。</p>\n<h2 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h2><p>池化层（Pooling Layer）在卷积神经网络（CNN）中扮演着重要的角色，主要用于特征融合和降维，以减少计算量和控制过拟合。池化层的输入尺寸和输出尺寸计算公式可以根据不同的参数设置而有所不同，但基本思路是相似的。</p>\n<h3 id=\"池化层的输出尺寸计算公式\"><a href=\"#池化层的输出尺寸计算公式\" class=\"headerlink\" title=\"池化层的输出尺寸计算公式\"></a>池化层的输出尺寸计算公式</h3><p>池化层的输出尺寸计算公式可以表示为：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - F}{S} + 1 \\right\\rfloor $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），即池化层输出的特征图的高度和宽度。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度和宽度。</li>\n<li>$F$ 是池化窗口（Pooling Window）的大小，即池化操作覆盖的输入特征图的区域大小。</li>\n<li>$S$ 是步长（Stride），即池化窗口在输入特征图上移动的步数。</li>\n<li>$P$ 是填充（Padding），即在输入特征图的边界上添加的零的层数，用于控制输出尺寸。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><ul>\n<li>池化层通常不涉及权重和偏置参数，因此它们不会影响模型的学习能力，但对于减少计算量和控制过拟合非常有帮助。</li>\n<li>在实际应用中，池化窗口的大小$F$和步长$S$通常设置为相同的值，如2或3，这样可以更有效地降低特征图的维度。</li>\n<li>填充$P$的值可以是0（无填充），也可以是其他正整数（有填充），具体取决于需要保持输出特征图尺寸与输入特征图尺寸的比例关系。</li>\n</ul>\n<h2 id=\"全连接层（Fully-Connected-Layer-FC-Layer）\"><a href=\"#全连接层（Fully-Connected-Layer-FC-Layer）\" class=\"headerlink\" title=\"全连接层（Fully Connected Layer, FC Layer）\"></a>全连接层（Fully Connected Layer, FC Layer）</h2><p>在CNN的末端，通常会有几个全连接层。这些层中的每个神经元都与前一层的所有神经元相连接。全连接层的作用是将前面层学到的“分布式特征表示”映射到样本标记空间。在分类任务中，全连接层的输出可以传递给softmax函数来生成最终的类别概率分布。</p>\n<h2 id=\"参数共享（Parameter-Sharing）\"><a href=\"#参数共享（Parameter-Sharing）\" class=\"headerlink\" title=\"参数共享（Parameter Sharing）\"></a>参数共享（Parameter Sharing）</h2><p>在CNN中，卷积核的参数是在整个输入数据上共享的。这意味着无论数据中的哪个位置，卷积核都使用相同的权重和偏置参数进行卷积操作。这种参数共享机制减少了模型的参数量，并有助于模型学习到输入数据的空间层次结构。</p>\n<h2 id=\"局部连接（Local-Connectivity）\"><a href=\"#局部连接（Local-Connectivity）\" class=\"headerlink\" title=\"局部连接（Local Connectivity）\"></a>局部连接（Local Connectivity）</h2><p>在卷积层中，每个神经元仅与输入数据的一个局部区域（即感受野）相连接，而不是与整个输入数据相连接。这种局部连接机制使得CNN能够学习到数据的局部特征，这与人类视觉系统的处理机制相似。</p>\n<h2 id=\"反向传播（Backpropagation）\"><a href=\"#反向传播（Backpropagation）\" class=\"headerlink\" title=\"反向传播（Backpropagation）\"></a>反向传播（Backpropagation）</h2><p>反向传播算法是训练CNN（以及其他类型的神经网络）的关键算法。在训练过程中，通过计算损失函数关于网络参数的梯度，并利用梯度下降（或其变体）来更新网络参数，以最小化损失函数。反向传播算法通过链式法则在网络的每一层中传播梯度信息。</p>\n<h1 id=\"MNIST数据集介绍\"><a href=\"#MNIST数据集介绍\" class=\"headerlink\" title=\"MNIST数据集介绍\"></a>MNIST数据集介绍</h1><p>在探索机器学习领域的广阔天地时，手写数字识别作为一个经典且基础的任务，始终占据着重要的地位。而MNIST（Modified National Institute of Standards and Technology）数据集，正是这一任务中最常用、最经典的数据集之一。本文将首先介绍MNIST数据集，为后续的手写数字识别模型训练与测试打下坚实的基础。</p>\n<h2 id=\"MNIST数据集概述\"><a href=\"#MNIST数据集概述\" class=\"headerlink\" title=\"MNIST数据集概述\"></a>MNIST数据集概述</h2><p>MNIST数据集由Yann LeCun等人搜集整理，是一个大型的手写体数字数据库。该数据集最初来源于NIST（National Institute of Standards and Technology）的两个数据库：专用数据库1（Special Database 1）和特殊数据库3（Special Database 3）。通过精心筛选与预处理，MNIST最终成为了一个包含大量手写数字图像的标准数据集，广泛应用于各种图像处理系统和机器学习算法的训练与测试中。</p>\n<h2 id=\"数据集的构成\"><a href=\"#数据集的构成\" class=\"headerlink\" title=\"数据集的构成\"></a>数据集的构成</h2><p>MNIST数据集由60,000个训练样本和10,000个测试样本组成，每个样本都是一张<code>28x28</code>像素的灰度图像，表示一个手写数字（0-9）。这些图像均已被归一化，像素值范围在0到255之间，其中<code>0</code>代表黑色，<code>255</code>代表白色。数据集的图像由来自不同人群的手写体构成，包括高中生和美国人口普查局的工作人员，确保了数据的多样性和代表性。</p>\n<h2 id=\"数据集的特点\"><a href=\"#数据集的特点\" class=\"headerlink\" title=\"数据集的特点\"></a>数据集的特点</h2><ol>\n<li><p><strong>简单性</strong>：虽然MNIST数据集包含的手写数字种类繁多，但由于其图像尺寸小（28x28像素）、像素深度低（灰度图像），使得处理起来相对简单。这使其成为机器学习初学者练习图像识别、模式识别等任务的理想选择。</p>\n</li>\n<li><p><strong>代表性</strong>：MNIST数据集中的手写数字覆盖了各种书写风格和变体，使得训练出的模型能够较好地泛化到未知的手写数字上。因此，该数据集在评估机器学习算法性能时具有很高的参考价值。</p>\n</li>\n<li><p><strong>广泛应用</strong>：由于其简单性和代表性，MNIST数据集在机器学习领域得到了广泛应用。从简单的神经网络到复杂的深度学习模型，几乎所有的图像识别算法都会使用MNIST数据集进行训练和测试。</p>\n</li>\n</ol>\n<h2 id=\"数据集的下载与使用\"><a href=\"#数据集的下载与使用\" class=\"headerlink\" title=\"数据集的下载与使用\"></a>数据集的下载与使用</h2><p>MNIST数据集可以通过多种途径下载，其中最常用的方式是通过互联网直接下载。用户可以从Yann LeCun的官方网站（<a href=\"http://yann.lecun.com/exdb/mnist/%EF%BC%89%E6%88%96%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB%E5%B9%B3%E5%8F%B0%E8%8E%B7%E5%8F%96%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82%E4%B8%8B%E8%BD%BD%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E9%80%9A%E5%B8%B8%E5%8C%85%E5%90%AB%E5%9B%9B%E4%B8%AA%E6%96%87%E4%BB%B6%EF%BC%9A%E8%AE%AD%E7%BB%83%E9%9B%86%E5%9B%BE%E5%83%8F%E3%80%81%E8%AE%AD%E7%BB%83%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E5%9B%BE%E5%83%8F%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%82%E8%BF%99%E4%BA%9B%E6%96%87%E4%BB%B6%E5%9D%87%E4%B8%BA%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%EF%BC%8C%E7%94%A8%E6%88%B7%E9%9C%80%E8%A6%81%E8%A7%A3%E5%8E%8B%E5%90%8E%E6%89%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E3%80%82\">http://yann.lecun.com/exdb/mnist/）或其他数据共享平台获取该数据集。下载后的数据集通常包含四个文件：训练集图像、训练集标签、测试集图像和测试集标签。这些文件均为压缩格式，用户需要解压后才能使用。</a></p>\n<p>在使用MNIST数据集时，用户需要根据自己的需求进行预处理和加载操作。例如，可以使用Python的NumPy库或Pandas库来读取和处理数据集中的图像和标签信息；也可以使用深度学习框架（如TensorFlow、PyTorch等）中提供的数据加载工具来简化这一过程。</p>\n<h1 id=\"下载并导入数据集\"><a href=\"#下载并导入数据集\" class=\"headerlink\" title=\"下载并导入数据集\"></a>下载并导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入必要的库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> datasets, transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据预处理</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Compose组合多个变换，这里将数据转换为张量并进行标准化</span></span><br><span class=\"line\">transform = transforms.Compose([</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">0.5</span>,))</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 下载MNIST数据集并划分为训练集和测试集</span></span><br><span class=\"line\">train_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">True</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\">test_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">False</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>查看训练集属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看整个训练集的样本数量及单个样本的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;训练集大小: <span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_dataset)&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 查看第一个样本的数据和标签</span></span><br><span class=\"line\">first_sample, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本数据形状: <span class=\"subst\">&#123;first_sample.shape&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本标签: <span class=\"subst\">&#123;first_label&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果想查看前几个样本的具体数据内容，可以通过循环实现</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    data, label = train_dataset[i]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;样本 <span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span> 的数据:\\n<span class=\"subst\">&#123;data&#125;</span>\\n标签: <span class=\"subst\">&#123;label&#125;</span>\\n&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>训练集大小: 60000\n首个样本数据形状: torch.Size([1, 28, 28])\n首个样本标签: 5\n样本 1 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588,\n          -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,\n           1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.7647,  0.3490,  0.9843,\n           0.8980,  0.5294, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6157,  0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9686, -0.2706, -0.3569, -0.3569,\n          -0.5608, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8588,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.5529,  0.4275,  0.9373,  0.8902, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078,\n          -0.9137, -1.0000, -0.6627,  0.2078, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902,\n          -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843,\n          -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,\n           0.7647,  0.2549, -0.1529, -0.9922, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,\n           0.8824,  0.9843,  0.9843, -0.0667, -0.8039, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6471,  0.4588,  0.9843,  0.9843,  0.1765, -0.7882, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8745, -0.2706,  0.9765,  0.9843,  0.4667, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9529,  0.9843,  0.9529, -0.4980,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6392,  0.0196,  0.4353,  0.9843,  0.9843,  0.6235, -0.9843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,\n           0.7961,  0.9843,  0.9843,  0.9843,  0.9608,  0.4275, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.5765, -0.3882, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.5529, -0.3647, -0.9843, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n           0.3412,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294,\n          -0.3725, -0.9294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,\n           0.9843,  0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 5\n\n样本 2 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6000,  0.2471,  0.9843,  0.2471, -0.6078, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6235,  0.8667,  0.9765,  0.9765,  0.9765,  0.8588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5765,\n           0.7804,  0.9843,  0.9765,  0.8745,  0.8275,  0.9765, -0.5529,\n          -0.9529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9216, -0.5294,  0.7569,\n           0.9765,  0.9843,  0.9765,  0.5843, -0.3412,  0.9765,  0.9843,\n          -0.0431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9765,  0.9765,\n           0.9765,  0.9843,  0.9765,  0.9765, -0.2471,  0.4824,  0.9843,\n           0.3098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6000,  0.8667,  0.9843,  0.9843,\n           0.4902, -0.1059,  0.9843,  0.7882, -0.6314, -0.3804,  1.0000,\n           0.3176, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.8667,  0.9765,  0.9765,  0.4039,\n          -0.9059, -0.4118, -0.0510, -0.8353, -1.0000, -1.0000,  0.9843,\n           0.9059, -0.6078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7020,  0.2941,  0.9843,  0.8275,  0.6314, -0.3412,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.9451,  0.3961,  0.9765,  0.8824, -0.4431, -0.8510, -0.7804,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5529,  0.9765,  0.9765, -0.5059, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.5529,  0.9843,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n           0.9843,  0.5373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4039,\n           0.9294,  0.9765, -0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.1608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.8039, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9451,  0.0588,  0.9843,\n           0.4588, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.9451,  0.0275,  0.9765,  0.7647,\n          -0.4431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.2941,  0.9765,  0.3569, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3255,\n           0.9843,  0.7647, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.1059,  0.8667,  0.9843,  0.2706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9529,  0.1451, -0.6235, -0.7725, -0.3333,  0.3961,\n           0.7647,  0.9843,  0.7490,  0.3098, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9765,  0.9765,  0.7961,  0.6863,  0.9765,  0.9765,\n           0.9765,  0.5373,  0.0196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7804,\n           0.5608,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,  0.8275,\n           0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8039,  0.0039,  0.9765,  0.9843,  0.9765,  0.1059, -0.7098,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 0\n\n样本 3 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n           0.8196, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5137, -0.3647, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0588,\n           0.4118, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0118,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2000,\n           0.6471, -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7255,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7882,  0.9922,\n           0.2706, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7412,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4353,  0.9922,\n          -0.0196, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.6392,  0.9216,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5529,  0.9922,\n          -0.5608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.0588,  0.9922,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.8196,  0.8118,  0.9922,\n          -0.7725, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.0588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9922,  0.6941,\n          -0.8745, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.4745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902, -0.3255,  0.3961,  0.9451,  0.9922, -0.2863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.3333, -1.0000,\n          -1.0000, -1.0000, -0.6314, -0.6157, -0.0902,  0.1294,  0.1765,\n           0.8902,  0.9059,  0.8353,  0.4039,  0.8902,  0.9765, -0.6863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.1765,  0.9843,  0.8588,  0.6235,\n           0.6235,  0.6235,  0.9843,  0.9922,  0.9608,  0.8824,  0.5529,\n           0.1216, -0.2863, -0.7804, -0.9608,  0.8275,  0.9608, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0667,  0.3882,  0.3882,\n           0.3882,  0.3882,  0.3882, -0.2314, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2000,  0.9922,  0.7255, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.0745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.2471, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 4\n\n样本 4 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.0275,  0.9843,  1.0000,\n          -0.5059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9137,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.0039,  0.9686,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.4667,  0.8510,  0.9686,  0.6549, -0.7569,\n          -0.9373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.5294,  0.7882,  0.9686,  0.9686, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.2157,  0.9843,  0.9843,  0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8431,  0.9843,  0.9686,  0.8431, -0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,\n           0.6078,  0.9843,  0.9686, -0.0118, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1843,\n           0.9686,  0.9843,  0.4431, -0.8824, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.8824,\n           0.9686,  0.5137, -0.8196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.9843,  0.9843,\n           0.9843,  0.2471, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.1843,  0.9686,  0.9686,\n           0.9686, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6235,  0.7333,  0.9686,  0.9686,\n           0.3490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.8353,  0.9686,  0.9686,  0.5373,\n          -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9843,  0.9686,  0.9686, -0.3020,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000,  0.2471,  1.0000,  0.9843,  0.9843, -0.7569,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6235,  0.7882,  0.9843,  0.9373,  0.0980, -0.9373,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8118,  0.5137,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 1\n\n样本 5 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.1608,\n           0.6471,  0.9843,  0.9843, -0.1137, -0.3176,  0.1608, -0.5686,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.3176,  0.8196,  0.9765,\n           0.9843,  0.4824,  0.6471,  0.9765,  0.9765,  0.9843,  0.3176,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9686, -0.5529,  0.8980,  0.9765,  0.4902,\n          -0.4902, -0.9608, -0.9059,  0.4275,  0.9765,  0.9843, -0.0902,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2471,  0.9765,  0.9765,  0.4353, -0.8902,\n          -1.0000, -1.0000, -0.2784,  0.9765,  0.9765,  0.7647, -0.8353,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.0353,  0.9843,  0.9765,  0.1451, -0.8902, -1.0000,\n          -1.0000, -1.0000,  0.6863,  0.9765,  0.9765, -0.3804, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.0118,  0.9843,  0.9373,  0.3804, -0.9294, -1.0000, -1.0000,\n          -0.9373, -0.3882,  0.9216,  0.9843,  0.0118, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8196,  0.9765,  0.3804, -1.0000, -1.0000, -1.0000, -0.7176,\n           0.5765,  0.9765,  0.9765,  0.3255, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8275,\n           0.9765,  0.9765, -0.7647, -0.8275, -0.0667,  0.5451,  0.8902,\n           0.9843,  0.9765,  0.9686, -0.3961, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8118,  0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.7725,\n           0.7804,  0.9765,  0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5686,  0.8431,  0.9843,  0.7020,  0.0824, -0.6706, -0.8118,\n           0.5059,  0.9765,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5137,\n           1.0000,  0.9843, -0.1451, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1686,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6471,\n           1.0000,  0.9843, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.7098,  0.9765, -0.5608, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.2471,  0.9765,  0.4824, -0.6706, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8902,  0.4431,  0.9765,  0.3333, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902,  0.1529,  0.9765, -0.6706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 9\n</code></pre>\n<p>展示训练集中的第一幅图片：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据和标签</span></span><br><span class=\"line\">first_image, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将图像数据从形状 (1, 28, 28) 转换为 (28, 28)，以便显示</span></span><br><span class=\"line\">first_image = first_image.squeeze().numpy()  <span class=\"comment\"># 去除通道维度，并转换为numpy数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图像</span></span><br><span class=\"line\">plt.imshow(first_image, cmap=<span class=\"string\">&#x27;gray&#x27;</span>)  <span class=\"comment\"># 使用灰度色阶显示图像</span></span><br><span class=\"line\">plt.title(<span class=\"string\">f&#x27;Label: <span class=\"subst\">&#123;first_label&#125;</span>&#x27;</span>)  <span class=\"comment\"># 显示图像标签作为标题</span></span><br><span class=\"line\">plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)  <span class=\"comment\"># 不显示坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据</span></span><br><span class=\"line\">first_image, _ = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将张量转换为numpy数组以便打印</span></span><br><span class=\"line\">first_image_np = first_image.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印原始的三维数组（包含通道维度）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;原始图像数据（包含通道维度）:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_np)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>原始图像数据（包含通道维度）:\n[[[-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.9764706  -0.85882354 -0.85882354\n   -0.85882354 -0.01176471  0.06666672  0.37254906 -0.79607844\n    0.30196083  1.          0.9372549  -0.00392157 -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.7647059  -0.7176471\n   -0.26274508  0.20784318  0.33333337  0.9843137   0.9843137\n    0.9843137   0.9843137   0.9843137   0.7647059   0.34901965\n    0.9843137   0.8980392   0.5294118  -0.4980392  -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.6156863   0.8666667   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n    0.9843137   0.9843137   0.96862745 -0.27058822 -0.35686272\n   -0.35686272 -0.56078434 -0.69411767 -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.85882354  0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.5529412\n    0.427451    0.9372549   0.8901961  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.372549    0.22352946\n   -0.1607843   0.9843137   0.9843137   0.60784316 -0.9137255\n   -1.         -0.6627451   0.20784318 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.8901961\n   -0.99215686  0.20784318  0.9843137  -0.29411763 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.          0.09019613  0.9843137   0.4901961  -0.9843137\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -0.9137255   0.4901961   0.9843137  -0.45098037\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.7254902   0.8901961   0.7647059\n    0.254902   -0.15294117 -0.99215686 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.36470586  0.88235295\n    0.9843137   0.9843137  -0.06666666 -0.8039216  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.64705884\n    0.45882356  0.9843137   0.9843137   0.17647064 -0.7882353\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8745098  -0.27058822  0.9764706   0.9843137   0.4666667\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.          0.9529412   0.9843137   0.9529412\n   -0.4980392  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.6392157\n    0.0196079   0.43529415  0.9843137   0.9843137   0.62352943\n   -0.9843137  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.69411767  0.16078436  0.79607844\n    0.9843137   0.9843137   0.9843137   0.9607843   0.427451\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8117647  -0.10588235  0.73333335  0.9843137   0.9843137\n    0.9843137   0.9843137   0.5764706  -0.38823527 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.81960785 -0.4823529\n    0.67058825  0.9843137   0.9843137   0.9843137   0.9843137\n    0.5529412  -0.36470586 -0.9843137  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -0.85882354  0.3411765   0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.5294118  -0.372549\n   -0.92941177 -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -0.5686275\n    0.34901965  0.77254903  0.9843137   0.9843137   0.9843137\n    0.9843137   0.9137255   0.04313731 -0.9137255  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.          0.06666672\n    0.9843137   0.9843137   0.9843137   0.6627451   0.05882359\n    0.03529418 -0.8745098  -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]]]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果你想去掉通道维度，打印二维矩阵</span></span><br><span class=\"line\">first_image_2d = first_image_np.squeeze()  <span class=\"comment\"># 去掉通道维度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n去掉通道后的二维矩阵:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_2d)</span><br></pre></td></tr></table></figure>\n\n<pre><code>去掉通道后的二维矩阵:\n[[-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.9764706  -0.85882354 -0.85882354 -0.85882354 -0.01176471  0.06666672\n   0.37254906 -0.79607844  0.30196083  1.          0.9372549  -0.00392157\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.7647059  -0.7176471  -0.26274508  0.20784318\n   0.33333337  0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n   0.7647059   0.34901965  0.9843137   0.8980392   0.5294118  -0.4980392\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.6156863   0.8666667   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.9843137   0.9843137   0.9843137   0.96862745\n  -0.27058822 -0.35686272 -0.35686272 -0.56078434 -0.69411767 -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.85882354  0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.5529412   0.427451    0.9372549   0.8901961\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.372549    0.22352946 -0.1607843   0.9843137\n   0.9843137   0.60784316 -0.9137255  -1.         -0.6627451   0.20784318\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8901961  -0.99215686  0.20784318\n   0.9843137  -0.29411763 -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.09019613\n   0.9843137   0.4901961  -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -0.9137255\n   0.4901961   0.9843137  -0.45098037 -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.7254902   0.8901961   0.7647059   0.254902   -0.15294117 -0.99215686\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.36470586  0.88235295  0.9843137   0.9843137  -0.06666666\n  -0.8039216  -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.64705884  0.45882356  0.9843137   0.9843137\n   0.17647064 -0.7882353  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8745098  -0.27058822  0.9764706\n   0.9843137   0.4666667  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.9529412\n   0.9843137   0.9529412  -0.4980392  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.6392157   0.0196079   0.43529415  0.9843137\n   0.9843137   0.62352943 -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.69411767  0.16078436  0.79607844  0.9843137   0.9843137   0.9843137\n   0.9607843   0.427451   -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -0.8117647  -0.10588235\n   0.73333335  0.9843137   0.9843137   0.9843137   0.9843137   0.5764706\n  -0.38823527 -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.81960785 -0.4823529   0.67058825  0.9843137\n   0.9843137   0.9843137   0.9843137   0.5529412  -0.36470586 -0.9843137\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -0.85882354  0.3411765   0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.5294118  -0.372549   -0.92941177 -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -0.5686275   0.34901965\n   0.77254903  0.9843137   0.9843137   0.9843137   0.9843137   0.9137255\n   0.04313731 -0.9137255  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.          0.06666672  0.9843137\n   0.9843137   0.9843137   0.6627451   0.05882359  0.03529418 -0.8745098\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]]\n</code></pre>\n<h1 id=\"定义模型、优化器、损失函数\"><a href=\"#定义模型、优化器、损失函数\" class=\"headerlink\" title=\"定义模型、优化器、损失函数\"></a>定义模型、优化器、损失函数</h1><p>进行2次卷积和2次池化，得到64<em>7</em>17，再进行2次全连接，得到10个输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义批次大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">64</span></span><br><span class=\"line\"><span class=\"comment\"># 使用DataLoader加载数据，以便在训练过程中更方便地迭代数据</span></span><br><span class=\"line\">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 初始化卷积层、池化层、全连接层和dropout层</span></span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个卷积层，用于提取特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为1（适用于灰度图像），输出通道数为32，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次卷积后生成的特征图大小为32*28*28</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义最大池化层，用于降低特征维度，减少计算量</span></span><br><span class=\"line\">        <span class=\"comment\"># 池化窗口大小为2x2，步长为2，无padding</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次池化后生成的特征图大小为32*14*14</span></span><br><span class=\"line\">        self.pool = nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个卷积层，进一步提取和整合特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为32，输出通道数为64，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第二次卷积后生成的特征图大小为64*14*14</span></span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">32</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个全连接层，用于分类前的特征转换</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为64*7*7（这里的尺寸为64*7*7是因为在前向传播时对第二次卷积进行了池化操作），输出大小为1024</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>, <span class=\"number\">1024</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个全连接层，用于最终的分类输出</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为1024，输出大小为10（假设分类任务有10个类别）</span></span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 定义Dropout层，用于训练过程中的正则化，防止过拟合</span></span><br><span class=\"line\">        <span class=\"comment\"># Dropout比例为0.5，即在训练过程中随机将50%的元素置为0</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 定义前向传播过程</span></span><br><span class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class=\"line\">        x = x.view(-<span class=\"number\">1</span>, <span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>)</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = self.dropout(x)</span><br><span class=\"line\">        x = self.fc2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 实例化模型</span></span><br><span class=\"line\">model = Net()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义优化器</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Adam优化器更新模型参数</span></span><br><span class=\"line\">optimizer = optim.Adam(model.parameters(), lr=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\"><span class=\"comment\"># 使用交叉熵损失函数进行分类任务</span></span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将模型转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义训练轮数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"comment\"># 开始训练过程</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (images, labels) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_loader):</span><br><span class=\"line\">        <span class=\"comment\"># 将数据转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 反向传播和优化</span></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 打印损失信息</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Epoch [<span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;num_epochs&#125;</span>], Step [<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_loader)&#125;</span>], Loss: <span class=\"subst\">&#123;loss.item():<span class=\"number\">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存模型参数到文件</span></span><br><span class=\"line\">torch.save(model.state_dict(), <span class=\"string\">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Epoch [1/10], Step [100/938], Loss: 0.0037\nEpoch [1/10], Step [200/938], Loss: 0.0029\nEpoch [1/10], Step [300/938], Loss: 0.0022\nEpoch [1/10], Step [400/938], Loss: 0.0651\nEpoch [1/10], Step [500/938], Loss: 0.0038\nEpoch [1/10], Step [600/938], Loss: 0.0022\nEpoch [1/10], Step [700/938], Loss: 0.0079\nEpoch [1/10], Step [800/938], Loss: 0.0019\nEpoch [1/10], Step [900/938], Loss: 0.0016\nEpoch [2/10], Step [100/938], Loss: 0.0016\nEpoch [2/10], Step [200/938], Loss: 0.0102\nEpoch [2/10], Step [300/938], Loss: 0.0341\nEpoch [2/10], Step [400/938], Loss: 0.0060\nEpoch [2/10], Step [500/938], Loss: 0.0257\nEpoch [2/10], Step [600/938], Loss: 0.0013\nEpoch [2/10], Step [700/938], Loss: 0.0767\nEpoch [2/10], Step [800/938], Loss: 0.0018\nEpoch [2/10], Step [900/938], Loss: 0.0343\nEpoch [3/10], Step [100/938], Loss: 0.0063\nEpoch [3/10], Step [200/938], Loss: 0.0096\nEpoch [3/10], Step [300/938], Loss: 0.0007\nEpoch [3/10], Step [400/938], Loss: 0.0002\nEpoch [3/10], Step [500/938], Loss: 0.0124\nEpoch [3/10], Step [600/938], Loss: 0.0109\nEpoch [3/10], Step [700/938], Loss: 0.0340\nEpoch [3/10], Step [800/938], Loss: 0.0004\nEpoch [3/10], Step [900/938], Loss: 0.0586\nEpoch [4/10], Step [100/938], Loss: 0.0002\nEpoch [4/10], Step [200/938], Loss: 0.0554\nEpoch [4/10], Step [300/938], Loss: 0.0008\nEpoch [4/10], Step [400/938], Loss: 0.0029\nEpoch [4/10], Step [500/938], Loss: 0.0036\nEpoch [4/10], Step [600/938], Loss: 0.0009\nEpoch [4/10], Step [700/938], Loss: 0.0281\nEpoch [4/10], Step [800/938], Loss: 0.0826\nEpoch [4/10], Step [900/938], Loss: 0.0003\nEpoch [5/10], Step [100/938], Loss: 0.0001\nEpoch [5/10], Step [200/938], Loss: 0.0240\nEpoch [5/10], Step [300/938], Loss: 0.0040\nEpoch [5/10], Step [400/938], Loss: 0.0003\nEpoch [5/10], Step [500/938], Loss: 0.0107\nEpoch [5/10], Step [600/938], Loss: 0.0019\nEpoch [5/10], Step [700/938], Loss: 0.0002\nEpoch [5/10], Step [800/938], Loss: 0.0006\nEpoch [5/10], Step [900/938], Loss: 0.0008\nEpoch [6/10], Step [100/938], Loss: 0.0003\nEpoch [6/10], Step [200/938], Loss: 0.0001\nEpoch [6/10], Step [300/938], Loss: 0.0003\nEpoch [6/10], Step [400/938], Loss: 0.0226\nEpoch [6/10], Step [500/938], Loss: 0.0024\nEpoch [6/10], Step [600/938], Loss: 0.0020\nEpoch [6/10], Step [700/938], Loss: 0.0005\nEpoch [6/10], Step [800/938], Loss: 0.0007\nEpoch [6/10], Step [900/938], Loss: 0.0188\nEpoch [7/10], Step [100/938], Loss: 0.0286\nEpoch [7/10], Step [200/938], Loss: 0.0007\nEpoch [7/10], Step [300/938], Loss: 0.0004\nEpoch [7/10], Step [400/938], Loss: 0.0008\nEpoch [7/10], Step [500/938], Loss: 0.0001\nEpoch [7/10], Step [600/938], Loss: 0.0006\nEpoch [7/10], Step [700/938], Loss: 0.0005\nEpoch [7/10], Step [800/938], Loss: 0.0007\nEpoch [7/10], Step [900/938], Loss: 0.0432\nEpoch [8/10], Step [100/938], Loss: 0.0005\nEpoch [8/10], Step [200/938], Loss: 0.0005\nEpoch [8/10], Step [300/938], Loss: 0.0000\nEpoch [8/10], Step [400/938], Loss: 0.0013\nEpoch [8/10], Step [500/938], Loss: 0.0005\nEpoch [8/10], Step [600/938], Loss: 0.0002\nEpoch [8/10], Step [700/938], Loss: 0.0004\nEpoch [8/10], Step [800/938], Loss: 0.0111\nEpoch [8/10], Step [900/938], Loss: 0.0001\nEpoch [9/10], Step [100/938], Loss: 0.0004\nEpoch [9/10], Step [200/938], Loss: 0.0693\nEpoch [9/10], Step [300/938], Loss: 0.0071\nEpoch [9/10], Step [400/938], Loss: 0.0000\nEpoch [9/10], Step [500/938], Loss: 0.0003\nEpoch [9/10], Step [600/938], Loss: 0.0003\nEpoch [9/10], Step [700/938], Loss: 0.0001\nEpoch [9/10], Step [800/938], Loss: 0.0000\nEpoch [9/10], Step [900/938], Loss: 0.0029\nEpoch [10/10], Step [100/938], Loss: 0.0008\nEpoch [10/10], Step [200/938], Loss: 0.0001\nEpoch [10/10], Step [300/938], Loss: 0.0000\nEpoch [10/10], Step [400/938], Loss: 0.0273\nEpoch [10/10], Step [500/938], Loss: 0.0001\nEpoch [10/10], Step [600/938], Loss: 0.0002\nEpoch [10/10], Step [700/938], Loss: 0.0010\nEpoch [10/10], Step [800/938], Loss: 0.0019\nEpoch [10/10], Step [900/938], Loss: 0.0000\n</code></pre>\n<h1 id=\"评估模型\"><a href=\"#评估模型\" class=\"headerlink\" title=\"评估模型\"></a>评估模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 实例化一个与原模型结构相同的模型</span></span><br><span class=\"line\">model = Net().to(device)  <span class=\"comment\"># 确保模型被放置在正确的设备上</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载模型参数</span></span><br><span class=\"line\">model.load_state_dict(torch.load(<span class=\"string\">&#x27;model.pth&#x27;</span>, map_location=device, weights_only=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将模型设置为评估模式</span></span><br><span class=\"line\">model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"><span class=\"comment\"># 禁用梯度计算以减少内存消耗</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    correct = <span class=\"number\">0</span></span><br><span class=\"line\">    total = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"comment\"># 在测试集上进行预测</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> images, labels <span class=\"keyword\">in</span> test_loader:</span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        _, predicted = torch.<span class=\"built_in\">max</span>(outputs.data, <span class=\"number\">1</span>)</span><br><span class=\"line\">        total += labels.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        correct += (predicted == labels).<span class=\"built_in\">sum</span>().item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 打印测试集上的准确率</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Test Accuracy of the model on the <span class=\"subst\">&#123;total&#125;</span> test images: <span class=\"subst\">&#123;<span class=\"number\">100</span> * correct / total&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Test Accuracy of the model on the 10000 test images: 99.29%\n</code></pre>\n<h1 id=\"代码获取\"><a href=\"#代码获取\" class=\"headerlink\" title=\"代码获取\"></a>代码获取</h1><p>关注公众号“生信之巅”，聊天窗口回复“a7fe”获取完整版代码下载链接。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"PyTorch","path":"api/tags/PyTorch.json"},{"name":"卷积神经网络","path":"api/tags/卷积神经网络.json"}]}]}
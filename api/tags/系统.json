{"name":"系统","postlist":[{"title":"终版实测！CentOS 7.9（glibc 2.17）升级 Go 1.24.4 编译 Ollama 0.13.3，4 张 L4 GPU 满血运行～","slug":"CentOS7中Go1.24.4安装与Ollama0.13.3源码编译","date":"2025-12-16T11:18:18.000Z","updated":"2025-12-18T11:15:07.448Z","comments":true,"path":"api/articles/CentOS7中Go1.24.4安装与Ollama0.13.3源码编译.json","excerpt":null,"keywords":null,"cover":null,"content":"<p>🔥 作为坚守 CentOS 7.9 的运维 &#x2F; 开发者，谁懂啊！官方 Ollama 二进制包直接报错「GLIBC_2.27 not found」，而系统 glibc 2.17 根本没法升级（怕崩系统）😫 但 4 张 NVIDIA L4 GPU（总计 96GB 显存）不能浪费，非要在原生环境跑 Ollama 部署大模型，于是硬着头皮走 Go 编译路线，过程踩遍坑，现在整理成保姆级教程，造福同款老系统用户！</p>\n<h1 id=\"📌-核心背景：为啥非要自己动手？\"><a href=\"#📌-核心背景：为啥非要自己动手？\" class=\"headerlink\" title=\"📌 核心背景：为啥非要自己动手？\"></a>📌 核心背景：为啥非要自己动手？</h1><ul>\n<li><p>系统限制：CentOS 7.9 默认 glibc 2.17，官方 Ollama 所有版本都依赖 glibc≥2.27，直接运行必报错；</p>\n</li>\n<li><p>硬件刚需：4 张 L4 GPU 想最大化算力，原生编译比 Docker 少虚拟化损耗，多卡并行更高效。</p>\n</li>\n<li><p>无替代方案：社区无适配 glibc 2.17 的 Ollama 预编译包，只能源码编译定制。</p>\n</li>\n</ul>\n<h1 id=\"🛠️-全程实操（按自身路径调整）\"><a href=\"#🛠️-全程实操（按自身路径调整）\" class=\"headerlink\" title=\"🛠️ 全程实操（按自身路径调整）\"></a>🛠️ 全程实操（按自身路径调整）</h1><ul>\n<li>版本选择：Go 1.24.4（适配 Ollama 0.13.3，兼容 glibc 2.17）+ Ollama 0.13.3 源码包（<a href=\"https://github.com/ollama/ollama/archive/refs/tags/v0.13.3.tar.gz\">https://github.com/ollama/ollama/archive/refs/tags/v0.13.3.tar.gz</a>）；</li>\n</ul>\n<h2 id=\"1️⃣-升级-Go-到-1-24-4（编译-Ollama-的前提，不同用户路径不同，务必自查！）\"><a href=\"#1️⃣-升级-Go-到-1-24-4（编译-Ollama-的前提，不同用户路径不同，务必自查！）\" class=\"headerlink\" title=\"1️⃣ 升级 Go 到 1.24.4（编译 Ollama 的前提，不同用户路径不同，务必自查！）\"></a>1️⃣ 升级 Go 到 1.24.4（编译 Ollama 的前提，不同用户路径不同，务必自查！）</h2><ul>\n<li>卸载旧 Go（示例路径，替换成自己的！）：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">rm</span> -rf /share/opt/go  <span class=\"comment\"># 我之前装的Go 1.18.2路径</span></span><br><span class=\"line\">sed -i <span class=\"string\">&#x27;/\\/share\\/opt\\/go\\/bin/d&#x27;</span> ~/.bashrc</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>下载 Go 1.24.4（阿里云镜像，稳定不翻车）：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://mirrors.aliyun.com/golang/go1.24.4.linux-amd64.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\">tar -C /usr/local -xzf go1.24.4.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>配置 Go 环境变量（全局生效）：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vi /etc/profile</span><br></pre></td></tr></table></figure>\n\n<p>新增以下内容：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export GOROOT=/usr/local/go</span><br><span class=\"line\">export PATH=$GOROOT/bin:$PATH</span><br><span class=\"line\">export GOPATH=$HOME/go</span><br><span class=\"line\">export GOPROXY=https://mirrors.aliyun.com/goproxy/,direct  # 国内代理避坑</span><br></pre></td></tr></table></figure>\n\n<p>保存并退出（ESC， shift + ;, wq, 回车），使配置生效：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">source</span> /etc/profile</span><br></pre></td></tr></table></figure>\n\n\n<ul>\n<li>验证：<code>go version</code> 输出 <code>go1.24.4 linux/amd64</code> 即成功。</li>\n</ul>\n<h2 id=\"2️⃣-下载-Ollama-0-13-3-源码包（无需-git，解压即编译）\"><a href=\"#2️⃣-下载-Ollama-0-13-3-源码包（无需-git，解压即编译）\" class=\"headerlink\" title=\"2️⃣ 下载 Ollama 0.13.3 源码包（无需 git，解压即编译）\"></a>2️⃣ 下载 Ollama 0.13.3 源码包（无需 git，解压即编译）</h2><ul>\n<li><p>下载源码：<code>wget https://github.com/ollama/ollama/archive/refs/tags/v0.13.3.tar.gz</code>；</p>\n</li>\n<li><p>解压进入目录：</p>\n</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tar -zxvf v0.13.3.tar.gz</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 看清解压后的目录名再进入</span></span><br><span class=\"line\"><span class=\"built_in\">cd</span> ollama-0.13.3</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3️⃣-编译-Ollama-0-13-3（适配-glibc-2-17-核心参数）\"><a href=\"#3️⃣-编译-Ollama-0-13-3（适配-glibc-2-17-核心参数）\" class=\"headerlink\" title=\"3️⃣ 编译 Ollama 0.13.3（适配 glibc 2.17 核心参数）\"></a>3️⃣ 编译 Ollama 0.13.3（适配 glibc 2.17 核心参数）</h2><ul>\n<li>配置编译环境变量（静态编译，规避 glibc 版本问题）：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> CGO_ENABLED=1</span><br><span class=\"line\"><span class=\"built_in\">export</span> CGO_CFLAGS=<span class=\"string\">&quot;-D_GLIBCXX_USE_CXX11_ABI=0 -m64 -Wl,--hash-style=gnu -Wl,--as-needed&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> CGO_LDFLAGS=<span class=\"string\">&quot;-lrt -lm -ldl -Wl,--disable-new-dtags -Wl,--rpath=/lib64&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> GOFLAGS=<span class=\"string\">&quot;-buildmode=pie -trimpath&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> CC=<span class=\"string\">&quot;gcc -static-libgcc -static-libstdc++&quot;</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>执行编译（0.13.3 主入口在根目录 main.go，别写错！）：</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">go build -ldflags <span class=\"string\">&quot;-s -w -linkmode=external -extldflags &#x27;-static-libgcc -static-libstdc++ -lm -ldl&#x27;&quot;</span> -o ollama ./main.go</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>验证：<code>./ollama --help</code> 无 GLIBC 报错，将二进制移到系统路径：<code>cp ./ollama /usr/local/bin/</code>，或将当前路径加入环境变量（我选择的后者：&#x2F;share&#x2F;tools&#x2F;ollama-0.13.3）。</li>\n</ul>\n<h2 id=\"4️⃣-配置开机自启（后台稳跑，省心！）\"><a href=\"#4️⃣-配置开机自启（后台稳跑，省心！）\" class=\"headerlink\" title=\"4️⃣ 配置开机自启（后台稳跑，省心！）\"></a>4️⃣ 配置开机自启（后台稳跑，省心！）</h2><ul>\n<li>创建 systemd 服务文件：<code>vi /etc/systemd/system/ollama.service</code>，写入：</li>\n</ul>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Unit]</span><br><span class=\"line\"></span><br><span class=\"line\">Description=Ollama Service</span><br><span class=\"line\"></span><br><span class=\"line\">After=network.target</span><br><span class=\"line\"></span><br><span class=\"line\">[Service]</span><br><span class=\"line\"></span><br><span class=\"line\">Type=simple</span><br><span class=\"line\"></span><br><span class=\"line\">User=root</span><br><span class=\"line\"></span><br><span class=\"line\"># 绑定4张L4 GPU，开启TensorRT加速</span><br><span class=\"line\"></span><br><span class=\"line\">Environment=&quot;CUDA_VISIBLE_DEVICES=0,1,2,3&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">Environment=&quot;OLLAMA_TENSORRT=1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\"># 自定义模型存放路径，避免占满/root</span><br><span class=\"line\"></span><br><span class=\"line\">Environment=&quot;OLLAMA_MODELS=/share/public_db/ollama/models&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">ExecStart=/share/tools/ollama-0.13.3/ollama serve</span><br><span class=\"line\"></span><br><span class=\"line\">Restart=on-failure</span><br><span class=\"line\"></span><br><span class=\"line\">[Install]</span><br><span class=\"line\"></span><br><span class=\"line\">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>生效配置：<code>systemctl daemon-reload &amp;&amp; systemctl enable --now ollama</code>；</p>\n</li>\n<li><p>验证服务：<code>systemctl status ollama</code> 显示<code>active(running)</code>即可。</p>\n</li>\n</ul>\n<h2 id=\"5️⃣-环境变量-模型路径优化（长期使用必备）\"><a href=\"#5️⃣-环境变量-模型路径优化（长期使用必备）\" class=\"headerlink\" title=\"5️⃣ 环境变量 + 模型路径优化（长期使用必备）\"></a>5️⃣ 环境变量 + 模型路径优化（长期使用必备）</h2><ul>\n<li>全局配置（&#x2F;etc&#x2F;profile 新增）：</li>\n</ul>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export OLLAMA_MODELS=/share/public_db/ollama/models</span><br><span class=\"line\"></span><br><span class=\"line\">export OLLAMA_TENSORRT=1</span><br><span class=\"line\"></span><br><span class=\"line\">export GOPROXY=https://mirrors.aliyun.com/goproxy/,direct</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><p>生效：<code>source /etc/profile</code>；</p>\n</li>\n<li><p>模型路径权限（必做！）：</p>\n</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">mkdir</span> -p /share/public_db/ollama/models</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">chmod</span> 755 /share/public_db/ollama/models</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">chown</span> -R root:root /share/public_db/ollama/models</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>测试：<code>ollama pull llama3:8b</code>，模型自动存到<code>/share/public_db/ollama/models</code>，验证路径生效。</li>\n</ul>\n<h1 id=\"✅-最终成果\"><a href=\"#✅-最终成果\" class=\"headerlink\" title=\"✅ 最终成果\"></a>✅ 最终成果</h1><ul>\n<li><p>环境：CentOS 7.9（glibc 2.17）+ Go 1.24.4 + Ollama 0.13.3（原生编译）；</p>\n</li>\n<li><p>硬件：4 张 L4 GPU 满负载运行，llama3:70b（INT4 量化）；</p>\n</li>\n<li><p>稳定性：开机自启无故障，TensorRT 加速加持，延迟大幅降低。</p>\n</li>\n</ul>\n<h1 id=\"🚨-关键避坑点\"><a href=\"#🚨-关键避坑点\" class=\"headerlink\" title=\"🚨 关键避坑点\"></a>🚨 关键避坑点</h1><ol>\n<li><p>Go 路径：不同用户安装位置不同，用<code>which go</code>自查，确保 PATH 优先指向 Go 1.24.4；</p>\n</li>\n<li><p>编译路径：Ollama 0.13.3 主入口是<code>./main.go</code>，别用<code>./cmd/ollama</code>（报目录不存在）；</p>\n</li>\n<li><p>权限：模型路径必须提前赋权，否则拉取模型提示权限拒绝；</p>\n</li>\n<li><p>加速：L4 GPU 开启 OLLAMA_TENSORRT&#x3D;1，推理效率提升 30%+。</p>\n</li>\n</ol>\n<p>老系统也能拿捏大模型！Go 1.24.4+Ollama 0.13.3 这套组合，完美适配 CentOS 7.9，4 张 L4 GPU 终于物尽其用～ 转发给同处境的朋友，少走弯路！🚀</p>\n<p>#CentOS7 #Ollama0.13.3 #Go1.24.4 #glibc2.17 #源码编译 #NVIDIA L4 #大模型部署</p>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"Linux","path":"api/tags/Linux.json"},{"name":"系统","path":"api/tags/系统.json"},{"name":"Ollama","path":"api/tags/Ollama.json"}]},{"title":"CentOS 7 升级 GCC 教程（以 GCC 9 为例）","slug":"CentOS-7-升级-GCC-教程（以-GCC-9-为例）","date":"2025-10-19T13:33:35.000Z","updated":"2025-10-19T13:40:18.210Z","comments":true,"path":"api/articles/CentOS-7-升级-GCC-教程（以-GCC-9-为例）.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg","content":"<h2 id=\"一、教程背景\"><a href=\"#一、教程背景\" class=\"headerlink\" title=\"一、教程背景\"></a>一、教程背景</h2><p>CentOS 7 系统默认预装的 GCC 版本为 <strong>4.8.5</strong>（发布于 2015 年），而现代开发场景中，许多软件（如高版本 Python、TensorFlow、C++ 11+ 项目等）对 GCC 版本要求较高（需 5.4+ 及以上）。因此需升级 GCC，但直接覆盖系统默认 GCC 可能导致依赖冲突（如 <code>yum</code> 工具依赖旧版本）。</p>\n<p>本教程采用 <strong>SCL（Software Collections）仓库</strong> 方式升级，可在不覆盖系统默认 GCC 的前提下，安装并使用高版本 GCC（以 GCC 9 为例，稳定且兼容性强），安全且灵活。</p>\n<h2 id=\"二、升级前准备\"><a href=\"#二、升级前准备\" class=\"headerlink\" title=\"二、升级前准备\"></a>二、升级前准备</h2><p>确保系统已联网，且拥有 <code>root</code> 权限（或使用 <code>sudo</code> 权限执行命令）。</p>\n<h2 id=\"三、详细升级步骤\"><a href=\"#三、详细升级步骤\" class=\"headerlink\" title=\"三、详细升级步骤\"></a>三、详细升级步骤</h2><h3 id=\"1-备份原有-YUM-源（关键：防止配置出错）\"><a href=\"#1-备份原有-YUM-源（关键：防止配置出错）\" class=\"headerlink\" title=\"1. 备份原有 YUM 源（关键：防止配置出错）\"></a>1. 备份原有 YUM 源（关键：防止配置出错）</h3><p>CentOS 7 停止官方维护后，默认 YUM 源可能失效，且后续需添加新仓库配置。先备份原有源文件，避免误操作后无法恢复：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建备份目录</span></span><br><span class=\"line\">sudo <span class=\"built_in\">mkdir</span> -p /etc/yum.repos.d/backup</span><br><span class=\"line\"><span class=\"comment\"># 将所有 .repo 源文件移动到备份目录</span></span><br><span class=\"line\">sudo <span class=\"built_in\">mv</span> /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"2-添加-SCL-仓库配置文件（使用阿里云源）\"><a href=\"#2-添加-SCL-仓库配置文件（使用阿里云源）\" class=\"headerlink\" title=\"2. 添加 SCL 仓库配置文件（使用阿里云源）\"></a>2. 添加 SCL 仓库配置文件（使用阿里云源）</h3><p>SCL 仓库是升级 GCC 的核心，这里选择 <strong>阿里云 SCL 源</strong>（国内访问速度快，稳定性高），手动创建仓库配置文件：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用 vi 编辑器创建并编辑 centos-sclo-rh.repo 文件</span></span><br><span class=\"line\">sudo vi /etc/yum.repos.d/centos-sclo-rh.repo</span><br></pre></td></tr></table></figure>\n\n<p>打开编辑器后，按 <code>i</code> 进入编辑模式，<strong>完整复制以下内容</strong>（不要遗漏或修改格式）：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[centos-sclo-rh]</span><br><span class=\"line\">name=CentOS-7 - SCLo rh</span><br><span class=\"line\">baseurl=https://mirrors.aliyun.com/centos/7/sclo/x86_64/rh/</span><br><span class=\"line\">gpgcheck=1</span><br><span class=\"line\">gpgkey=https://mirrors.aliyun.com/centos/RPM-GPG-KEY-CentOS-SIG-SCLo</span><br><span class=\"line\">enabled=1</span><br></pre></td></tr></table></figure>\n\n<p>复制完成后，按 <code>Esc</code> 退出编辑模式，输入 <code>:wq</code> 并回车（保存并退出 vi 编辑器）。</p>\n<h3 id=\"3-清理-YUM-缓存并生成新缓存\"><a href=\"#3-清理-YUM-缓存并生成新缓存\" class=\"headerlink\" title=\"3. 清理 YUM 缓存并生成新缓存\"></a>3. 清理 YUM 缓存并生成新缓存</h3><p>新添加仓库后，需清理旧缓存、加载新仓库的包列表，确保后续安装能找到正确的软件包：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 清理所有 YUM 缓存</span></span><br><span class=\"line\">sudo yum clean all &amp;&amp; </span><br><span class=\"line\"><span class=\"comment\"># 生成新的缓存（下载阿里云源的包列表）</span></span><br><span class=\"line\">sudo yum makecache</span><br></pre></td></tr></table></figure>\n\n<p>执行后耐心等待（时间取决于网络速度），若无报错则缓存生成成功。</p>\n<h3 id=\"4-（可选）安装-SCL-仓库基础包\"><a href=\"#4-（可选）安装-SCL-仓库基础包\" class=\"headerlink\" title=\"4. （可选）安装 SCL 仓库基础包\"></a>4. （可选）安装 SCL 仓库基础包</h3><p>多数情况下，步骤 2 添加的配置文件已足够，但若后续安装 GCC 时提示“仓库不存在”，可执行此步骤安装 SCL 仓库基础包：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y centos-release-scl</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"5-安装高版本-GCC（GCC-9）\"><a href=\"#5-安装高版本-GCC（GCC-9）\" class=\"headerlink\" title=\"5. 安装高版本 GCC（GCC 9）\"></a>5. 安装高版本 GCC（GCC 9）</h3><p>通过 YUM 安装 <code>devtoolset-9</code>（对应 GCC 9 版本），并添加 <code>--nogpgcheck</code> 参数（跳过 GPG 密钥验证，因 CentOS 7 官方密钥源已失效，不影响包安全性）：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo yum install -y devtoolset-9-gcc devtoolset-9-gcc-c++ devtoolset-9-binutils --nogpgcheck</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li><code>devtoolset-9-gcc</code>：GCC 9 核心包</li>\n<li><code>devtoolset-9-gcc-c++</code>：C++ 编译组件（如需编译 C++ 项目必装）</li>\n<li><code>devtoolset-9-binutils</code>：二进制工具集（辅助编译）</li>\n</ul>\n<p>执行后等待安装完成（约 50MB 下载量），无报错则安装成功。</p>\n<h3 id=\"6-启用高版本-GCC\"><a href=\"#6-启用高版本-GCC\" class=\"headerlink\" title=\"6. 启用高版本 GCC\"></a>6. 启用高版本 GCC</h3><p>SCL 安装的 GCC 不会自动生效，需手动启用，支持 <strong>临时启用</strong> 和 <strong>永久启用</strong>，根据需求选择：</p>\n<h4 id=\"方式-1：临时启用（仅当前终端有效）\"><a href=\"#方式-1：临时启用（仅当前终端有效）\" class=\"headerlink\" title=\"方式 1：临时启用（仅当前终端有效）\"></a>方式 1：临时启用（仅当前终端有效）</h4><p>适用于“临时使用高版本 GCC”的场景，关闭终端后恢复默认 GCC 4.8.5：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scl <span class=\"built_in\">enable</span> devtoolset-9 bash</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"方式-2：永久启用（推荐）\"><a href=\"#方式-2：永久启用（推荐）\" class=\"headerlink\" title=\"方式 2：永久启用（推荐）\"></a>方式 2：永久启用（推荐）</h4><p>若需每次登录终端都自动使用 GCC 9，可将启用命令写入环境变量文件，分“当前用户”和“全局所有用户”两种场景：</p>\n<h5 id=\"场景-A：仅当前用户生效（推荐普通用户）\"><a href=\"#场景-A：仅当前用户生效（推荐普通用户）\" class=\"headerlink\" title=\"场景 A：仅当前用户生效（推荐普通用户）\"></a>场景 A：仅当前用户生效（推荐普通用户）</h5><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将启用命令写入当前用户的 .bashrc 文件</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;source /opt/rh/devtoolset-9/enable&quot;</span> &gt;&gt; ~/.bashrc</span><br><span class=\"line\"><span class=\"comment\"># 立即生效（无需重启终端）</span></span><br><span class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"场景-B：全局所有用户生效（需-root-权限）\"><a href=\"#场景-B：全局所有用户生效（需-root-权限）\" class=\"headerlink\" title=\"场景 B：全局所有用户生效（需 root 权限）\"></a>场景 B：全局所有用户生效（需 root 权限）</h5><p>适用于多用户服务器，所有用户登录后均使用 GCC 9：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 切换到 root 用户（若当前非 root）</span></span><br><span class=\"line\">su root</span><br><span class=\"line\"><span class=\"comment\"># 将启用命令写入全局环境变量文件 /etc/profile</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;source /opt/rh/devtoolset-9/enable&quot;</span> &gt;&gt; /etc/profile</span><br><span class=\"line\"><span class=\"comment\"># 立即生效</span></span><br><span class=\"line\"><span class=\"built_in\">source</span> /etc/profile</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"7-验证-GCC-升级结果\"><a href=\"#7-验证-GCC-升级结果\" class=\"headerlink\" title=\"7. 验证 GCC 升级结果\"></a>7. 验证 GCC 升级结果</h3><p>执行以下命令查看当前 GCC 版本，确认是否升级成功：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gcc -v</span><br></pre></td></tr></table></figure>\n\n<p>若输出类似以下内容（版本号为 9.3.1 及以上），则说明升级成功：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Using built-in specs.</span><br><span class=\"line\">COLLECT_GCC=gcc</span><br><span class=\"line\">COLLECT_LTO_WRAPPER=/opt/rh/devtoolset-9/root/usr/libexec/gcc/x86_64-redhat-linux/9/lto-wrapper</span><br><span class=\"line\">Target: x86_64-redhat-linux</span><br><span class=\"line\">Configured with: ../configure --enable-bootstrap --enable-languages=c,c++,fortran,lto --prefix=/opt/rh/devtoolset-9/root/usr --mandir=/opt/rh/devtoolset-9/root/usr/share/man --infodir=/opt/rh/devtoolset-9/root/usr/share/info --with-bugurl=http://bugzilla.redhat.com/bugzilla --enable-shared --enable-threads=posix --enable-checking=release --enable-multilib --with-system-zlib --enable-__cxa_atexit --disable-libunwind-exceptions --enable-gnu-unique-object --enable-linker-build-id --with-gcc-major-version-only --with-linker-hash-style=gnu --with-default-libstdcxx-abi=gcc4-compatible --enable-plugin --enable-initfini-array --with-isl=/builddir/build/BUILD/gcc-9.3.1-20200408/obj-x86_64-redhat-linux/isl-install --disable-libmpx --enable-gnu-indirect-function --with-tune=generic --with-arch_32=x86-64 --build=x86_64-redhat-linux</span><br><span class=\"line\">Thread model: posix</span><br><span class=\"line\">gcc version 9.3.1 20200408 (Red Hat 9.3.1-2.2.el7) (GCC)</span><br></pre></td></tr></table></figure>\n\n\n<h2 id=\"四、常见问题与注意事项\"><a href=\"#四、常见问题与注意事项\" class=\"headerlink\" title=\"四、常见问题与注意事项\"></a>四、常见问题与注意事项</h2><ol>\n<li><p><strong>若需安装其他 GCC 版本</strong>：<br>本教程以 GCC 9 为例，若需安装 GCC 10，只需将命令中的 <code>devtoolset-9</code> 替换为 <code>devtoolset-10</code>（如 <code>devtoolset-10-gcc</code>），其他步骤一致。</p>\n</li>\n<li><p><strong>系统默认 GCC 未被覆盖</strong>：<br>SCL 安装的 GCC 位于 <code>/opt/rh/devtoolset-9/root/usr/bin/</code>，系统默认 GCC（4.8.5）仍在 <code>/usr/bin/gcc</code>，若需临时使用旧版本，可直接执行 <code>/usr/bin/gcc -v</code>。</p>\n</li>\n<li><p><strong>安装时提示“无可用软件包”</strong>：<br>重新执行步骤 3（清理并生成缓存），或检查步骤 2 的仓库配置文件是否正确（确保 <code>baseurl</code> 未写错）。</p>\n</li>\n</ol>\n<h2 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h2><p>通过本教程，可在 CentOS 7 上安全升级 GCC 至 9 版本（或更高），满足现代软件开发需求，且不影响系统原有依赖。若后续需编译软件（如 Python、C++ 项目），直接使用 <code>gcc</code> 或 <code>g++</code> 命令即可调用高版本工具。</p>\n<h2 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h2><p>关注公众号“生信之巅”。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"Linux","path":"api/tags/Linux.json"},{"name":"系统","path":"api/tags/系统.json"}]},{"title":"加装 GPU 后 IB 网卡（ib0）消失？CentOS 7.9 自动恢复教程（亲测有效）","slug":"CentOS7加装GPU后IB网卡自动恢复教程","date":"2025-12-16T10:18:18.000Z","updated":"2025-12-16T10:36:01.929Z","comments":true,"path":"api/articles/CentOS7加装GPU后IB网卡自动恢复教程.json","excerpt":null,"keywords":null,"cover":null,"content":"<p>在集群运维中，IB（InfiniBand）网卡是高速数据传输的核心，但很多人遇到过这样的坑：给服务器加装 GPU 扩展卡后，原本正常的 ib0 设备突然消失，无法通过 IB 交换机与其他节点通信。本文结合实际案例，详细拆解问题根源，提供一步到位的自动修复方案，让 ib0 开机即启，无需手动干预。</p>\n<h1 id=\"一、问题背景\"><a href=\"#一、问题背景\" class=\"headerlink\" title=\"一、问题背景\"></a>一、问题背景</h1><ul>\n<li><p><strong>环境</strong>：CentOS 7 服务器（node1），搭载 Mellanox ConnectX 系列双模网卡（支持 IB&#x2F;ETH 模式），原本已配置 IB 模式（ib0 设备），与其他节点通过 IB 交换机实现 20.20.20.0&#x2F;24 网段高速通信。</p>\n</li>\n<li><p><strong>触发条件</strong>：为提升算力加装 GPU 扩展卡后，执行 <code>ip link show ib0</code> 提示 “Device ‘ib0’ does not exist”，IB 网卡失效。</p>\n</li>\n<li><p><strong>核心需求</strong>：恢复 ib0 设备，保持 IB 网段（20.20.20.0&#x2F;24）连通性，实现开机自动配置，不影响 GPU 正常工作。</p>\n</li>\n</ul>\n<h1 id=\"二、问题根源深度解析\"><a href=\"#二、问题根源深度解析\" class=\"headerlink\" title=\"二、问题根源深度解析\"></a>二、问题根源深度解析</h1><p>加装 GPU 后 IB 网卡失效，并非硬件损坏，而是<strong>多重连锁问题</strong>导致：</p>\n<ol>\n<li><p><strong>PCIe 资源冲突</strong>：GPU 占用大量 PCIe 通道，可能触发 BIOS 重置 PCIe 配置，或导致 IB 网卡的 PCIe 资源被占用，驱动加载异常。</p>\n</li>\n<li><p><strong>MLX_OFED 驱动冲突</strong>：服务器原本依赖 Mellanox 官方 OFED 驱动实现 IB 功能，加装 GPU 后，系统原生 RDMA 模块（rdma_cm&#x2F;rdma_ucm）与 OFED 驱动冲突，导致 openibd 服务启动失败（状态码 3&#x2F;NOTIMPLEMENTED），无法初始化 IB 设备。</p>\n</li>\n<li><p><strong>IB 模块未自动加载</strong>：openibd 服务失效后，MLX5 IB 核心模块（mlx5_ib）、IB 协议模块（ib_ipoib 等）无法开机自动加载，IB 网卡虽被识别但未生成 ib0 设备。</p>\n</li>\n</ol>\n<h1 id=\"三、解决方案：systemd-服务实现-ib0-自动初始化\"><a href=\"#三、解决方案：systemd-服务实现-ib0-自动初始化\" class=\"headerlink\" title=\"三、解决方案：systemd 服务实现 ib0 自动初始化\"></a>三、解决方案：systemd 服务实现 ib0 自动初始化</h1><p>无需修复复杂的驱动冲突，也不用手动执行命令，通过创建 systemd 服务，实现 “加载模块→创建 ib0→配置 IP” 全流程自动化，重启后依然生效。</p>\n<h2 id=\"步骤-1：创建-systemd-服务文件\"><a href=\"#步骤-1：创建-systemd-服务文件\" class=\"headerlink\" title=\"步骤 1：创建 systemd 服务文件\"></a>步骤 1：创建 systemd 服务文件</h2><p>创建<code>ib0-init.service</code>服务，适配 CentOS 7 老版本语法，避免参数报错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo vi /etc/systemd/system/ib0-init.service</span><br></pre></td></tr></table></figure>\n\n<p>粘贴以下内容（核心兼容老版本 iproute2，无<code>parent</code>参数）：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[Unit]</span><br><span class=\"line\">Description=Initialize IB0 device (CentOS 7 iproute2 compatible)</span><br><span class=\"line\">After=network.target  # 确保网络服务启动后再执行</span><br><span class=\"line\"></span><br><span class=\"line\">[Service]</span><br><span class=\"line\">Type=oneshot  # 一次性执行，执行完保持状态</span><br><span class=\"line\"># 核心命令：加载IB模块→创建ib0→启用设备→配置20网段IP</span><br><span class=\"line\">ExecStart=/bin/bash -c &quot;modprobe mlx5_ib; modprobe ib_core; modprobe ib_cm; modprobe ib_ipoib; ip link add ib0 type ipoib; ip link set ib0 up; ip addr add 20.20.20.109/24 dev ib0&quot;</span><br><span class=\"line\"># 停止命令：删除ib0→卸载模块（可选）</span><br><span class=\"line\">ExecStop=/bin/bash -c &quot;ip link delete ib0; modprobe -r ib_ipoib ib_cm ib_core mlx5_ib&quot;</span><br><span class=\"line\">RemainAfterExit=yes  # 执行完后保持服务活跃状态</span><br><span class=\"line\"></span><br><span class=\"line\">[Install]</span><br><span class=\"line\">WantedBy=multi-user.target  # 多用户模式下生效</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>说明：<code>20.20.20.101/24</code> 需替换为你的 IB 网段 IP（与其他节点保持同网段）；若集群 IB 分区使用 PKEY（如 0x8001），可在<code>ip link add ib0 type ipoib</code>后添加<code>pkey 0x8001</code>。</li>\n</ul>\n<h2 id=\"步骤-2：启用并启动服务\"><a href=\"#步骤-2：启用并启动服务\" class=\"headerlink\" title=\"步骤 2：启用并启动服务\"></a>步骤 2：启用并启动服务</h2><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 重新加载systemd配置，让新服务生效</span><br><span class=\"line\">sudo systemctl daemon-reload</span><br><span class=\"line\"></span><br><span class=\"line\"># 设置开机自启（关键！重启后自动执行）</span><br><span class=\"line\">sudo systemctl enable ib0-init.service</span><br><span class=\"line\"></span><br><span class=\"line\"># 立即启动服务，无需重启</span><br><span class=\"line\">sudo systemctl start ib0-init.service</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"步骤-3：验证-ib0-是否创建成功\"><a href=\"#步骤-3：验证-ib0-是否创建成功\" class=\"headerlink\" title=\"步骤 3：验证 ib0 是否创建成功\"></a>步骤 3：验证 ib0 是否创建成功</h2><p>执行以下命令，确认配置生效：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># 1. 检查ib0设备是否存在（状态UP为正常）</span><br><span class=\"line\">ip link show ib0</span><br><span class=\"line\"># 预期输出：</span><br><span class=\"line\"># 4: ib0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 4096 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000</span><br><span class=\"line\">#    link/ib 00:00:00:00:00:00:00:00 brd 00:00:00:00:00:00:00:00</span><br><span class=\"line\"></span><br><span class=\"line\"># 2. 检查IB网段IP是否配置成功</span><br><span class=\"line\">ip addr show ib0 | grep 20.20.20.</span><br><span class=\"line\"># 预期输出：inet 20.20.20.101/24 scope global ib0</span><br><span class=\"line\"></span><br><span class=\"line\"># 3. 测试与其他IB节点的连通性（如node2的20.20.20.107）</span><br><span class=\"line\">ping 20.20.20.107 -c 3</span><br><span class=\"line\"># 输出“3 packets transmitted, 3 received”即为连通正常</span><br><span class=\"line\"></span><br><span class=\"line\"># 4. 验证NFS等依赖IB的服务（若有）</span><br><span class=\"line\">sudo mount -a</span><br><span class=\"line\">df -h /share  # 确认IB网段NFS挂载正常</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"步骤-4：重启验证永久生效\"><a href=\"#步骤-4：重启验证永久生效\" class=\"headerlink\" title=\"步骤 4：重启验证永久生效\"></a>步骤 4：重启验证永久生效</h2><p>为确保重启后不失效，执行重启测试：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo reboot</span><br></pre></td></tr></table></figure>\n\n<p>重启后再次执行<code>ip link show ib0</code>和<code>ping ``20.20.20.101</code>，若 ib0 存在且连通正常，说明服务配置成功。</p>\n<h1 id=\"四、关键补充：避坑指南\"><a href=\"#四、关键补充：避坑指南\" class=\"headerlink\" title=\"四、关键补充：避坑指南\"></a>四、关键补充：避坑指南</h1><ol>\n<li><p><strong>模块加载顺序不能乱</strong>：必须先加载<code>mlx5_ib</code>（MLX5 网卡专用 IB 模块），再加载<code>ib_core</code>等协议模块，否则会导致设备创建失败。</p>\n</li>\n<li><p><strong>若 ib0 仍未创建</strong>：</p>\n</li>\n</ol>\n<ul>\n<li><p>执行<code>ibv_devinfo</code>检查物理 IB 设备是否存在（如 mlx5_0），若无输出，需重新安装 Mellanox OFED 驱动（兼容 GPU 驱动版本）。</p>\n</li>\n<li><p>排查 IB 线缆是否插紧、IB 交换机对应端口是否启用（物理链路中断也会导致 ib0 创建失败）。</p>\n</li>\n</ul>\n<ol>\n<li><p><strong>GPU 与 IB 网卡资源冲突</strong>：若重启后 GPU 失效，需进入 BIOS 调整 PCIe 通道分配，将 IB 网卡和 GPU 分配到不同的 PCIe 根复合体，避免资源竞争。</p>\n</li>\n<li><p><strong>兜底方案</strong>：若 IB 模式始终无法恢复，可沿用以太网转发方案（无需 ib0）：</p>\n</li>\n</ol>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">echo &quot;20.20.20.0/24 via 10.20.0.1 dev eno1 metric 200&quot; &gt;&gt; /etc/sysconfig/network-scripts/route-eno1</span><br><span class=\"line\"></span><br><span class=\"line\">sudo systemctl restart network</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"五、总结\"><a href=\"#五、总结\" class=\"headerlink\" title=\"五、总结\"></a>五、总结</h1><p>加装 GPU 后 IB 网卡失效的核心是 “驱动冲突 + 模块未自动加载”，通过 systemd 服务可完美解决：无需修改 BIOS、不卸载 GPU 驱动，仅需 3 步配置，即可实现 ib0 开机自动创建、IP 自动配置，恢复 IB 高速传输功能。</p>\n<p>该方案已在生产环境验证，适配 CentOS 7 所有版本，兼容 Mellanox ConnectX-4&#x2F;5&#x2F;6 系列网卡。如果遇到类似问题，欢迎留言交流，分享你的排查经验！</p>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"Linux","path":"api/tags/Linux.json"},{"name":"系统","path":"api/tags/系统.json"}]},{"title":"CentOS7.9 Ollama GPU 调用血泪史！从编译失败到 Docker 部署全攻略","slug":"CentOS7.9 Ollama GPU 调用血泪史！从编译失败到 Docker 部署全攻略","date":"2025-12-18T11:18:18.000Z","updated":"2025-12-18T11:21:09.255Z","comments":true,"path":"api/articles/CentOS7.9 Ollama GPU 调用血泪史！从编译失败到 Docker 部署全攻略.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/fef2/fig1.jpg","content":"<p>📝作为 CentOS7.9 老用户，想跑 Ollama 大模型却被狠狠拿捏：glibc 版本低、编译后 GPU 不识别、装了 CUDA 也白搭… 折腾一周终于打通全流程，把每个坑和解决方案都扒得明明白白，同款环境的朋友直接抄作业！</p>\n<h1 id=\"🚨-事情的来龙去脉：从编译成功到-GPU-卡壳\"><a href=\"#🚨-事情的来龙去脉：从编译成功到-GPU-卡壳\" class=\"headerlink\" title=\"🚨 事情的来龙去脉：从编译成功到 GPU 卡壳\"></a>🚨 事情的来龙去脉：从编译成功到 GPU 卡壳</h1><h2 id=\"阶段-1：glibc-太低，直接装-Ollama-没戏\"><a href=\"#阶段-1：glibc-太低，直接装-Ollama-没戏\" class=\"headerlink\" title=\"阶段 1：glibc 太低，直接装 Ollama 没戏\"></a>阶段 1：glibc 太低，直接装 Ollama 没戏</h2><p>CentOS7.9 默认 glibc 2.17，而 Ollama GPU 版要求 glibc≥2.27，升级系统 glibc 怕搞崩 yum、bash 等核心工具（血的教训：千万别硬升！）。</p>\n<p>没办法只能曲线救国 —— 用 Go 手动编译 Ollama 源码，一顿操作后终于能运行，但新问题来了：<strong>模型生成速度慢到离谱</strong>，nvidia-smi 一看，GPU 利用率始终 0%，全程 CPU 在硬扛！</p>\n<h2 id=\"阶段-2：装-CUDA-NCCL，依旧无法唤醒-GPU\"><a href=\"#阶段-2：装-CUDA-NCCL，依旧无法唤醒-GPU\" class=\"headerlink\" title=\"阶段 2：装 CUDA+NCCL，依旧无法唤醒 GPU\"></a>阶段 2：装 CUDA+NCCL，依旧无法唤醒 GPU</h2><p>以为是缺 GPU 依赖，火速安排：</p>\n<ul>\n<li><p>装了 CUDA Toolkit 12.4 Update 1，nvcc -V 能看到版本（centOS 7.9 最高支持 CUDA Toolkit 12.4 Update 1）；</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda-repo-rhel7-12-4-local-12.4.1_550.54.15-1.x86_64.rpm</span><br><span class=\"line\"><span class=\"variable\">$sudo</span> rpm -i cuda-repo-rhel7-12-4-local-12.4.1_550.54.15-1.x86_64.rpm</span><br><span class=\"line\"><span class=\"variable\">$sudo</span> yum clean all</span><br><span class=\"line\"><span class=\"variable\">$sudo</span> yum -y install cuda-toolkit-12-4</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>补了 NVIDIA NCCL 库，ls &#x2F;usr&#x2F;lib64&#x2F;libnccl.so * 也能看到文件（NCCL 2.21.5, for CUDA 12.4, April 3rd, 2024）；</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo</span><br><span class=\"line\">yum install libnccl-2.21.5-1+cuda12.4 libnccl-devel-2.21.5-1+cuda12.4 libnccl-static-2.21.5-1+cuda12.4</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>配置了 LD_LIBRARY_PATH，重启 Ollama… 结果还是 CPU 在跑！</p>\n</li>\n</ul>\n<p>排查发现：编译后的 Ollama 和 CentOS7.9 的 glibc 环境不兼容，就算装了 CUDA&#x2F;NCCL，也无法加载 GPU 驱动链路 —— 这才意识到，隔离环境才是唯一出路！</p>\n<h1 id=\"✅-破局关键：Docker-国内镜像，一步解决所有问题\"><a href=\"#✅-破局关键：Docker-国内镜像，一步解决所有问题\" class=\"headerlink\" title=\"✅ 破局关键：Docker + 国内镜像，一步解决所有问题\"></a>✅ 破局关键：Docker + 国内镜像，一步解决所有问题</h1><p>试过阿里云、网易云镜像，拉取 Ollama 官方镜像还是超时，直到发现「<a href=\"https://docker.1ms.run/\">https://docker.1ms.run</a>」国内镜像，速度直接起飞！配合 NVIDIA 容器工具包，完美绕开 glibc 限制，GPU 瞬间被唤醒～</p>\n<h2 id=\"Step1：清理之前的-“烂摊子”（避免冲突）\"><a href=\"#Step1：清理之前的-“烂摊子”（避免冲突）\" class=\"headerlink\" title=\"Step1：清理之前的 “烂摊子”（避免冲突）\"></a>Step1：清理之前的 “烂摊子”（避免冲突）</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 停止手动编译的Ollama进程（避免端口占用）</span></span><br><span class=\"line\">pkill -9 ollama</span><br><span class=\"line\">ps -ef | grep ollama | grep -v grep  <span class=\"comment\"># 确认无残留</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 卸载可能冲突的旧容器（若之前试过Docker）</span></span><br><span class=\"line\">docker stop ollama-gpu 2&gt;/dev/null || <span class=\"literal\">true</span></span><br><span class=\"line\">docker <span class=\"built_in\">rm</span> ollama-gpu 2&gt;/dev/null || <span class=\"literal\">true</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Step2：配置-Docker-国内镜像源（解决拉取超时）\"><a href=\"#Step2：配置-Docker-国内镜像源（解决拉取超时）\" class=\"headerlink\" title=\"Step2：配置 Docker 国内镜像源（解决拉取超时）\"></a>Step2：配置 Docker 国内镜像源（解决拉取超时）</h2><p>核心是用「<a href=\"https://docker.1ms.run/\">https://docker.1ms.run</a>」，国内拉取 Ollama 镜像不超时：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 停止Docker服务</span></span><br><span class=\"line\">systemctl stop docker</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 重建daemon.json（避免语法错误）</span></span><br><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&#x27;&#123;</span></span><br><span class=\"line\"><span class=\"string\">  &quot;registry-mirrors&quot;: [&quot;https://docker.1ms.run&quot;]</span></span><br><span class=\"line\"><span class=\"string\">&#125;&#x27;</span> &gt; /etc/docker/daemon.json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 验证JSON语法（无输出=正确）</span></span><br><span class=\"line\">python -m json.tool /etc/docker/daemon.json</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 重启Docker并验证镜像源</span></span><br><span class=\"line\">systemctl daemon-reload &amp;&amp; systemctl start docker</span><br><span class=\"line\">docker info | grep <span class=\"string\">&quot;Registry Mirrors&quot;</span>  <span class=\"comment\"># 输出含1ms.run即生效</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Step3：安装-NVIDIA-Container-Toolkit（GPU-识别关键！）\"><a href=\"#Step3：安装-NVIDIA-Container-Toolkit（GPU-识别关键！）\" class=\"headerlink\" title=\"Step3：安装 NVIDIA Container Toolkit（GPU 识别关键！）\"></a>Step3：安装 NVIDIA Container Toolkit（GPU 识别关键！）</h2><p>这是 Docker 能调用 GPU 的核心，之前踩坑就是漏了这步：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加NVIDIA官方源（适配CentOS7）</span></span><br><span class=\"line\">distribution=$(. /etc/os-release;<span class=\"built_in\">echo</span> $ID<span class=\"variable\">$VERSION_ID</span>)</span><br><span class=\"line\">curl -s -L https://nvidia.github.io/libnvidia-container/<span class=\"variable\">$distribution</span>/libnvidia-container.repo | <span class=\"built_in\">tee</span> /etc/yum.repos.d/nvidia-container-toolkit.repo</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 安装工具包</span></span><br><span class=\"line\">yum install -y nvidia-container-toolkit-base nvidia-container-toolkit</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 配置Docker识别NVIDIA驱动</span></span><br><span class=\"line\">nvidia-ctk runtime configure --runtime=docker</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 重启Docker</span></span><br><span class=\"line\">systemctl restart docker</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Step4：启动-Ollama-GPU-容器（长期稳定运行）\"><a href=\"#Step4：启动-Ollama-GPU-容器（长期稳定运行）\" class=\"headerlink\" title=\"Step4：启动 Ollama GPU 容器（长期稳定运行）\"></a>Step4：启动 Ollama GPU 容器（长期稳定运行）</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 启动容器（参数全解析，少一个都可能出问题）</span></span><br><span class=\"line\">docker run -d \\</span><br><span class=\"line\">  --name ollama-gpu \\</span><br><span class=\"line\">  --restart=always \\  <span class=\"comment\"># 意外退出/服务器重启自动恢复</span></span><br><span class=\"line\">  --gpus=all \\        <span class=\"comment\"># 启用所有GPU（我这里是4块L4）</span></span><br><span class=\"line\">  -p 0.0.0.0:11434:11434 \\  <span class=\"comment\"># 对外开放API，外部可调用</span></span><br><span class=\"line\">  -v /share/public_db/ollama/models:/root/.ollama \\  <span class=\"comment\"># 模型持久化（删容器不丢模型）</span></span><br><span class=\"line\">  -e OLLAMA_GPU=100 \\  <span class=\"comment\"># 100%使用GPU显存（可按需调）</span></span><br><span class=\"line\">  -e OLLAMA_HOST=0.0.0.0:11434 \\  <span class=\"comment\"># 允许外部访问API</span></span><br><span class=\"line\">  docker.1ms.run/ollama/ollama:latest  <span class=\"comment\"># 国内镜像拉取，不超时</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Step5：验证-GPU-是否真正被调用\"><a href=\"#Step5：验证-GPU-是否真正被调用\" class=\"headerlink\" title=\"Step5：验证 GPU 是否真正被调用\"></a>Step5：验证 GPU 是否真正被调用</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 新开终端监控GPU状态</span></span><br><span class=\"line\">watch -n 1 nvidia-smi</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 进入容器运行模型（测试GPU占用）</span></span><br><span class=\"line\">docker <span class=\"built_in\">exec</span> -it ollama-gpu ollama run qwen3-vl:32b</span><br></pre></td></tr></table></figure>\n\n<p>此时能看到 nvidia-smi 中 GPU 显存从 1MiB 飙升到数 GB，GPU-Util≥10%—— 终于不是 CPU 在 “孤军奋战” 了！</p>\n<h1 id=\"📡-API-调用示例（本地-远程都能用）\"><a href=\"#📡-API-调用示例（本地-远程都能用）\" class=\"headerlink\" title=\"📡 API 调用示例（本地 &#x2F; 远程都能用）\"></a>📡 API 调用示例（本地 &#x2F; 远程都能用）</h1><p>容器启动后，无需手动进容器操作，外部程序直接通过 HTTP 调用，完美集成到自己的项目中～</p>\n<h2 id=\"1-curl-快速测试（无需写代码）\"><a href=\"#1-curl-快速测试（无需写代码）\" class=\"headerlink\" title=\"1. curl 快速测试（无需写代码）\"></a>1. curl 快速测试（无需写代码）</h2><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看已下载的模型</span></span><br><span class=\"line\">curl http://你的宿主机IP:11434/api/tags</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调用模型生成内容（以历史故事为例）</span></span><br><span class=\"line\">curl http://192.168.1.100:11434/api/generate -d <span class=\"string\">&#x27;&#123;</span></span><br><span class=\"line\"><span class=\"string\">  &quot;model&quot;: &quot;qwen3-vl:32b&quot;,</span></span><br><span class=\"line\"><span class=\"string\">  &quot;prompt&quot;: &quot;生成一篇关于宋朝苏轼的历史故事，风格儿童趣味，300字左右&quot;,</span></span><br><span class=\"line\"><span class=\"string\">  &quot;stream&quot;: false,  # 非流式返回，方便程序处理</span></span><br><span class=\"line\"><span class=\"string\">  &quot;temperature&quot;: 0.6,  # 控制随机性</span></span><br><span class=\"line\"><span class=\"string\">  &quot;max_tokens&quot;: 1500</span></span><br><span class=\"line\"><span class=\"string\">&#125;&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"2-Python-封装（集成到项目中）\"><a href=\"#2-Python-封装（集成到项目中）\" class=\"headerlink\" title=\"2. Python 封装（集成到项目中）\"></a>2. Python 封装（集成到项目中）</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">from</span> typing <span class=\"keyword\">import</span> <span class=\"type\">Optional</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">OllamaGPUClient</span>:</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, host: <span class=\"built_in\">str</span> = <span class=\"string\">&quot;localhost&quot;</span>, port: <span class=\"built_in\">int</span> = <span class=\"number\">11434</span></span>):</span><br><span class=\"line\">        self.base_url = <span class=\"string\">f&quot;http://<span class=\"subst\">&#123;host&#125;</span>:<span class=\"subst\">&#123;port&#125;</span>/api/generate&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">generate_content</span>(<span class=\"params\">self, model: <span class=\"built_in\">str</span>, prompt: <span class=\"built_in\">str</span></span>) -&gt; <span class=\"type\">Optional</span>[<span class=\"built_in\">str</span>]:</span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            response = requests.post(</span><br><span class=\"line\">                self.base_url,</span><br><span class=\"line\">                json=&#123;</span><br><span class=\"line\">                    <span class=\"string\">&quot;model&quot;</span>: model,</span><br><span class=\"line\">                    <span class=\"string\">&quot;prompt&quot;</span>: prompt,</span><br><span class=\"line\">                    <span class=\"string\">&quot;stream&quot;</span>: <span class=\"literal\">False</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;temperature&quot;</span>: <span class=\"number\">0.7</span>,</span><br><span class=\"line\">                    <span class=\"string\">&quot;max_tokens&quot;</span>: <span class=\"number\">2000</span></span><br><span class=\"line\">                &#125;,</span><br><span class=\"line\">                timeout=<span class=\"number\">60</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            response.raise_for_status()</span><br><span class=\"line\">            <span class=\"keyword\">return</span> response.json()[<span class=\"string\">&quot;response&quot;</span>]</span><br><span class=\"line\">        <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;调用失败：<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用示例</span></span><br><span class=\"line\">client = OllamaGPUClient(host=<span class=\"string\">&quot;192.168.1.100&quot;</span>)  <span class=\"comment\"># 替换为你的宿主机IP</span></span><br><span class=\"line\">result = client.generate_content(</span><br><span class=\"line\">    model=<span class=\"string\">&quot;qwen3-vl:32b&quot;</span>,</span><br><span class=\"line\">    prompt=<span class=\"string\">&quot;解释什么是量子计算，用通俗的语言&quot;</span></span><br><span class=\"line\">)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"3-完整例子\"><a href=\"#3-完整例子\" class=\"headerlink\" title=\"3. 完整例子\"></a>3. 完整例子</h2><p>历史故事Agent</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br><span class=\"line\">133</span><br><span class=\"line\">134</span><br><span class=\"line\">135</span><br><span class=\"line\">136</span><br><span class=\"line\">137</span><br><span class=\"line\">138</span><br><span class=\"line\">139</span><br><span class=\"line\">140</span><br><span class=\"line\">141</span><br><span class=\"line\">142</span><br><span class=\"line\">143</span><br><span class=\"line\">144</span><br><span class=\"line\">145</span><br><span class=\"line\">146</span><br><span class=\"line\">147</span><br><span class=\"line\">148</span><br><span class=\"line\">149</span><br><span class=\"line\">150</span><br><span class=\"line\">151</span><br><span class=\"line\">152</span><br><span class=\"line\">153</span><br><span class=\"line\">154</span><br><span class=\"line\">155</span><br><span class=\"line\">156</span><br><span class=\"line\">157</span><br><span class=\"line\">158</span><br><span class=\"line\">159</span><br><span class=\"line\">160</span><br><span class=\"line\">161</span><br><span class=\"line\">162</span><br><span class=\"line\">163</span><br><span class=\"line\">164</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> requests</span><br><span class=\"line\"><span class=\"keyword\">import</span> json</span><br><span class=\"line\"><span class=\"keyword\">import</span> argparse</span><br><span class=\"line\"><span class=\"keyword\">from</span> typing <span class=\"keyword\">import</span> <span class=\"type\">Optional</span>, <span class=\"type\">Dict</span>, <span class=\"type\">Any</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">HistoryStoryAgent</span>:</span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;历史故事生成Agent，调用ollama API使用qwen3-vl:32b生成历史故事&quot;&quot;&quot;</span></span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, ollama_host: <span class=\"built_in\">str</span> = <span class=\"string\">&quot;localhost&quot;</span>, ollama_port: <span class=\"built_in\">int</span> = <span class=\"number\">11434</span></span>):</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        初始化Agent</span></span><br><span class=\"line\"><span class=\"string\">        :param ollama_host: ollama容器所在的宿主机IP/域名</span></span><br><span class=\"line\"><span class=\"string\">        :param ollama_port: ollama API端口（默认11434）</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        self.base_url = <span class=\"string\">f&quot;http://<span class=\"subst\">&#123;ollama_host&#125;</span>:<span class=\"subst\">&#123;ollama_port&#125;</span>&quot;</span></span><br><span class=\"line\">        self.model_name = <span class=\"string\">&quot;qwen3-vl:32b&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 检查ollama服务是否可用</span></span><br><span class=\"line\">        self._check_ollama_health()</span><br><span class=\"line\">        <span class=\"comment\"># 检查模型是否已下载</span></span><br><span class=\"line\">        self._check_model_exists()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_check_ollama_health</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;检查ollama API服务是否正常&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            response = requests.get(<span class=\"string\">f&quot;<span class=\"subst\">&#123;self.base_url&#125;</span>/api/tags&quot;</span>, timeout=<span class=\"number\">5</span>)</span><br><span class=\"line\">            <span class=\"keyword\">if</span> response.status_code != <span class=\"number\">200</span>:</span><br><span class=\"line\">                <span class=\"keyword\">raise</span> ConnectionError(<span class=\"string\">f&quot;ollama服务异常，状态码：<span class=\"subst\">&#123;response.status_code&#125;</span>&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">except</span> requests.exceptions.ConnectionError:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> ConnectionError(</span><br><span class=\"line\">                <span class=\"string\">f&quot;无法连接到ollama服务，请检查：\\n&quot;</span></span><br><span class=\"line\">                <span class=\"string\">f&quot;1. ollama容器是否运行（docker ps | grep ollama-gpu）\\n&quot;</span></span><br><span class=\"line\">                <span class=\"string\">f&quot;2. 宿主机IP/端口是否正确（当前配置：<span class=\"subst\">&#123;self.base_url&#125;</span>）\\n&quot;</span></span><br><span class=\"line\">                <span class=\"string\">f&quot;3. 防火墙是否开放<span class=\"subst\">&#123;self.base_url.split(<span class=\"string\">&#x27;:&#x27;</span>)[-<span class=\"number\">1</span>]&#125;</span>端口&quot;</span></span><br><span class=\"line\">            )</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">_check_model_exists</span>(<span class=\"params\">self</span>) -&gt; <span class=\"literal\">None</span>:</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;检查qwen3-vl:32b模型是否已下载&quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            response = requests.get(<span class=\"string\">f&quot;<span class=\"subst\">&#123;self.base_url&#125;</span>/api/tags&quot;</span>, timeout=<span class=\"number\">5</span>)</span><br><span class=\"line\">            models = [m[<span class=\"string\">&quot;name&quot;</span>] <span class=\"keyword\">for</span> m <span class=\"keyword\">in</span> response.json()[<span class=\"string\">&quot;models&quot;</span>]]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> self.model_name <span class=\"keyword\">not</span> <span class=\"keyword\">in</span> models:</span><br><span class=\"line\">                <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;⚠️ 未检测到<span class=\"subst\">&#123;self.model_name&#125;</span>模型，正在自动下载（首次下载可能耗时较长）...&quot;</span>)</span><br><span class=\"line\">                <span class=\"comment\"># 调用API下载模型</span></span><br><span class=\"line\">                download_resp = requests.post(</span><br><span class=\"line\">                    <span class=\"string\">f&quot;<span class=\"subst\">&#123;self.base_url&#125;</span>/api/pull&quot;</span>,</span><br><span class=\"line\">                    json=&#123;<span class=\"string\">&quot;name&quot;</span>: self.model_name&#125;,</span><br><span class=\"line\">                    stream=<span class=\"literal\">True</span>,</span><br><span class=\"line\">                    timeout=<span class=\"number\">3600</span>  <span class=\"comment\"># 下载超时设为1小时</span></span><br><span class=\"line\">                )</span><br><span class=\"line\">                <span class=\"comment\"># 打印下载进度</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> download_resp.iter_lines():</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> line:</span><br><span class=\"line\">                        data = json.loads(line)</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> <span class=\"string\">&quot;status&quot;</span> <span class=\"keyword\">in</span> data:</span><br><span class=\"line\">                            <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;下载进度：<span class=\"subst\">&#123;data[<span class=\"string\">&#x27;status&#x27;</span>]&#125;</span>&quot;</span>, end=<span class=\"string\">&quot;\\r&quot;</span>)</span><br><span class=\"line\">                <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;\\n✅ <span class=\"subst\">&#123;self.model_name&#125;</span>模型下载完成！&quot;</span>)</span><br><span class=\"line\">        <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">            <span class=\"keyword\">raise</span> RuntimeError(<span class=\"string\">f&quot;检查/下载模型失败：<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">generate_story</span>(<span class=\"params\"></span></span><br><span class=\"line\"><span class=\"params\">        self,</span></span><br><span class=\"line\"><span class=\"params\">        topic: <span class=\"built_in\">str</span>,</span></span><br><span class=\"line\"><span class=\"params\">        style: <span class=\"built_in\">str</span> = <span class=\"string\">&quot;通俗易懂&quot;</span>,</span></span><br><span class=\"line\"><span class=\"params\">        length: <span class=\"built_in\">str</span> = <span class=\"string\">&quot;中等（500字左右）&quot;</span>,</span></span><br><span class=\"line\"><span class=\"params\">        stream: <span class=\"built_in\">bool</span> = <span class=\"literal\">False</span></span></span><br><span class=\"line\"><span class=\"params\">    </span>) -&gt; <span class=\"type\">Optional</span>[<span class=\"built_in\">str</span>]:</span><br><span class=\"line\">        <span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        生成历史故事</span></span><br><span class=\"line\"><span class=\"string\">        :param topic: 故事主题（如：唐朝李白、三国赤壁之战、宋朝苏轼）</span></span><br><span class=\"line\"><span class=\"string\">        :param style: 故事风格（如：通俗易懂、文言文、儿童趣味）</span></span><br><span class=\"line\"><span class=\"string\">        :param length: 故事长度（如：简短（200字）、中等（500字）、详细（1000字））</span></span><br><span class=\"line\"><span class=\"string\">        :param stream: 是否流式返回（True=逐字输出，False=一次性返回）</span></span><br><span class=\"line\"><span class=\"string\">        :return: 生成的历史故事字符串</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\">        <span class=\"comment\"># 构造高质量prompt（关键：引导模型生成结构化、准确的内容）</span></span><br><span class=\"line\">        prompt = <span class=\"string\">f&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">        请你作为专业的历史老师，根据以下要求生成一篇历史故事：</span></span><br><span class=\"line\"><span class=\"string\">        1. 主题：<span class=\"subst\">&#123;topic&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">        2. 风格：<span class=\"subst\">&#123;style&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">        3. 长度：<span class=\"subst\">&#123;length&#125;</span></span></span><br><span class=\"line\"><span class=\"string\">        4. 输出格式：</span></span><br><span class=\"line\"><span class=\"string\">           - 标题：[故事标题]</span></span><br><span class=\"line\"><span class=\"string\">           - 正文：[生动的历史故事内容，确保史实准确，避免虚构]</span></span><br><span class=\"line\"><span class=\"string\">           - 关键知识点：[列出1-3个与故事相关的核心历史知识点]</span></span><br><span class=\"line\"><span class=\"string\">        5. 要求：内容准确、逻辑清晰、语言流畅，符合指定风格和长度。</span></span><br><span class=\"line\"><span class=\"string\">        &quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 构造API请求参数</span></span><br><span class=\"line\">        payload = &#123;</span><br><span class=\"line\">            <span class=\"string\">&quot;model&quot;</span>: self.model_name,</span><br><span class=\"line\">            <span class=\"string\">&quot;prompt&quot;</span>: prompt.strip(),</span><br><span class=\"line\">            <span class=\"string\">&quot;stream&quot;</span>: stream,</span><br><span class=\"line\">            <span class=\"string\">&quot;temperature&quot;</span>: <span class=\"number\">0.7</span>,  <span class=\"comment\"># 控制生成随机性（0=严谨，1=灵活）</span></span><br><span class=\"line\">            <span class=\"string\">&quot;max_tokens&quot;</span>: <span class=\"number\">2000</span>    <span class=\"comment\"># 最大生成字符数</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">try</span>:</span><br><span class=\"line\">            <span class=\"comment\"># 调用ollama generate API</span></span><br><span class=\"line\">            response = requests.post(</span><br><span class=\"line\">                <span class=\"string\">f&quot;<span class=\"subst\">&#123;self.base_url&#125;</span>/api/generate&quot;</span>,</span><br><span class=\"line\">                json=payload,</span><br><span class=\"line\">                stream=stream,</span><br><span class=\"line\">                timeout=<span class=\"number\">60</span>  <span class=\"comment\"># 生成超时设为60秒</span></span><br><span class=\"line\">            )</span><br><span class=\"line\">            response.raise_for_status()  <span class=\"comment\"># 抛出HTTP错误</span></span><br><span class=\"line\"></span><br><span class=\"line\">            <span class=\"comment\"># 处理响应</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> stream:</span><br><span class=\"line\">                <span class=\"comment\"># 流式返回（逐字输出，适合实时展示）</span></span><br><span class=\"line\">                <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n📜 正在生成历史故事（流式输出）：\\n&quot;</span>)</span><br><span class=\"line\">                story = <span class=\"string\">&quot;&quot;</span></span><br><span class=\"line\">                <span class=\"keyword\">for</span> line <span class=\"keyword\">in</span> response.iter_lines():</span><br><span class=\"line\">                    <span class=\"keyword\">if</span> line:</span><br><span class=\"line\">                        data = json.loads(line)</span><br><span class=\"line\">                        <span class=\"keyword\">if</span> <span class=\"string\">&quot;response&quot;</span> <span class=\"keyword\">in</span> data:</span><br><span class=\"line\">                            story += data[<span class=\"string\">&quot;response&quot;</span>]</span><br><span class=\"line\">                            <span class=\"built_in\">print</span>(data[<span class=\"string\">&quot;response&quot;</span>], end=<span class=\"string\">&quot;&quot;</span>, flush=<span class=\"literal\">True</span>)</span><br><span class=\"line\">                            <span class=\"keyword\">if</span> data.get(<span class=\"string\">&quot;done&quot;</span>):</span><br><span class=\"line\">                                <span class=\"keyword\">break</span></span><br><span class=\"line\">                <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n&quot;</span>)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> story</span><br><span class=\"line\">            <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                <span class=\"comment\"># 非流式返回（一次性获取完整结果）</span></span><br><span class=\"line\">                result = response.json()</span><br><span class=\"line\">                <span class=\"keyword\">return</span> result[<span class=\"string\">&quot;response&quot;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">except</span> requests.exceptions.Timeout:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;❌ 生成超时，请缩短故事长度或检查模型响应速度&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\">        <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;❌ 生成故事失败：<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">None</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 命令行交互入口</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">main</span>():</span><br><span class=\"line\">    <span class=\"comment\"># 解析命令行参数</span></span><br><span class=\"line\">    parser = argparse.ArgumentParser(description=<span class=\"string\">&quot;历史故事生成Agent - 调用ollama qwen3-vl:32b模型&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--host&quot;</span>, default=<span class=\"string\">&quot;localhost&quot;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;ollama宿主机IP（默认localhost）&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--port&quot;</span>, <span class=\"built_in\">type</span>=<span class=\"built_in\">int</span>, default=<span class=\"number\">11434</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;ollama API端口（默认11434）&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--topic&quot;</span>, required=<span class=\"literal\">True</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;历史故事主题（如：唐朝李白、三国赤壁之战）&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--style&quot;</span>, default=<span class=\"string\">&quot;通俗易懂&quot;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;故事风格（如：儿童趣味、文言文）&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--length&quot;</span>, default=<span class=\"string\">&quot;中等（500字左右）&quot;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;故事长度&quot;</span>)</span><br><span class=\"line\">    parser.add_argument(<span class=\"string\">&quot;--stream&quot;</span>, action=<span class=\"string\">&quot;store_true&quot;</span>, <span class=\"built_in\">help</span>=<span class=\"string\">&quot;是否流式输出（默认否）&quot;</span>)</span><br><span class=\"line\">    </span><br><span class=\"line\">    args = parser.parse_args()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 初始化Agent并生成故事</span></span><br><span class=\"line\">    <span class=\"keyword\">try</span>:</span><br><span class=\"line\">        agent = HistoryStoryAgent(ollama_host=args.host, ollama_port=args.port)</span><br><span class=\"line\">        story = agent.generate_story(</span><br><span class=\"line\">            topic=args.topic,</span><br><span class=\"line\">            style=args.style,</span><br><span class=\"line\">            length=args.length,</span><br><span class=\"line\">            stream=args.stream</span><br><span class=\"line\">        )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> story:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n✅ 历史故事生成完成：\\n&quot;</span> + <span class=\"string\">&quot;-&quot;</span>*<span class=\"number\">50</span>)</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(story)</span><br><span class=\"line\">    <span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">        <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;❌ Agent初始化失败：<span class=\"subst\">&#123;<span class=\"built_in\">str</span>(e)&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">if</span> __name__ == <span class=\"string\">&quot;__main__&quot;</span>:</span><br><span class=\"line\">    main()</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>用法：</p>\n<ul>\n<li><p>本机调用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">python history_story_agent.py --topic <span class=\"string\">&quot;宋朝苏轼&quot;</span> --stream</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/fef2/fig1.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/fef2/fig1.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"本地调用API\"></p>\n</li>\n<li><p>远程调用</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 根据实际情况设置host</span></span><br><span class=\"line\">python history_story_agent.py --host 192.168.0.100 --port 11434 --topic <span class=\"string\">&quot;三国赤壁之战&quot;</span> --style <span class=\"string\">&quot;儿童趣味&quot;</span> --stream</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/fef2/fig2.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/fef2/fig2.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"远程调用API\"></p>\n</li>\n</ul>\n<h1 id=\"🚫-踩过的坑（避坑指南！）\"><a href=\"#🚫-踩过的坑（避坑指南！）\" class=\"headerlink\" title=\"🚫 踩过的坑（避坑指南！）\"></a>🚫 踩过的坑（避坑指南！）</h1><ol>\n<li><p>坑 1：硬升 glibc→系统崩溃</p>\n<p>解决方案：用 Docker 隔离环境，绕开系统 glibc 限制，绝对安全！</p>\n</li>\n<li><p>坑 2：装了 CUDA&#x2F;NCCL 还是不识别 GPU</p>\n<p>原因：编译后的 Ollama 和系统环境不兼容，依赖链路断裂</p>\n<p>解决方案：放弃本地编译，直接用 Docker 版 Ollama</p>\n</li>\n<li><p>坑 3：Docker 拉取 Ollama 镜像超时</p>\n<p>解决方案：用「<a href=\"https://docker.1ms.run/\">https://docker.1ms.run</a>」国内镜像，速度秒杀官方源</p>\n</li>\n<li><p>坑 4：Docker 启动后还是不识别 GPU</p>\n<p>原因：没装 NVIDIA Container Toolkit，Docker 无法调用 GPU 驱动</p>\n<p>解决方案：按 Step3 完整安装配置</p>\n</li>\n<li><p>坑 5：API 远程调用失败</p>\n<p>原因：容器没绑定 0.0.0.0，或防火墙没开放端口</p>\n<p>解决方案：启动容器加<code>-p ``0.0.0.0:11434``:11434</code>，开放端口<code>firewall-cmd --add-port=11434/tcp --permanent &amp;&amp; firewall-cmd --reload</code></p>\n</li>\n</ol>\n<h1 id=\"🎯-最终效果-总结\"><a href=\"#🎯-最终效果-总结\" class=\"headerlink\" title=\"🎯 最终效果 &amp; 总结\"></a>🎯 最终效果 &amp; 总结</h1><ul>\n<li><p>4 块 L4 GPU 满负荷运行，qwen3-vl:32b 生成 500 字内容从之前的 30 秒→现在 3 秒；</p>\n</li>\n<li><p>容器长期稳定运行，服务器重启后自动恢复，模型持久化不重复下载；</p>\n</li>\n<li><p>本地 &#x2F; 远程程序通过 API 轻松调用，完美集成到自己的项目中。</p>\n</li>\n</ul>\n<h1 id=\"核心结论\"><a href=\"#核心结论\" class=\"headerlink\" title=\"核心结论\"></a>核心结论</h1><ol>\n<li><p>CentOS7.9+Ollama GPU：Docker 是唯一稳定解，别折腾本地编译和 glibc 升级；</p>\n</li>\n<li><p>国内镜像优先选「<a href=\"https://docker.1ms.run/\">https://docker.1ms.run</a>」，解决 Ollama 镜像拉取超时的痛点；</p>\n</li>\n<li><p>NVIDIA Container Toolkit 是 GPU 识别的 “钥匙”，少一步都不行；</p>\n</li>\n<li><p>容器启动参数要配全：<code>--restart=always</code>+<code>OLLAMA_HOST=``0.0.0.0</code>+ 端口映射，兼顾稳定性和可访问性。</p>\n</li>\n</ol>\n<p>如果还有其他踩坑点，欢迎评论区交流～ 祝大家都能让 Ollama 跑满 GPU，告别 CPU 慢如蜗牛的日子！</p>\n<p>#Ollama #GPU 部署 #CentOS7 #Docker 避坑 #AI 工具 #程序员攻略 #大模型部署</p>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"Linux","path":"api/tags/Linux.json"},{"name":"系统","path":"api/tags/系统.json"},{"name":"Ollama","path":"api/tags/Ollama.json"}]},{"title":"WSL安装Docker避坑指北","slug":"WSL安装Docker避坑指北","date":"2022-03-10T02:43:42.000Z","updated":"2024-03-11T13:07:17.000Z","comments":true,"path":"api/articles/WSL安装Docker避坑指北.json","excerpt":null,"keywords":null,"cover":"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg","content":"<h1 id=\"升级WSL\"><a href=\"#升级WSL\" class=\"headerlink\" title=\"升级WSL\"></a>升级WSL</h1><ul>\n<li><p>查看已安装的WSL版本(Windows PowerShell中运行)</p>\n  <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl.exe -l -v</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>  显示如下：</p>\n  <figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">  NAME                   STATE           VERSION</span><br><span class=\"line\">* Ubuntu-20.04           Running         1</span><br><span class=\"line\">  docker-desktop-data    Stopped         2</span><br></pre></td></tr></table></figure>\n<p>  看到安装的为WSL1，据说WSL1不支持Docker，所以换成WSL2，至于WSL1 和WSL2的异同可参考<a href=\"https://docs.microsoft.com/zh-cn/windows/wsl/compare-versions\">比较 WSL 1 和 WSL 2</a>。</p>\n</li>\n<li><p>安装WSL2 (Windows PowerShell中运行)</p>\n  <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wsl.exe --set-version ubuntu-20.04 2</span><br></pre></td></tr></table></figure>\n<p>  显示如下：</p>\n  <figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">正在进行转换，这可能需要几分钟时间...</span><br><span class=\"line\">有关与 WSL 2 的主要区别的信息，请访问 https://aka.ms/wsl2</span><br><span class=\"line\">转换完成。</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"安装Docker\"><a href=\"#安装Docker\" class=\"headerlink\" title=\"安装Docker\"></a>安装Docker</h1><p>在WSL中运行如下命令：</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get remove docker docker-engine docker.io</span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt install apt-transport-https ca-certificates curl software-properties-common</span><br><span class=\"line\">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class=\"line\">sudo apt-key fingerprint 0EBFCD88</span><br><span class=\"line\">sudo add-apt-repository <span class=\"string\">&quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu <span class=\"subst\">$(lsb_release -cs)</span> stable&quot;</span></span><br><span class=\"line\">sudo apt update</span><br><span class=\"line\">sudo apt install docker-ce</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Root用户启动Docker\"><a href=\"#Root用户启动Docker\" class=\"headerlink\" title=\"Root用户启动Docker\"></a>Root用户启动Docker</h1><figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo su</span><br><span class=\"line\"><span class=\"built_in\">export</span> DOCKER_HOST=tcp://localhost:2375</span><br><span class=\"line\">service docker start</span><br></pre></td></tr></table></figure>\n<p>显示如下：</p>\n<figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">* Starting Docker: docker</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"测试-还是在root用户\"><a href=\"#测试-还是在root用户\" class=\"headerlink\" title=\"测试(还是在root用户)\"></a>测试(还是在root用户)</h1><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker version</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"报错\"><a href=\"#报错\" class=\"headerlink\" title=\"报错\"></a>报错</h1><p>以下是报错信息：</p>\n<figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Client: Docker Engine - Community</span><br><span class=\"line\"> Version:           20.10.12</span><br><span class=\"line\"> API version:       1.41</span><br><span class=\"line\"> Go version:        go1.16.12</span><br><span class=\"line\"> Git commit:        e91ed57</span><br><span class=\"line\"> Built:             Mon Dec 13 11:45:33 2021</span><br><span class=\"line\"> OS/Arch:           linux/amd64</span><br><span class=\"line\"> Context:           default</span><br><span class=\"line\"> Experimental:      true</span><br><span class=\"line\">Cannot connect to the Docker daemon at tcp://localhost:2375. Is the docker daemon running?</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"修复\"><a href=\"#修复\" class=\"headerlink\" title=\"修复\"></a>修复</h1><p><strong>回到普通用户运行</strong></p>\n<ul>\n<li><p>编辑环境变量</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vi ~/.bashrc</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 在最下面添加一行：</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> DOCKER_HOST=<span class=\"string\">&#x27;unix:///var/run/docker.sock&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>\n</li>\n<li><p>再次测试</p>\n  <figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker version</span><br></pre></td></tr></table></figure>\n<p>  显示如下表明成功：</p>\n  <figure class=\"highlight tex\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Client: Docker Engine - Community</span><br><span class=\"line\"> Version:           20.10.12</span><br><span class=\"line\"> API version:       1.41</span><br><span class=\"line\"> Go version:        go1.16.12</span><br><span class=\"line\"> Git commit:        e91ed57</span><br><span class=\"line\"> Built:             Mon Dec 13 11:45:33 2021</span><br><span class=\"line\"> OS/Arch:           linux/amd64</span><br><span class=\"line\"> Context:           default</span><br><span class=\"line\"> Experimental:      true</span><br><span class=\"line\"></span><br><span class=\"line\">Server: Docker Engine - Community</span><br><span class=\"line\"> Engine:</span><br><span class=\"line\">  Version:          20.10.12</span><br><span class=\"line\">  API version:      1.41 (minimum version 1.12)</span><br><span class=\"line\">  Go version:       go1.16.12</span><br><span class=\"line\">  Git commit:       459d0df</span><br><span class=\"line\">  Built:            Mon Dec 13 11:43:42 2021</span><br><span class=\"line\">  OS/Arch:          linux/amd64</span><br><span class=\"line\">  Experimental:     false</span><br><span class=\"line\"> containerd:</span><br><span class=\"line\">  Version:          1.4.13</span><br><span class=\"line\">  GitCommit:        9cc61520f4cd876b86e77edfeb88fbcd536d1f9d</span><br><span class=\"line\"> runc:</span><br><span class=\"line\">  Version:          1.0.3</span><br><span class=\"line\">  GitCommit:        v1.0.3-0-gf46b6ba</span><br><span class=\"line\"> docker-init:</span><br><span class=\"line\">  Version:          0.19.0</span><br><span class=\"line\">  GitCommit:        de40ad0</span><br></pre></td></tr></table></figure></li>\n</ul>\n<h1 id=\"参考\"><a href=\"#参考\" class=\"headerlink\" title=\"参考\"></a>参考</h1><ul>\n<li><a href=\"https://blog.csdn.net/qq_44701736/article/details/119411485\">WSL安装docker</a></li>\n<li><a href=\"https://www.jianshu.com/p/7c0084fd9003\">Cannot connect to the Docker daemon at tcp:&#x2F;&#x2F;localhost:2375. Is the docker daemon running?</a></li>\n</ul>\n<h1 id=\"代码获取\"><a href=\"#代码获取\" class=\"headerlink\" title=\"代码获取\"></a>代码获取</h1><p>关注公众号“生信之巅”，聊天窗口回复“”获取下载链接。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"Linux","path":"api/tags/Linux.json"},{"name":"系统","path":"api/tags/系统.json"},{"name":"虚拟机","path":"api/tags/虚拟机.json"}]},{"title":"windows 10 中文乱码解决方案","slug":"windows-10-中文乱码解决方案","date":"2019-03-01T13:53:01.000Z","updated":"2024-03-11T13:07:17.000Z","comments":true,"path":"api/articles/windows-10-中文乱码解决方案.json","excerpt":null,"keywords":null,"cover":"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_1.png","content":"<p>安装的英文原版系统，导致多数中文乱码，即便是安装了中文语言包，如果设置错误，仍旧无法正确显示中文。</p>\n<span id=\"more\"></span>\n<p>进入正题，按如图所示一步步完成设置，重启系统便可解决问题。我的系统是18845.1001版本，其他版本应该不会差别太大。</p>\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_1.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"通过开始菜单进入控制面板\"></p>\n<center>通过开始菜单进入控制面板</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_2.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"日期、时间和区域格式设置\"></p>\n<center>日期、时间和区域格式设置</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_3.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_3.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"下拉相关设置\"></p>\n<center>下拉相关设置</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_4.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_4.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"选择区域\"></p>\n<center>选择区域</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_5.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_5.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"管理-更改系统区域设置\"></p>\n<center>管理-更改系统区域设置</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_6.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_6.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"设置为中文，不要选择Beta版\"></p>\n<center>设置为中文，不要选择Beta版</center>\n\n<p><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_7.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/news/control_panel_7.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"复制设置\"></p>\n<center>复制设置</center>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"系统","path":"api/tags/系统.json"}]},{"title":"获取下一版本的内部预览版windows系统","slug":"获取下一个版本的内部预览版windows系统","date":"2019-02-17T11:19:04.000Z","updated":"2024-03-11T13:07:17.000Z","comments":true,"path":"api/articles/获取下一个版本的内部预览版windows系统.json","excerpt":null,"keywords":null,"cover":"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/papers/skip_user.png","content":"<p>Windows内部预览版系统快速通道已经关闭，但是可以通过修改注册表以获取下一个版本的Windows。</p>\n<span id=\"more\"></span>\n\n<ol>\n<li><p>首先，您必须已经是Windows Insider才能利用这一技巧。 如果您是Windows Insider，点击“设置”→“更新和安全”→“Windows预览体验计划”，将“你希望接收哪类内容”一项设置为“跳到下一个Windows版本”。注意：调整完该选项时请保持窗口不动，即不要关闭也不要切换其他面板；</p>\n</li>\n<li><p>启动注册表编辑器（点击开始菜单左下角搜索框，输入“regedit”进入注册表编辑器）， 找到：</p>\n</li>\n</ol>\n<p>HKEY_LOCAL_MACHINE\\SOFTWARE \\Microsoft\\WindowsSelfHost\\UI\\Selection</p>\n<p>双击“UIContentType”并将其更改为“Skip”。 然后，您应该找到“UIRing”并将其更改为“WIF”。</p>\n<ol start=\"3\">\n<li>然后找到：</li>\n</ol>\n<p>HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\WindowsSelfHost\\Applicability</p>\n<p>将“Ring”的值改为“WIF”，“Content Type”和ContentBackup的值更改为“Skip”。</p>\n<ol start=\"4\">\n<li><p>退回“Windows预览体验计划”面板，随便点击一个标签再切换回来，如果打开后依旧固定在“跳到下一个Windows版本”选项，即代表上车成功！<br><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/papers/skip_user.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/papers/skip_user.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"Skip用户\"></p>\n<center>Skip用户</center>\n\n</li>\n<li><p>点击“Windows更新”→“检查更新”，获取新版本系统<br><img src=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/papers/skip_user2.png\" class=\"lazyload placeholder\" data-srcset=\"https://fastly.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/custom/papers/skip_user2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"Skip用户的更新界面\"></p>\n<center>Skip用户的更新界面</center></li>\n</ol>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"系统","path":"api/tags/系统.json"}]}]}
{"name":"机器学习","postlist":[{"title":"PyTorch实战-利用卷积神经网络完成手写数字识别","slug":"PyTorch实战-利用卷积神经网络完成手写数字识别","date":"2024-09-01T04:37:55.000Z","updated":"2024-09-01T04:57:42.611Z","comments":true,"path":"api/articles/PyTorch实战-利用卷积神经网络完成手写数字识别.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>卷积神经网络（Convolutional Neural Networks, CNNs）是一种特殊类型的神经网络，在图像和视频识别、推荐系统、图像分类、医学图像分析、自然语言处理等领域有着广泛的应用。它们能够自动从原始图像中提取特征，并通过多层网络结构学习这些特征的高级表示。本文通过手写数字识别项目带大家学习卷积神经网络。</p>\n<h1 id=\"卷积神经网络基本概念\"><a href=\"#卷积神经网络基本概念\" class=\"headerlink\" title=\"卷积神经网络基本概念\"></a>卷积神经网络基本概念</h1><h2 id=\"CNN结构\"><a href=\"#CNN结构\" class=\"headerlink\" title=\"CNN结构\"></a>CNN结构</h2><p>输入层（Input Layer）–&gt; {卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）–&gt; 卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）}(重复) –&gt; 全连接层（Fully Connected Layer）</p>\n<h2 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h2><p>在深度学习和计算机视觉中，尤其是在处理卷积神经网络（CNN）时，计算输出尺寸（Output Size）的公式非常重要。您给出的公式是卷积层输出尺寸计算的一个基本公式，但通常我们会用更直观的符号来表示它。下面是该公式转换为常用表示方法的形式：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - K}{S} \\right\\rfloor + 1 $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），通常是输出特征图的高度或宽度（假设它们是相等的，即正方形特征图）。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度或宽度。</li>\n<li>$P$ 代表填充（Padding）的大小，即在输入特征图的边界上添加的零的层数。注意，这里的 $2P$ 表示在输入特征图的两侧（或上下两侧，取决于维度）都添加了 $P$ 层的零。</li>\n<li>$K$ 代表卷积核（Kernel Size）的大小，即卷积核的高度或宽度（在正方形卷积核的情况下）。</li>\n<li>$S$ 代表步长（Stride），即卷积核在输入特征图上移动的步数。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<p>这个公式适用于计算卷积层（包括标准卷积层和转置卷积层，但转置卷积层有额外的参数和复杂性）后的输出特征图尺寸。在实际应用中，了解如何根据这些参数调整网络结构以得到期望的输出尺寸是非常重要的。</p>\n<h2 id=\"激活函数（Activation-Function）\"><a href=\"#激活函数（Activation-Function）\" class=\"headerlink\" title=\"激活函数（Activation Function）\"></a>激活函数（Activation Function）</h2><p>激活函数用于在卷积层（以及其他类型的神经网络层）之后引入非线性。常见的激活函数包括ReLU（Rectified Linear Unit，修正线性单元）、sigmoid和tanh等。ReLU因其简单性和减少梯度消失问题的能力而在CNN中广泛使用。</p>\n<h2 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h2><p>池化层（Pooling Layer）在卷积神经网络（CNN）中扮演着重要的角色，主要用于特征融合和降维，以减少计算量和控制过拟合。池化层的输入尺寸和输出尺寸计算公式可以根据不同的参数设置而有所不同，但基本思路是相似的。</p>\n<h3 id=\"池化层的输出尺寸计算公式\"><a href=\"#池化层的输出尺寸计算公式\" class=\"headerlink\" title=\"池化层的输出尺寸计算公式\"></a>池化层的输出尺寸计算公式</h3><p>池化层的输出尺寸计算公式可以表示为：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - F}{S} + 1 \\right\\rfloor $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），即池化层输出的特征图的高度和宽度。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度和宽度。</li>\n<li>$F$ 是池化窗口（Pooling Window）的大小，即池化操作覆盖的输入特征图的区域大小。</li>\n<li>$S$ 是步长（Stride），即池化窗口在输入特征图上移动的步数。</li>\n<li>$P$ 是填充（Padding），即在输入特征图的边界上添加的零的层数，用于控制输出尺寸。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><ul>\n<li>池化层通常不涉及权重和偏置参数，因此它们不会影响模型的学习能力，但对于减少计算量和控制过拟合非常有帮助。</li>\n<li>在实际应用中，池化窗口的大小$F$和步长$S$通常设置为相同的值，如2或3，这样可以更有效地降低特征图的维度。</li>\n<li>填充$P$的值可以是0（无填充），也可以是其他正整数（有填充），具体取决于需要保持输出特征图尺寸与输入特征图尺寸的比例关系。</li>\n</ul>\n<h2 id=\"全连接层（Fully-Connected-Layer-FC-Layer）\"><a href=\"#全连接层（Fully-Connected-Layer-FC-Layer）\" class=\"headerlink\" title=\"全连接层（Fully Connected Layer, FC Layer）\"></a>全连接层（Fully Connected Layer, FC Layer）</h2><p>在CNN的末端，通常会有几个全连接层。这些层中的每个神经元都与前一层的所有神经元相连接。全连接层的作用是将前面层学到的“分布式特征表示”映射到样本标记空间。在分类任务中，全连接层的输出可以传递给softmax函数来生成最终的类别概率分布。</p>\n<h2 id=\"参数共享（Parameter-Sharing）\"><a href=\"#参数共享（Parameter-Sharing）\" class=\"headerlink\" title=\"参数共享（Parameter Sharing）\"></a>参数共享（Parameter Sharing）</h2><p>在CNN中，卷积核的参数是在整个输入数据上共享的。这意味着无论数据中的哪个位置，卷积核都使用相同的权重和偏置参数进行卷积操作。这种参数共享机制减少了模型的参数量，并有助于模型学习到输入数据的空间层次结构。</p>\n<h2 id=\"局部连接（Local-Connectivity）\"><a href=\"#局部连接（Local-Connectivity）\" class=\"headerlink\" title=\"局部连接（Local Connectivity）\"></a>局部连接（Local Connectivity）</h2><p>在卷积层中，每个神经元仅与输入数据的一个局部区域（即感受野）相连接，而不是与整个输入数据相连接。这种局部连接机制使得CNN能够学习到数据的局部特征，这与人类视觉系统的处理机制相似。</p>\n<h2 id=\"反向传播（Backpropagation）\"><a href=\"#反向传播（Backpropagation）\" class=\"headerlink\" title=\"反向传播（Backpropagation）\"></a>反向传播（Backpropagation）</h2><p>反向传播算法是训练CNN（以及其他类型的神经网络）的关键算法。在训练过程中，通过计算损失函数关于网络参数的梯度，并利用梯度下降（或其变体）来更新网络参数，以最小化损失函数。反向传播算法通过链式法则在网络的每一层中传播梯度信息。</p>\n<h1 id=\"MNIST数据集介绍\"><a href=\"#MNIST数据集介绍\" class=\"headerlink\" title=\"MNIST数据集介绍\"></a>MNIST数据集介绍</h1><p>在探索机器学习领域的广阔天地时，手写数字识别作为一个经典且基础的任务，始终占据着重要的地位。而MNIST（Modified National Institute of Standards and Technology）数据集，正是这一任务中最常用、最经典的数据集之一。本文将首先介绍MNIST数据集，为后续的手写数字识别模型训练与测试打下坚实的基础。</p>\n<h2 id=\"MNIST数据集概述\"><a href=\"#MNIST数据集概述\" class=\"headerlink\" title=\"MNIST数据集概述\"></a>MNIST数据集概述</h2><p>MNIST数据集由Yann LeCun等人搜集整理，是一个大型的手写体数字数据库。该数据集最初来源于NIST（National Institute of Standards and Technology）的两个数据库：专用数据库1（Special Database 1）和特殊数据库3（Special Database 3）。通过精心筛选与预处理，MNIST最终成为了一个包含大量手写数字图像的标准数据集，广泛应用于各种图像处理系统和机器学习算法的训练与测试中。</p>\n<h2 id=\"数据集的构成\"><a href=\"#数据集的构成\" class=\"headerlink\" title=\"数据集的构成\"></a>数据集的构成</h2><p>MNIST数据集由60,000个训练样本和10,000个测试样本组成，每个样本都是一张<code>28x28</code>像素的灰度图像，表示一个手写数字（0-9）。这些图像均已被归一化，像素值范围在0到255之间，其中<code>0</code>代表黑色，<code>255</code>代表白色。数据集的图像由来自不同人群的手写体构成，包括高中生和美国人口普查局的工作人员，确保了数据的多样性和代表性。</p>\n<h2 id=\"数据集的特点\"><a href=\"#数据集的特点\" class=\"headerlink\" title=\"数据集的特点\"></a>数据集的特点</h2><ol>\n<li><p><strong>简单性</strong>：虽然MNIST数据集包含的手写数字种类繁多，但由于其图像尺寸小（28x28像素）、像素深度低（灰度图像），使得处理起来相对简单。这使其成为机器学习初学者练习图像识别、模式识别等任务的理想选择。</p>\n</li>\n<li><p><strong>代表性</strong>：MNIST数据集中的手写数字覆盖了各种书写风格和变体，使得训练出的模型能够较好地泛化到未知的手写数字上。因此，该数据集在评估机器学习算法性能时具有很高的参考价值。</p>\n</li>\n<li><p><strong>广泛应用</strong>：由于其简单性和代表性，MNIST数据集在机器学习领域得到了广泛应用。从简单的神经网络到复杂的深度学习模型，几乎所有的图像识别算法都会使用MNIST数据集进行训练和测试。</p>\n</li>\n</ol>\n<h2 id=\"数据集的下载与使用\"><a href=\"#数据集的下载与使用\" class=\"headerlink\" title=\"数据集的下载与使用\"></a>数据集的下载与使用</h2><p>MNIST数据集可以通过多种途径下载，其中最常用的方式是通过互联网直接下载。用户可以从Yann LeCun的官方网站（<a href=\"http://yann.lecun.com/exdb/mnist/%EF%BC%89%E6%88%96%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB%E5%B9%B3%E5%8F%B0%E8%8E%B7%E5%8F%96%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82%E4%B8%8B%E8%BD%BD%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E9%80%9A%E5%B8%B8%E5%8C%85%E5%90%AB%E5%9B%9B%E4%B8%AA%E6%96%87%E4%BB%B6%EF%BC%9A%E8%AE%AD%E7%BB%83%E9%9B%86%E5%9B%BE%E5%83%8F%E3%80%81%E8%AE%AD%E7%BB%83%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E5%9B%BE%E5%83%8F%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%82%E8%BF%99%E4%BA%9B%E6%96%87%E4%BB%B6%E5%9D%87%E4%B8%BA%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%EF%BC%8C%E7%94%A8%E6%88%B7%E9%9C%80%E8%A6%81%E8%A7%A3%E5%8E%8B%E5%90%8E%E6%89%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E3%80%82\">http://yann.lecun.com/exdb/mnist/）或其他数据共享平台获取该数据集。下载后的数据集通常包含四个文件：训练集图像、训练集标签、测试集图像和测试集标签。这些文件均为压缩格式，用户需要解压后才能使用。</a></p>\n<p>在使用MNIST数据集时，用户需要根据自己的需求进行预处理和加载操作。例如，可以使用Python的NumPy库或Pandas库来读取和处理数据集中的图像和标签信息；也可以使用深度学习框架（如TensorFlow、PyTorch等）中提供的数据加载工具来简化这一过程。</p>\n<h1 id=\"下载并导入数据集\"><a href=\"#下载并导入数据集\" class=\"headerlink\" title=\"下载并导入数据集\"></a>下载并导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入必要的库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> datasets, transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据预处理</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Compose组合多个变换，这里将数据转换为张量并进行标准化</span></span><br><span class=\"line\">transform = transforms.Compose([</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">0.5</span>,))</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 下载MNIST数据集并划分为训练集和测试集</span></span><br><span class=\"line\">train_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">True</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\">test_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">False</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>查看训练集属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看整个训练集的样本数量及单个样本的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;训练集大小: <span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_dataset)&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 查看第一个样本的数据和标签</span></span><br><span class=\"line\">first_sample, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本数据形状: <span class=\"subst\">&#123;first_sample.shape&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本标签: <span class=\"subst\">&#123;first_label&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果想查看前几个样本的具体数据内容，可以通过循环实现</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    data, label = train_dataset[i]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;样本 <span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span> 的数据:\\n<span class=\"subst\">&#123;data&#125;</span>\\n标签: <span class=\"subst\">&#123;label&#125;</span>\\n&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>训练集大小: 60000\n首个样本数据形状: torch.Size([1, 28, 28])\n首个样本标签: 5\n样本 1 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588,\n          -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,\n           1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.7647,  0.3490,  0.9843,\n           0.8980,  0.5294, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6157,  0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9686, -0.2706, -0.3569, -0.3569,\n          -0.5608, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8588,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.5529,  0.4275,  0.9373,  0.8902, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078,\n          -0.9137, -1.0000, -0.6627,  0.2078, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902,\n          -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843,\n          -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,\n           0.7647,  0.2549, -0.1529, -0.9922, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,\n           0.8824,  0.9843,  0.9843, -0.0667, -0.8039, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6471,  0.4588,  0.9843,  0.9843,  0.1765, -0.7882, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8745, -0.2706,  0.9765,  0.9843,  0.4667, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9529,  0.9843,  0.9529, -0.4980,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6392,  0.0196,  0.4353,  0.9843,  0.9843,  0.6235, -0.9843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,\n           0.7961,  0.9843,  0.9843,  0.9843,  0.9608,  0.4275, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.5765, -0.3882, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.5529, -0.3647, -0.9843, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n           0.3412,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294,\n          -0.3725, -0.9294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,\n           0.9843,  0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 5\n\n样本 2 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6000,  0.2471,  0.9843,  0.2471, -0.6078, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6235,  0.8667,  0.9765,  0.9765,  0.9765,  0.8588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5765,\n           0.7804,  0.9843,  0.9765,  0.8745,  0.8275,  0.9765, -0.5529,\n          -0.9529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9216, -0.5294,  0.7569,\n           0.9765,  0.9843,  0.9765,  0.5843, -0.3412,  0.9765,  0.9843,\n          -0.0431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9765,  0.9765,\n           0.9765,  0.9843,  0.9765,  0.9765, -0.2471,  0.4824,  0.9843,\n           0.3098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6000,  0.8667,  0.9843,  0.9843,\n           0.4902, -0.1059,  0.9843,  0.7882, -0.6314, -0.3804,  1.0000,\n           0.3176, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.8667,  0.9765,  0.9765,  0.4039,\n          -0.9059, -0.4118, -0.0510, -0.8353, -1.0000, -1.0000,  0.9843,\n           0.9059, -0.6078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7020,  0.2941,  0.9843,  0.8275,  0.6314, -0.3412,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.9451,  0.3961,  0.9765,  0.8824, -0.4431, -0.8510, -0.7804,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5529,  0.9765,  0.9765, -0.5059, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.5529,  0.9843,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n           0.9843,  0.5373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4039,\n           0.9294,  0.9765, -0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.1608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.8039, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9451,  0.0588,  0.9843,\n           0.4588, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.9451,  0.0275,  0.9765,  0.7647,\n          -0.4431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.2941,  0.9765,  0.3569, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3255,\n           0.9843,  0.7647, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.1059,  0.8667,  0.9843,  0.2706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9529,  0.1451, -0.6235, -0.7725, -0.3333,  0.3961,\n           0.7647,  0.9843,  0.7490,  0.3098, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9765,  0.9765,  0.7961,  0.6863,  0.9765,  0.9765,\n           0.9765,  0.5373,  0.0196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7804,\n           0.5608,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,  0.8275,\n           0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8039,  0.0039,  0.9765,  0.9843,  0.9765,  0.1059, -0.7098,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 0\n\n样本 3 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n           0.8196, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5137, -0.3647, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0588,\n           0.4118, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0118,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2000,\n           0.6471, -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7255,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7882,  0.9922,\n           0.2706, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7412,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4353,  0.9922,\n          -0.0196, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.6392,  0.9216,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5529,  0.9922,\n          -0.5608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.0588,  0.9922,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.8196,  0.8118,  0.9922,\n          -0.7725, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.0588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9922,  0.6941,\n          -0.8745, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.4745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902, -0.3255,  0.3961,  0.9451,  0.9922, -0.2863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.3333, -1.0000,\n          -1.0000, -1.0000, -0.6314, -0.6157, -0.0902,  0.1294,  0.1765,\n           0.8902,  0.9059,  0.8353,  0.4039,  0.8902,  0.9765, -0.6863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.1765,  0.9843,  0.8588,  0.6235,\n           0.6235,  0.6235,  0.9843,  0.9922,  0.9608,  0.8824,  0.5529,\n           0.1216, -0.2863, -0.7804, -0.9608,  0.8275,  0.9608, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0667,  0.3882,  0.3882,\n           0.3882,  0.3882,  0.3882, -0.2314, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2000,  0.9922,  0.7255, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.0745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.2471, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 4\n\n样本 4 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.0275,  0.9843,  1.0000,\n          -0.5059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9137,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.0039,  0.9686,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.4667,  0.8510,  0.9686,  0.6549, -0.7569,\n          -0.9373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.5294,  0.7882,  0.9686,  0.9686, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.2157,  0.9843,  0.9843,  0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8431,  0.9843,  0.9686,  0.8431, -0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,\n           0.6078,  0.9843,  0.9686, -0.0118, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1843,\n           0.9686,  0.9843,  0.4431, -0.8824, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.8824,\n           0.9686,  0.5137, -0.8196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.9843,  0.9843,\n           0.9843,  0.2471, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.1843,  0.9686,  0.9686,\n           0.9686, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6235,  0.7333,  0.9686,  0.9686,\n           0.3490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.8353,  0.9686,  0.9686,  0.5373,\n          -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9843,  0.9686,  0.9686, -0.3020,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000,  0.2471,  1.0000,  0.9843,  0.9843, -0.7569,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6235,  0.7882,  0.9843,  0.9373,  0.0980, -0.9373,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8118,  0.5137,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 1\n\n样本 5 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.1608,\n           0.6471,  0.9843,  0.9843, -0.1137, -0.3176,  0.1608, -0.5686,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.3176,  0.8196,  0.9765,\n           0.9843,  0.4824,  0.6471,  0.9765,  0.9765,  0.9843,  0.3176,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9686, -0.5529,  0.8980,  0.9765,  0.4902,\n          -0.4902, -0.9608, -0.9059,  0.4275,  0.9765,  0.9843, -0.0902,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2471,  0.9765,  0.9765,  0.4353, -0.8902,\n          -1.0000, -1.0000, -0.2784,  0.9765,  0.9765,  0.7647, -0.8353,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.0353,  0.9843,  0.9765,  0.1451, -0.8902, -1.0000,\n          -1.0000, -1.0000,  0.6863,  0.9765,  0.9765, -0.3804, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.0118,  0.9843,  0.9373,  0.3804, -0.9294, -1.0000, -1.0000,\n          -0.9373, -0.3882,  0.9216,  0.9843,  0.0118, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8196,  0.9765,  0.3804, -1.0000, -1.0000, -1.0000, -0.7176,\n           0.5765,  0.9765,  0.9765,  0.3255, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8275,\n           0.9765,  0.9765, -0.7647, -0.8275, -0.0667,  0.5451,  0.8902,\n           0.9843,  0.9765,  0.9686, -0.3961, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8118,  0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.7725,\n           0.7804,  0.9765,  0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5686,  0.8431,  0.9843,  0.7020,  0.0824, -0.6706, -0.8118,\n           0.5059,  0.9765,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5137,\n           1.0000,  0.9843, -0.1451, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1686,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6471,\n           1.0000,  0.9843, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.7098,  0.9765, -0.5608, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.2471,  0.9765,  0.4824, -0.6706, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8902,  0.4431,  0.9765,  0.3333, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902,  0.1529,  0.9765, -0.6706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 9\n</code></pre>\n<p>展示训练集中的第一幅图片：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据和标签</span></span><br><span class=\"line\">first_image, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将图像数据从形状 (1, 28, 28) 转换为 (28, 28)，以便显示</span></span><br><span class=\"line\">first_image = first_image.squeeze().numpy()  <span class=\"comment\"># 去除通道维度，并转换为numpy数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图像</span></span><br><span class=\"line\">plt.imshow(first_image, cmap=<span class=\"string\">&#x27;gray&#x27;</span>)  <span class=\"comment\"># 使用灰度色阶显示图像</span></span><br><span class=\"line\">plt.title(<span class=\"string\">f&#x27;Label: <span class=\"subst\">&#123;first_label&#125;</span>&#x27;</span>)  <span class=\"comment\"># 显示图像标签作为标题</span></span><br><span class=\"line\">plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)  <span class=\"comment\"># 不显示坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据</span></span><br><span class=\"line\">first_image, _ = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将张量转换为numpy数组以便打印</span></span><br><span class=\"line\">first_image_np = first_image.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印原始的三维数组（包含通道维度）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;原始图像数据（包含通道维度）:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_np)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>原始图像数据（包含通道维度）:\n[[[-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.9764706  -0.85882354 -0.85882354\n   -0.85882354 -0.01176471  0.06666672  0.37254906 -0.79607844\n    0.30196083  1.          0.9372549  -0.00392157 -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.7647059  -0.7176471\n   -0.26274508  0.20784318  0.33333337  0.9843137   0.9843137\n    0.9843137   0.9843137   0.9843137   0.7647059   0.34901965\n    0.9843137   0.8980392   0.5294118  -0.4980392  -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.6156863   0.8666667   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n    0.9843137   0.9843137   0.96862745 -0.27058822 -0.35686272\n   -0.35686272 -0.56078434 -0.69411767 -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.85882354  0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.5529412\n    0.427451    0.9372549   0.8901961  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.372549    0.22352946\n   -0.1607843   0.9843137   0.9843137   0.60784316 -0.9137255\n   -1.         -0.6627451   0.20784318 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.8901961\n   -0.99215686  0.20784318  0.9843137  -0.29411763 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.          0.09019613  0.9843137   0.4901961  -0.9843137\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -0.9137255   0.4901961   0.9843137  -0.45098037\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.7254902   0.8901961   0.7647059\n    0.254902   -0.15294117 -0.99215686 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.36470586  0.88235295\n    0.9843137   0.9843137  -0.06666666 -0.8039216  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.64705884\n    0.45882356  0.9843137   0.9843137   0.17647064 -0.7882353\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8745098  -0.27058822  0.9764706   0.9843137   0.4666667\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.          0.9529412   0.9843137   0.9529412\n   -0.4980392  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.6392157\n    0.0196079   0.43529415  0.9843137   0.9843137   0.62352943\n   -0.9843137  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.69411767  0.16078436  0.79607844\n    0.9843137   0.9843137   0.9843137   0.9607843   0.427451\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8117647  -0.10588235  0.73333335  0.9843137   0.9843137\n    0.9843137   0.9843137   0.5764706  -0.38823527 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.81960785 -0.4823529\n    0.67058825  0.9843137   0.9843137   0.9843137   0.9843137\n    0.5529412  -0.36470586 -0.9843137  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -0.85882354  0.3411765   0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.5294118  -0.372549\n   -0.92941177 -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -0.5686275\n    0.34901965  0.77254903  0.9843137   0.9843137   0.9843137\n    0.9843137   0.9137255   0.04313731 -0.9137255  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.          0.06666672\n    0.9843137   0.9843137   0.9843137   0.6627451   0.05882359\n    0.03529418 -0.8745098  -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]]]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果你想去掉通道维度，打印二维矩阵</span></span><br><span class=\"line\">first_image_2d = first_image_np.squeeze()  <span class=\"comment\"># 去掉通道维度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n去掉通道后的二维矩阵:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_2d)</span><br></pre></td></tr></table></figure>\n\n<pre><code>去掉通道后的二维矩阵:\n[[-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.9764706  -0.85882354 -0.85882354 -0.85882354 -0.01176471  0.06666672\n   0.37254906 -0.79607844  0.30196083  1.          0.9372549  -0.00392157\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.7647059  -0.7176471  -0.26274508  0.20784318\n   0.33333337  0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n   0.7647059   0.34901965  0.9843137   0.8980392   0.5294118  -0.4980392\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.6156863   0.8666667   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.9843137   0.9843137   0.9843137   0.96862745\n  -0.27058822 -0.35686272 -0.35686272 -0.56078434 -0.69411767 -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.85882354  0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.5529412   0.427451    0.9372549   0.8901961\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.372549    0.22352946 -0.1607843   0.9843137\n   0.9843137   0.60784316 -0.9137255  -1.         -0.6627451   0.20784318\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8901961  -0.99215686  0.20784318\n   0.9843137  -0.29411763 -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.09019613\n   0.9843137   0.4901961  -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -0.9137255\n   0.4901961   0.9843137  -0.45098037 -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.7254902   0.8901961   0.7647059   0.254902   -0.15294117 -0.99215686\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.36470586  0.88235295  0.9843137   0.9843137  -0.06666666\n  -0.8039216  -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.64705884  0.45882356  0.9843137   0.9843137\n   0.17647064 -0.7882353  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8745098  -0.27058822  0.9764706\n   0.9843137   0.4666667  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.9529412\n   0.9843137   0.9529412  -0.4980392  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.6392157   0.0196079   0.43529415  0.9843137\n   0.9843137   0.62352943 -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.69411767  0.16078436  0.79607844  0.9843137   0.9843137   0.9843137\n   0.9607843   0.427451   -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -0.8117647  -0.10588235\n   0.73333335  0.9843137   0.9843137   0.9843137   0.9843137   0.5764706\n  -0.38823527 -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.81960785 -0.4823529   0.67058825  0.9843137\n   0.9843137   0.9843137   0.9843137   0.5529412  -0.36470586 -0.9843137\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -0.85882354  0.3411765   0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.5294118  -0.372549   -0.92941177 -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -0.5686275   0.34901965\n   0.77254903  0.9843137   0.9843137   0.9843137   0.9843137   0.9137255\n   0.04313731 -0.9137255  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.          0.06666672  0.9843137\n   0.9843137   0.9843137   0.6627451   0.05882359  0.03529418 -0.8745098\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]]\n</code></pre>\n<h1 id=\"定义模型、优化器、损失函数\"><a href=\"#定义模型、优化器、损失函数\" class=\"headerlink\" title=\"定义模型、优化器、损失函数\"></a>定义模型、优化器、损失函数</h1><p>进行2次卷积和2次池化，得到64<em>7</em>17，再进行2次全连接，得到10个输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义批次大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">64</span></span><br><span class=\"line\"><span class=\"comment\"># 使用DataLoader加载数据，以便在训练过程中更方便地迭代数据</span></span><br><span class=\"line\">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 初始化卷积层、池化层、全连接层和dropout层</span></span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个卷积层，用于提取特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为1（适用于灰度图像），输出通道数为32，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次卷积后生成的特征图大小为32*28*28</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义最大池化层，用于降低特征维度，减少计算量</span></span><br><span class=\"line\">        <span class=\"comment\"># 池化窗口大小为2x2，步长为2，无padding</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次池化后生成的特征图大小为32*14*14</span></span><br><span class=\"line\">        self.pool = nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个卷积层，进一步提取和整合特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为32，输出通道数为64，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第二次卷积后生成的特征图大小为64*14*14</span></span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">32</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个全连接层，用于分类前的特征转换</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为64*7*7（这里的尺寸为64*7*7是因为在前向传播时对第二次卷积进行了池化操作），输出大小为1024</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>, <span class=\"number\">1024</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个全连接层，用于最终的分类输出</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为1024，输出大小为10（假设分类任务有10个类别）</span></span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 定义Dropout层，用于训练过程中的正则化，防止过拟合</span></span><br><span class=\"line\">        <span class=\"comment\"># Dropout比例为0.5，即在训练过程中随机将50%的元素置为0</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 定义前向传播过程</span></span><br><span class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class=\"line\">        x = x.view(-<span class=\"number\">1</span>, <span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>)</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = self.dropout(x)</span><br><span class=\"line\">        x = self.fc2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 实例化模型</span></span><br><span class=\"line\">model = Net()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义优化器</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Adam优化器更新模型参数</span></span><br><span class=\"line\">optimizer = optim.Adam(model.parameters(), lr=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\"><span class=\"comment\"># 使用交叉熵损失函数进行分类任务</span></span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将模型转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义训练轮数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"comment\"># 开始训练过程</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (images, labels) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_loader):</span><br><span class=\"line\">        <span class=\"comment\"># 将数据转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 反向传播和优化</span></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 打印损失信息</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Epoch [<span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;num_epochs&#125;</span>], Step [<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_loader)&#125;</span>], Loss: <span class=\"subst\">&#123;loss.item():<span class=\"number\">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存模型参数到文件</span></span><br><span class=\"line\">torch.save(model.state_dict(), <span class=\"string\">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Epoch [1/10], Step [100/938], Loss: 0.0037\nEpoch [1/10], Step [200/938], Loss: 0.0029\nEpoch [1/10], Step [300/938], Loss: 0.0022\nEpoch [1/10], Step [400/938], Loss: 0.0651\nEpoch [1/10], Step [500/938], Loss: 0.0038\nEpoch [1/10], Step [600/938], Loss: 0.0022\nEpoch [1/10], Step [700/938], Loss: 0.0079\nEpoch [1/10], Step [800/938], Loss: 0.0019\nEpoch [1/10], Step [900/938], Loss: 0.0016\nEpoch [2/10], Step [100/938], Loss: 0.0016\nEpoch [2/10], Step [200/938], Loss: 0.0102\nEpoch [2/10], Step [300/938], Loss: 0.0341\nEpoch [2/10], Step [400/938], Loss: 0.0060\nEpoch [2/10], Step [500/938], Loss: 0.0257\nEpoch [2/10], Step [600/938], Loss: 0.0013\nEpoch [2/10], Step [700/938], Loss: 0.0767\nEpoch [2/10], Step [800/938], Loss: 0.0018\nEpoch [2/10], Step [900/938], Loss: 0.0343\nEpoch [3/10], Step [100/938], Loss: 0.0063\nEpoch [3/10], Step [200/938], Loss: 0.0096\nEpoch [3/10], Step [300/938], Loss: 0.0007\nEpoch [3/10], Step [400/938], Loss: 0.0002\nEpoch [3/10], Step [500/938], Loss: 0.0124\nEpoch [3/10], Step [600/938], Loss: 0.0109\nEpoch [3/10], Step [700/938], Loss: 0.0340\nEpoch [3/10], Step [800/938], Loss: 0.0004\nEpoch [3/10], Step [900/938], Loss: 0.0586\nEpoch [4/10], Step [100/938], Loss: 0.0002\nEpoch [4/10], Step [200/938], Loss: 0.0554\nEpoch [4/10], Step [300/938], Loss: 0.0008\nEpoch [4/10], Step [400/938], Loss: 0.0029\nEpoch [4/10], Step [500/938], Loss: 0.0036\nEpoch [4/10], Step [600/938], Loss: 0.0009\nEpoch [4/10], Step [700/938], Loss: 0.0281\nEpoch [4/10], Step [800/938], Loss: 0.0826\nEpoch [4/10], Step [900/938], Loss: 0.0003\nEpoch [5/10], Step [100/938], Loss: 0.0001\nEpoch [5/10], Step [200/938], Loss: 0.0240\nEpoch [5/10], Step [300/938], Loss: 0.0040\nEpoch [5/10], Step [400/938], Loss: 0.0003\nEpoch [5/10], Step [500/938], Loss: 0.0107\nEpoch [5/10], Step [600/938], Loss: 0.0019\nEpoch [5/10], Step [700/938], Loss: 0.0002\nEpoch [5/10], Step [800/938], Loss: 0.0006\nEpoch [5/10], Step [900/938], Loss: 0.0008\nEpoch [6/10], Step [100/938], Loss: 0.0003\nEpoch [6/10], Step [200/938], Loss: 0.0001\nEpoch [6/10], Step [300/938], Loss: 0.0003\nEpoch [6/10], Step [400/938], Loss: 0.0226\nEpoch [6/10], Step [500/938], Loss: 0.0024\nEpoch [6/10], Step [600/938], Loss: 0.0020\nEpoch [6/10], Step [700/938], Loss: 0.0005\nEpoch [6/10], Step [800/938], Loss: 0.0007\nEpoch [6/10], Step [900/938], Loss: 0.0188\nEpoch [7/10], Step [100/938], Loss: 0.0286\nEpoch [7/10], Step [200/938], Loss: 0.0007\nEpoch [7/10], Step [300/938], Loss: 0.0004\nEpoch [7/10], Step [400/938], Loss: 0.0008\nEpoch [7/10], Step [500/938], Loss: 0.0001\nEpoch [7/10], Step [600/938], Loss: 0.0006\nEpoch [7/10], Step [700/938], Loss: 0.0005\nEpoch [7/10], Step [800/938], Loss: 0.0007\nEpoch [7/10], Step [900/938], Loss: 0.0432\nEpoch [8/10], Step [100/938], Loss: 0.0005\nEpoch [8/10], Step [200/938], Loss: 0.0005\nEpoch [8/10], Step [300/938], Loss: 0.0000\nEpoch [8/10], Step [400/938], Loss: 0.0013\nEpoch [8/10], Step [500/938], Loss: 0.0005\nEpoch [8/10], Step [600/938], Loss: 0.0002\nEpoch [8/10], Step [700/938], Loss: 0.0004\nEpoch [8/10], Step [800/938], Loss: 0.0111\nEpoch [8/10], Step [900/938], Loss: 0.0001\nEpoch [9/10], Step [100/938], Loss: 0.0004\nEpoch [9/10], Step [200/938], Loss: 0.0693\nEpoch [9/10], Step [300/938], Loss: 0.0071\nEpoch [9/10], Step [400/938], Loss: 0.0000\nEpoch [9/10], Step [500/938], Loss: 0.0003\nEpoch [9/10], Step [600/938], Loss: 0.0003\nEpoch [9/10], Step [700/938], Loss: 0.0001\nEpoch [9/10], Step [800/938], Loss: 0.0000\nEpoch [9/10], Step [900/938], Loss: 0.0029\nEpoch [10/10], Step [100/938], Loss: 0.0008\nEpoch [10/10], Step [200/938], Loss: 0.0001\nEpoch [10/10], Step [300/938], Loss: 0.0000\nEpoch [10/10], Step [400/938], Loss: 0.0273\nEpoch [10/10], Step [500/938], Loss: 0.0001\nEpoch [10/10], Step [600/938], Loss: 0.0002\nEpoch [10/10], Step [700/938], Loss: 0.0010\nEpoch [10/10], Step [800/938], Loss: 0.0019\nEpoch [10/10], Step [900/938], Loss: 0.0000\n</code></pre>\n<h1 id=\"评估模型\"><a href=\"#评估模型\" class=\"headerlink\" title=\"评估模型\"></a>评估模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 实例化一个与原模型结构相同的模型</span></span><br><span class=\"line\">model = Net().to(device)  <span class=\"comment\"># 确保模型被放置在正确的设备上</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载模型参数</span></span><br><span class=\"line\">model.load_state_dict(torch.load(<span class=\"string\">&#x27;model.pth&#x27;</span>, map_location=device, weights_only=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将模型设置为评估模式</span></span><br><span class=\"line\">model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"><span class=\"comment\"># 禁用梯度计算以减少内存消耗</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    correct = <span class=\"number\">0</span></span><br><span class=\"line\">    total = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"comment\"># 在测试集上进行预测</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> images, labels <span class=\"keyword\">in</span> test_loader:</span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        _, predicted = torch.<span class=\"built_in\">max</span>(outputs.data, <span class=\"number\">1</span>)</span><br><span class=\"line\">        total += labels.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        correct += (predicted == labels).<span class=\"built_in\">sum</span>().item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 打印测试集上的准确率</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Test Accuracy of the model on the <span class=\"subst\">&#123;total&#125;</span> test images: <span class=\"subst\">&#123;<span class=\"number\">100</span> * correct / total&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Test Accuracy of the model on the 10000 test images: 99.29%\n</code></pre>\n<h1 id=\"代码获取\"><a href=\"#代码获取\" class=\"headerlink\" title=\"代码获取\"></a>代码获取</h1><p>关注公众号“生信之巅”，聊天窗口回复“a7fe”获取完整版代码下载链接。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"PyTorch","path":"api/tags/PyTorch.json"},{"name":"卷积神经网络","path":"api/tags/卷积神经网络.json"}]},{"title":"Scikit-learn机器学习实战-HumanResourcesAnalytics","slug":"Scikit-learn机器学习实战-HumanResourcesAnalytics","date":"2024-08-30T14:24:05.000Z","updated":"2024-08-30T14:59:29.868Z","comments":true,"path":"api/articles/Scikit-learn机器学习实战-HumanResourcesAnalytics.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>本文构建预测员工是否会离职的模型，并使用模型对员工进行预测。通过本文可以学习到：</p>\n<ul>\n<li>查看数据集的统计信息</li>\n<li>特征工程</li>\n<li>数据集的划分</li>\n<li>数据集的预处理</li>\n<li>数据集的可视化</li>\n<li>模型训练</li>\n<li>模型调参</li>\n<li>模型评估</li>\n<li>模型预测</li>\n</ul>\n<h1 id=\"查看数据集信息\"><a href=\"#查看数据集信息\" class=\"headerlink\" title=\"查看数据集信息\"></a>查看数据集信息</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">url = <span class=\"string\">&#x27;https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/data/ML/HumanResourcesAnalytics/HR_comma_sep.csv&#x27;</span></span><br><span class=\"line\">df = pd.read_csv(url)</span><br><span class=\"line\"><span class=\"comment\">#df = pd.read_csv(&#x27;HR_comma_sep.csv&#x27;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(df.info()) <span class=\"comment\">#474241623</span></span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 14999 entries, 0 to 14998\nData columns (total 10 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   satisfaction_level     14999 non-null  float64\n 1   last_evaluation        14999 non-null  float64\n 2   number_project         14999 non-null  int64  \n 3   average_montly_hours   14999 non-null  int64  \n 4   time_spend_company     14999 non-null  int64  \n 5   Work_accident          14999 non-null  int64  \n 6   left                   14999 non-null  int64  \n 7   promotion_last_5years  14999 non-null  int64  \n 8   sales                  14999 non-null  object \n 9   salary                 14999 non-null  object \ndtypes: float64(2), int64(6), object(2)\nmemory usage: 1.1+ MB\nNone\n</code></pre>\n<div style=\"overflow-x: auto; width: 100%;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n</head>\n<body>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_montly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n      <th>sales</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<p><strong>header 信息</strong></p>\n<ul>\n<li>satisfaction_level\t员工满意度</li>\n<li>last_evaluation\t员工考核评分</li>\n<li>number_project\t员工参与的项目数</li>\n<li>average_montly_hours\t每个月均工作时长</li>\n<li>time_spend_company\t员工工作年限</li>\n<li>Work_accident\t是否发生过事故</li>\n<li>left\t员工是否离职</li>\n<li>promotion_last_5years\t过去5年中是否有升职</li>\n<li>sales\t员工岗位</li>\n<li>salary 员工薪资水平</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 更正列名</span></span><br><span class=\"line\">df.rename(columns=&#123;<span class=\"string\">&#x27;average_montly_hours&#x27;</span>:<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>, <span class=\"string\">&#x27;sales&#x27;</span>:<span class=\"string\">&#x27;department&#x27;</span>&#125;, </span><br><span class=\"line\">          inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n      <th>department</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 展示数据集的统计信息，仅展示数值列</span></span><br><span class=\"line\">df.describe()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.612834</td>\n      <td>0.716102</td>\n      <td>3.803054</td>\n      <td>201.050337</td>\n      <td>3.498233</td>\n      <td>0.144610</td>\n      <td>0.238083</td>\n      <td>0.021268</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.248631</td>\n      <td>0.171169</td>\n      <td>1.232592</td>\n      <td>49.943099</td>\n      <td>1.460136</td>\n      <td>0.351719</td>\n      <td>0.425924</td>\n      <td>0.144281</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.090000</td>\n      <td>0.360000</td>\n      <td>2.000000</td>\n      <td>96.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.440000</td>\n      <td>0.560000</td>\n      <td>3.000000</td>\n      <td>156.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.640000</td>\n      <td>0.720000</td>\n      <td>4.000000</td>\n      <td>200.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.820000</td>\n      <td>0.870000</td>\n      <td>5.000000</td>\n      <td>245.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>7.000000</td>\n      <td>310.000000</td>\n      <td>10.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看各元素的出现次数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;Departments:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (df[<span class=\"string\">&#x27;department&#x27;</span>].value_counts())</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;\\nSalary:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (df[<span class=\"string\">&#x27;salary&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure>\n\n<pre><code>Departments:\ndepartment\nsales          4140\ntechnical      2720\nsupport        2229\nIT             1227\nproduct_mng     902\nmarketing       858\nRandD           787\naccounting      767\nhr              739\nmanagement      630\nName: count, dtype: int64\n\nSalary:\nsalary\nlow       7316\nmedium    6446\nhigh      1237\nName: count, dtype: int64\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 记录各特征的类型和取值范围</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">satisfaction_level | Satisfaction level of employee based on survey | Continuous | [0.09, 1]</span></span><br><span class=\"line\"><span class=\"string\">last_evaluation | Score based on employee&#x27;s last evaluation | Continuous | [0.36, 1]</span></span><br><span class=\"line\"><span class=\"string\">number_project | Number of projects | Continuous | [2, 7]</span></span><br><span class=\"line\"><span class=\"string\">average_monthly_hours | Average monthly hours | Continuous | [96, 310]</span></span><br><span class=\"line\"><span class=\"string\">time_spend_company | Years at company | Continuous | [2, 10]</span></span><br><span class=\"line\"><span class=\"string\">Work_accident | Whether employee had a work accident | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">left | Whether employee had left (Outcome Variable) | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">promotion_last_5years | Whether employee had a promotion in the last 5 years | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">department | Department employee worked in | Categorical | 10 departments</span></span><br><span class=\"line\"><span class=\"string\">salary | Level of employee&#x27;s salary | Categorical | &#123;low, medium, high&#125;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;\\nsatisfaction_level | Satisfaction level of employee based on survey | Continuous | [0.09, 1]\\nlast_evaluation | Score based on employee&#x27;s last evaluation | Continuous | [0.36, 1]\\nnumber_project | Number of projects | Continuous | [2, 7]\\naverage_monthly_hours | Average monthly hours | Continuous | [96, 310]\\ntime_spend_company | Years at company | Continuous | [2, 10]\\nWork_accident | Whether employee had a work accident | Categorical | &#123;0, 1&#125;\\nleft | Whether employee had left (Outcome Variable) | Categorical | &#123;0, 1&#125;\\npromotion_last_5years | Whether employee had a promotion in the last 5 years | Categorical | &#123;0, 1&#125;\\ndepartment | Department employee worked in | Categorical | 10 departments\\nsalary | Level of employee&#x27;s salary | Categorical | &#123;low, medium, high&#125;\\n&quot;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h1><ul>\n<li>查找相关性大的特征，只保留其中的一个。</li>\n<li>也可查看与标签（left）相关性较大的特征，如此数据集中的<code>satisfaction_level</code>。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 筛选 DataFrame 中的所有数值列</span></span><br><span class=\"line\">numeric_df = df.select_dtypes(include=[np.number])</span><br><span class=\"line\"><span class=\"comment\"># 计算数值列之间的相关系数</span></span><br><span class=\"line\">numeric_df.corr()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>satisfaction_level</th>\n      <td>1.000000</td>\n      <td>0.105021</td>\n      <td>-0.142970</td>\n      <td>-0.020048</td>\n      <td>-0.100866</td>\n      <td>0.058697</td>\n      <td>-0.388375</td>\n      <td>0.025605</td>\n    </tr>\n    <tr>\n      <th>last_evaluation</th>\n      <td>0.105021</td>\n      <td>1.000000</td>\n      <td>0.349333</td>\n      <td>0.339742</td>\n      <td>0.131591</td>\n      <td>-0.007104</td>\n      <td>0.006567</td>\n      <td>-0.008684</td>\n    </tr>\n    <tr>\n      <th>number_project</th>\n      <td>-0.142970</td>\n      <td>0.349333</td>\n      <td>1.000000</td>\n      <td>0.417211</td>\n      <td>0.196786</td>\n      <td>-0.004741</td>\n      <td>0.023787</td>\n      <td>-0.006064</td>\n    </tr>\n    <tr>\n      <th>average_monthly_hours</th>\n      <td>-0.020048</td>\n      <td>0.339742</td>\n      <td>0.417211</td>\n      <td>1.000000</td>\n      <td>0.127755</td>\n      <td>-0.010143</td>\n      <td>0.071287</td>\n      <td>-0.003544</td>\n    </tr>\n    <tr>\n      <th>time_spend_company</th>\n      <td>-0.100866</td>\n      <td>0.131591</td>\n      <td>0.196786</td>\n      <td>0.127755</td>\n      <td>1.000000</td>\n      <td>0.002120</td>\n      <td>0.144822</td>\n      <td>0.067433</td>\n    </tr>\n    <tr>\n      <th>Work_accident</th>\n      <td>0.058697</td>\n      <td>-0.007104</td>\n      <td>-0.004741</td>\n      <td>-0.010143</td>\n      <td>0.002120</td>\n      <td>1.000000</td>\n      <td>-0.154622</td>\n      <td>0.039245</td>\n    </tr>\n    <tr>\n      <th>left</th>\n      <td>-0.388375</td>\n      <td>0.006567</td>\n      <td>0.023787</td>\n      <td>0.071287</td>\n      <td>0.144822</td>\n      <td>-0.154622</td>\n      <td>1.000000</td>\n      <td>-0.061788</td>\n    </tr>\n    <tr>\n      <th>promotion_last_5years</th>\n      <td>0.025605</td>\n      <td>-0.008684</td>\n      <td>-0.006064</td>\n      <td>-0.003544</td>\n      <td>0.067433</td>\n      <td>0.039245</td>\n      <td>-0.061788</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 查看离职员工部门分布，发现HR离职员工最多</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;department&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df)</span><br><span class=\"line\">plot.set_xticklabels(rotation=<span class=\"number\">45</span>, horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看工资水平和离职率的关系</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;salary&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_11_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_11_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看经理工资水平分布</span></span><br><span class=\"line\">df[df[<span class=\"string\">&#x27;department&#x27;</span>]==<span class=\"string\">&#x27;management&#x27;</span>][<span class=\"string\">&#x27;salary&#x27;</span>].value_counts().plot(kind=<span class=\"string\">&#x27;pie&#x27;</span>, title=<span class=\"string\">&#x27;Management salary level distribution&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_12_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_12_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看研发工资水平分布</span></span><br><span class=\"line\">df[df[<span class=\"string\">&#x27;department&#x27;</span>]==<span class=\"string\">&#x27;RandD&#x27;</span>][<span class=\"string\">&#x27;salary&#x27;</span>].value_counts().plot(kind=<span class=\"string\">&#x27;pie&#x27;</span>, title=<span class=\"string\">&#x27;R&amp;D dept salary level distribution&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_13_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_13_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制员工满意度分布的直方图，并分为两类员工：已离职和未离职</span></span><br><span class=\"line\"><span class=\"comment\"># 生成21个等间距的数值作为直方图的区间，范围从0.0001到1.0001</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">0.0001</span>, <span class=\"number\">1.0001</span>, <span class=\"number\">21</span>)</span><br><span class=\"line\"><span class=\"comment\"># 绘制直方图。首先筛选出已离职员工（df[&#x27;left&#x27;]==1）和未离职员工（df[&#x27;left&#x27;]==0）的满意度数据，使用指定的区间（bins）、透明度（alpha）和标签（label）进行绘制。</span></span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;satisfaction_level&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.7</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;satisfaction_level&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.5</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;satisfaction_level&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 设置x轴的显示范围从0到1.05</span></span><br><span class=\"line\">plt.xlim((<span class=\"number\">0</span>,<span class=\"number\">1.05</span>))</span><br><span class=\"line\"><span class=\"comment\"># 在最合适的位置添加图例</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_14_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_14_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>发现已离职员工对公司的满意度比较低（0~0.5），当然也存在满意度较高（0.8附近）的员工离职的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Last evaluation</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">0.3501</span>, <span class=\"number\">1.0001</span>, <span class=\"number\">14</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;last_evaluation&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;last_evaluation&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;last_evaluation&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_16_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_16_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>公司评分高（0.8~1.0）的员工离职了很多，原因可能是这部分员工能力强，跳槽寻求更好的工作机会。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Number of projects </span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">1.5</span>, <span class=\"number\">7.5</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;number_project&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;number_project&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;number_project&#x27;</span>)</span><br><span class=\"line\">plt.grid(axis=<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_18_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_18_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>项目少时离职了，可能因为员工锻炼机会少。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Average monthly hours</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">75</span>, <span class=\"number\">325</span>, <span class=\"number\">11</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_20_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_20_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>工作时长少和多都容易离职。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Years at company </span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">1.5</span>, <span class=\"number\">10.5</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;time_spend_company&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;time_spend_company&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;time_spend_company&#x27;</span>)</span><br><span class=\"line\">plt.xlim((<span class=\"number\">1</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\">plt.grid(axis=<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\">plt.xticks(np.arange(<span class=\"number\">2</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_22_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_22_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>工作年限3年，离职率最高。年限越长，离职率越低。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># whether employee had work accident</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;Work_accident&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_24_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_24_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>未发生工作事故的离职率较高，难以解释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#whether employee had promotion in last 5 years</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;promotion_last_5years&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_26_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_26_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>不升职的离职率较高。</p>\n<h1 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h1><h2 id=\"独热编码替换分类数据\"><a href=\"#独热编码替换分类数据\" class=\"headerlink\" title=\"独热编码替换分类数据\"></a>独热编码替换分类数据</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 丢弃标签（left）列</span></span><br><span class=\"line\">X = df.drop(<span class=\"string\">&#x27;left&#x27;</span>, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 提取标签列</span></span><br><span class=\"line\">y = df[<span class=\"string\">&#x27;left&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 删除部门与工资列，后面会通过独热编码将信息添加回来</span></span><br><span class=\"line\">X.drop([<span class=\"string\">&#x27;department&#x27;</span>,<span class=\"string\">&#x27;salary&#x27;</span>], axis=<span class=\"number\">1</span>, inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># One-hot encoding</span></span><br><span class=\"line\"><span class=\"comment\"># 对工资进行独热编码</span></span><br><span class=\"line\">salary_dummy = pd.get_dummies(df[<span class=\"string\">&#x27;salary&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 对部门进行独热编码</span></span><br><span class=\"line\">department_dummy = pd.get_dummies(df[<span class=\"string\">&#x27;department&#x27;</span>])</span><br><span class=\"line\">X = pd.concat([X, salary_dummy], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = pd.concat([X, department_dummy], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">X.head()</span><br></pre></td></tr></table></figure>\n\n<div style=\"overflow-x: auto; width: 100%;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>promotion_last_5years</th>\n      <th>high</th>\n      <th>low</th>\n      <th>medium</th>\n      <th>IT</th>\n      <th>RandD</th>\n      <th>accounting</th>\n      <th>hr</th>\n      <th>management</th>\n      <th>marketing</th>\n      <th>product_mng</th>\n      <th>sales</th>\n      <th>support</th>\n      <th>technical</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<h2 id=\"拆分训练集和测试集\"><a href=\"#拆分训练集和测试集\" class=\"headerlink\" title=\"拆分训练集和测试集\"></a>拆分训练集和测试集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 划分训练集和测试集 (70%/30%)</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"数据标准化\"><a href=\"#数据标准化\" class=\"headerlink\" title=\"数据标准化\"></a>数据标准化</h2><ul>\n<li>比较大的数值，算法会认为其比较重要，导致结果不准确。</li>\n<li>数值差异比较大的话，模型收敛较慢。</li>\n<li>因此，需要将数据标准化。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 数据标准化，这里是一个例子</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">stdsc = StandardScaler()</span><br><span class=\"line\">X_example = np.array([[ <span class=\"number\">10.</span>, -<span class=\"number\">2.</span>,  <span class=\"number\">23.</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">5.</span>,  <span class=\"number\">32.</span>,  <span class=\"number\">211.</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">10.</span>,  <span class=\"number\">1.</span>, -<span class=\"number\">130.</span>]])</span><br><span class=\"line\">X_example = stdsc.fit_transform(X_example)</span><br><span class=\"line\">X_example = pd.DataFrame(X_example)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_example)</span><br><span class=\"line\">X_example.describe()</span><br></pre></td></tr></table></figure>\n\n<pre><code>          0         1         2\n0  0.707107 -0.802454 -0.083658\n1 -1.414214  1.409716  1.264429\n2  0.707107 -0.607262 -1.180771\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.000000e+00</td>\n      <td>3.000000e+00</td>\n      <td>3.000000e+00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-2.960595e-16</td>\n      <td>-1.110223e-16</td>\n      <td>7.401487e-17</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.224745e+00</td>\n      <td>1.224745e+00</td>\n      <td>1.224745e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.414214e+00</td>\n      <td>-8.024539e-01</td>\n      <td>-1.180771e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-3.535534e-01</td>\n      <td>-7.048582e-01</td>\n      <td>-6.322145e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.071068e-01</td>\n      <td>-6.072624e-01</td>\n      <td>-8.365788e-02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.071068e-01</td>\n      <td>4.012270e-01</td>\n      <td>5.903856e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.071068e-01</td>\n      <td>1.409716e+00</td>\n      <td>1.264429e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 分别对训练集和测试集进行标准化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\">stdsc = StandardScaler()</span><br><span class=\"line\"><span class=\"comment\"># transform our training features</span></span><br><span class=\"line\">X_train_std = stdsc.fit_transform(X_train)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_train_std[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># transform the testing features in the same way</span></span><br><span class=\"line\">X_test_std = stdsc.transform(X_test)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[ 1.40697692 -0.21068428 -0.65422416 -1.37529896 -1.02172591 -0.41080801\n -0.14595719 -0.30564365 -0.98084819  1.16499228 -0.2981308  -0.23781569\n -0.22665375 -0.23057496 -0.21332806 -0.24641294 -0.25073288  1.62416352\n -0.41712208 -0.47247431]\n</code></pre>\n<h1 id=\"构建模型\"><a href=\"#构建模型\" class=\"headerlink\" title=\"构建模型\"></a>构建模型</h1><h2 id=\"随机森林法\"><a href=\"#随机森林法\" class=\"headerlink\" title=\"随机森林法\"></a>随机森林法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 交叉验证（Cross validation）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> ShuffleSplit</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 进行20折交叉验证</span></span><br><span class=\"line\">cv = ShuffleSplit(n_splits=<span class=\"number\">20</span>, test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 构建随机森林模型</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\">rf_model = RandomForestClassifier()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置RF模型，建立树的数量</span></span><br><span class=\"line\">rf_param = &#123;<span class=\"string\">&#x27;n_estimators&#x27;</span>: <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">11</span>)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 探索模型参数（最佳树的个数）</span></span><br><span class=\"line\">rf_grid = GridSearchCV(rf_model, rf_param, cv=cv)</span><br><span class=\"line\">rf_grid.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出最佳参数和最佳得分</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Parameter with best score:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rf_grid.best_params_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Cross validation score:&#x27;</span>, rf_grid.best_score_)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter with best score:\n&#123;&#39;n_estimators&#39;: 9&#125;\nCross validation score: 0.9835079365079364\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在测试集上评估模型</span></span><br><span class=\"line\">best_rf = rf_grid.best_estimator_</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Test score:&#x27;</span>, best_rf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>\n\n<pre><code>Test score: 0.9884444444444445\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 通过随机森林查看特征的重要性，原理是每次打乱一个特征（或添加噪音），然后看预测结果（错误率）是否发生变化，如果变化大，则该特征对预测结果有影响，否则没有影响</span></span><br><span class=\"line\">features = X.columns</span><br><span class=\"line\">feature_importances = best_rf.feature_importances_</span><br><span class=\"line\"></span><br><span class=\"line\">features_df = pd.DataFrame(&#123;<span class=\"string\">&#x27;Features&#x27;</span>: features, <span class=\"string\">&#x27;Importance Score&#x27;</span>: feature_importances&#125;)</span><br><span class=\"line\">features_df.sort_values(<span class=\"string\">&#x27;Importance Score&#x27;</span>, inplace=<span class=\"literal\">True</span>, ascending=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">features_df</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>Importance Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>satisfaction_level</td>\n      <td>0.260366</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average_monthly_hours</td>\n      <td>0.186585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>number_project</td>\n      <td>0.179788</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>time_spend_company</td>\n      <td>0.179571</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>last_evaluation</td>\n      <td>0.144083</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Work_accident</td>\n      <td>0.011949</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>low</td>\n      <td>0.006395</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>high</td>\n      <td>0.005206</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>medium</td>\n      <td>0.003336</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>sales</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>support</td>\n      <td>0.003070</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>technical</td>\n      <td>0.003039</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>RandD</td>\n      <td>0.002143</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>IT</td>\n      <td>0.002048</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>accounting</td>\n      <td>0.001887</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>promotion_last_5years</td>\n      <td>0.001799</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>management</td>\n      <td>0.001755</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hr</td>\n      <td>0.001425</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>product_mng</td>\n      <td>0.001182</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>marketing</td>\n      <td>0.001173</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算前五项特征的重要性之和</span></span><br><span class=\"line\">features_df[<span class=\"string\">&#x27;Importance Score&#x27;</span>][:<span class=\"number\">5</span>].<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n<pre><code>np.float64(0.9503925098929926)\n</code></pre>\n<h2 id=\"基于聚类模型的分析\"><a href=\"#基于聚类模型的分析\" class=\"headerlink\" title=\"基于聚类模型的分析\"></a>基于聚类模型的分析</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(url)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize = (<span class=\"number\">8</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.plot(data.satisfaction_level[data.left == <span class=\"number\">1</span>],data.last_evaluation[data.left == <span class=\"number\">1</span>],<span class=\"string\">&#x27;o&#x27;</span>, alpha = <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Employees who left&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction level&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Employees who stayed&#x27;</span>)</span><br><span class=\"line\">plt.plot(data.satisfaction_level[data.left == <span class=\"number\">0</span>],data.last_evaluation[data.left == <span class=\"number\">0</span>],<span class=\"string\">&#x27;o&#x27;</span>, alpha = <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.xlim([<span class=\"number\">0.4</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction level&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Text(0.5, 0, &#39;Satisfaction level&#39;)\n</code></pre>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_43_1.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_43_1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用KMeans聚类分析</span></span><br><span class=\"line\"><span class=\"comment\"># 导入KMeans聚类算法模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 选取数据中已经离职的员工（left列为1），并从这些数据中删除特定的列</span></span><br><span class=\"line\"><span class=\"comment\"># 这里axis=1表示按列删除，这些列包括：项目数量、月平均工作小时、公司服务时间、工作事故、是否离职、过去5年是否晋升、销售部门和薪水等</span></span><br><span class=\"line\">kmeans_df =  data[data.left == <span class=\"number\">1</span>].drop([ <span class=\"string\">u&#x27;number_project&#x27;</span>,</span><br><span class=\"line\">       <span class=\"string\">u&#x27;average_montly_hours&#x27;</span>, <span class=\"string\">u&#x27;time_spend_company&#x27;</span>, <span class=\"string\">u&#x27;Work_accident&#x27;</span>,</span><br><span class=\"line\">       <span class=\"string\">u&#x27;left&#x27;</span>, <span class=\"string\">u&#x27;promotion_last_5years&#x27;</span>, <span class=\"string\">u&#x27;sales&#x27;</span>, <span class=\"string\">u&#x27;salary&#x27;</span>],axis = <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用KMeans算法对处理后的数据进行聚类，设定聚类数为3，并设置随机种子为0以确保结果的可复现性</span></span><br><span class=\"line\"><span class=\"comment\"># 这里fit方法用于训练模型，使其学习数据的聚类结构</span></span><br><span class=\"line\">kmeans = KMeans(n_clusters = <span class=\"number\">3</span>, random_state = <span class=\"number\">0</span>).fit(kmeans_df)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 访问并输出每个聚类中心点的坐标，这些坐标表示了每个聚类的中心位置</span></span><br><span class=\"line\">kmeans.cluster_centers_</span><br></pre></td></tr></table></figure>\n\n<pre><code>array([[0.41014545, 0.51698182],\n       [0.80851586, 0.91170931],\n       [0.11115466, 0.86930085]])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 筛选出离职员工的数据</span></span><br><span class=\"line\">left = data[data.left == <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用布尔索引和 .loc 方法将 KMeans 聚类的标签分配给离职员工数据</span></span><br><span class=\"line\">left_labels = (data.left == <span class=\"number\">1</span>)</span><br><span class=\"line\">data.loc[left_labels, <span class=\"string\">&#x27;label&#x27;</span>] = kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 重新获取带有标签的离职员工数据</span></span><br><span class=\"line\">left = data[data.left == <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建一个新的图形窗口</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置 x 轴标签为满意度水平</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction Level&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置 y 轴标签为最后一次评估结果</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置图形标题为“离职员工的3个聚类”</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;3 Clusters of employees who left&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制不同聚类的离职员工的满意度水平和最后一次评估结果</span></span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">0</span>], left.last_evaluation[left.label==<span class=\"number\">0</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">1</span>], left.last_evaluation[left.label==<span class=\"number\">1</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;g&#x27;</span>)</span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">2</span>], left.last_evaluation[left.label==<span class=\"number\">2</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;b&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加图例，解释不同聚类的含义，并设置图例的位置和字体大小</span></span><br><span class=\"line\">plt.legend([<span class=\"string\">&#x27;Winners&#x27;</span>, <span class=\"string\">&#x27;Frustrated&#x27;</span>, <span class=\"string\">&#x27;Bad Match&#x27;</span>], loc=<span class=\"number\">3</span>, fontsize=<span class=\"number\">15</span>, frameon=<span class=\"literal\">True</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_45_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_45_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h1><p>关注公众号“生信之巅”</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"Scikit-learn","path":"api/tags/Scikit-learn.json"}]},{"title":"深入理解特征标准化：为何、如何及其重要性","slug":"特征标准化","date":"2024-08-12T10:30:19.000Z","updated":"2024-08-12T12:01:50.986Z","comments":true,"path":"api/articles/特征标准化.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg","content":"<h1 id=\"引言\"><a href=\"#引言\" class=\"headerlink\" title=\"引言\"></a>引言</h1><p>在数据驱动的时代，无论是机器学习模型的构建、数据分析的深入探索，还是统计建模的精确预测，数据预处理都是不可或缺的一环。数据预处理如同烹饪前的食材准备，它决定了最终成果的质量与口感。在众多预处理技术中，特征标准化（Feature Standardization）以其独特的优势，成为了提升模型性能、加速算法收敛、确保数值稳定性的重要手段。</p>\n<p>特征标准化，简而言之，是通过数学变换将原始数据特征转换为具有特定均值（通常为0）和标准差（通常为1）的新数据的过程。这一过程不仅消除了不同特征之间因量纲差异而可能导致的偏见，还使得模型在训练过程中能够更加高效地遍历参数空间，从而更快地找到最优解。此外，特征标准化还有助于避免极端值对模型训练造成的干扰，确保数值计算的稳定性和准确性。</p>\n<p>本文旨在深入探讨特征标准化的概念、方法、应用场景及其重要性。我们将从定义出发，逐步解析特征标准化的原理与优势；通过实例展示如何在实践中应用特征标准化技术；并探讨在实施过程中可能遇到的挑战与注意事项。希望通过本文的阐述，读者能够全面理解并掌握特征标准化的精髓，从而在数据分析和机器学习项目中更加得心应手。</p>\n<h1 id=\"一、特征标准化的基本概念\"><a href=\"#一、特征标准化的基本概念\" class=\"headerlink\" title=\"一、特征标准化的基本概念\"></a>一、特征标准化的基本概念</h1><p><strong>定义解析</strong>：</p>\n<p><code>特征标准化</code>，又称Z-score标准化或标准差标准化，是一种将数据按比例缩放，使其落入一个小的特定区间（通常是-1到1之间，但并非严格限制）的技术。这一过程主要通过去除数据的均值并除以数据的标准差来实现，从而确保处理后的数据具有单位方差和指定的均值（在Z-score标准化中，均值通常为0）。特征标准化的数学表达式通常基于Z-score公式，如下所示：</p>\n<p>$Z &#x3D; \\frac{X - \\mu}{\\sigma}$</p>\n<p>其中，$X$ 是原始数据特征中的某个值，$\\mu$ 是该特征所有值的均值，$\\sigma$ 是该特征所有值的标准差，而<em>Z</em>则是经过标准化处理后的新值。通过这个公式，我们可以将任何一组数据转换为具有相同尺度的数据，使得不同量纲或分布的数据能够在同一框架下进行比较和分析。</p>\n<p><strong>与归一化的区别</strong>：</p>\n<p>特征标准化与数据归一化（Min-Max Scaling）虽然都旨在将数据缩放到一个统一的范围内，但它们在实现方法和适用场景上存在显著差异。</p>\n<ul>\n<li><p><strong>实现方法</strong>：特征标准化通过去除均值并除以标准差来实现，而数据归一化则是通过将数据缩放到指定的最小值和最大值之间（通常是0到1）来完成。归一化的数学表达式可以表示为：</p>\n<p>$X_{\\text{norm}} &#x3D; {\\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}}\\times (range_{max} - range_{min}) + range_{min}$</p>\n<p>其中，$X_{\\text{norm}}$ 是归一化后的值，$X_{\\min}$ 和 $X_{\\max}$ 分别是原始数据中的最小值和最大值，$range_{max}$和$range_{min}$分别是缩放后的目标范围的最小值和最大值。</p>\n</li>\n<li><p><strong>适用场景</strong>：特征标准化更适合于那些分布符合高斯分布（或接近高斯分布）的数据集，以及那些对异常值不敏感或希望保留异常值影响的场景。因为标准化不会改变数据的分布形状，只是进行了尺度上的缩放。相比之下，归一化更适合于那些数据分布范围已知且较为稳定的场景，尤其是当数据分布明显偏离高斯分布时。此外，归一化对于需要限制数据范围到特定区间的算法（如某些神经网络层的激活函数）特别有用。</p>\n</li>\n</ul>\n<h1 id=\"二、为何需要特征标准化\"><a href=\"#二、为何需要特征标准化\" class=\"headerlink\" title=\"二、为何需要特征标准化\"></a>二、为何需要特征标准化</h1><p>在机器学习和数据科学领域，特征标准化是一项至关重要的预处理步骤，它对于提升模型性能、加快训练过程以及确保数值计算的稳定性具有显著作用。以下是特征标准化的几个关键原因：</p>\n<h2 id=\"1-消除量纲影响\"><a href=\"#1-消除量纲影响\" class=\"headerlink\" title=\"1. 消除量纲影响\"></a>1. 消除量纲影响</h2><p>不同特征往往具有不同的量纲和度量单位，例如，一个特征可能表示年龄（以年为单位），而另一个特征可能表示收入（以美元为单位）。这些不同量纲的数据在数值上差异巨大，如果直接用于模型训练，会导致某些特征在模型中的权重被不恰当地放大或缩小，从而影响模型的训练效果和泛化能力。通过特征标准化，即将所有特征缩放到同一尺度（如均值为0，标准差为1），可以消除这种量纲差异，使得每个特征在模型训练过程中都能被公平对待。</p>\n<h2 id=\"2-加快收敛速度\"><a href=\"#2-加快收敛速度\" class=\"headerlink\" title=\"2. 加快收敛速度\"></a>2. 加快收敛速度</h2><p>在大多数机器学习算法中，尤其是基于梯度下降的优化算法，特征标准化能够显著加快收敛速度。梯度下降算法通过计算损失函数关于模型参数的梯度来更新参数，以最小化损失函数。如果特征未经过标准化处理，不同特征的数值范围差异可能导致梯度在更新过程中呈现不同的步长，使得优化过程变得曲折且缓慢。通过标准化，所有特征的梯度更新步长变得相对一致，从而加快了算法的收敛速度，减少了达到最优解所需的迭代次数。</p>\n<h2 id=\"3-提升模型性能\"><a href=\"#3-提升模型性能\" class=\"headerlink\" title=\"3. 提升模型性能\"></a>3. 提升模型性能</h2><p>多项研究表明，特征标准化能够显著提升模型的准确率和稳定性。标准化后的数据使得模型更容易学习到数据中的真实模式，而不是被数据的量纲差异所误导。此外，标准化还有助于减少过拟合的风险，因为标准化后的数据分布更加均匀，减少了模型对特定数据点的过度依赖。通过实例或研究数据展示，我们可以发现，在相同的数据集和模型架构下，经过标准化的模型往往能够取得更高的准确率和更低的误差率。</p>\n<h2 id=\"4-避免数值问题\"><a href=\"#4-避免数值问题\" class=\"headerlink\" title=\"4. 避免数值问题\"></a>4. 避免数值问题</h2><p>极端值（如非常大或非常小的数值）在数据集中是常见的，它们可能导致数值计算问题，如梯度爆炸或梯度消失，进而影响模型的训练过程。梯度爆炸指的是在梯度更新过程中，梯度值变得异常大，导致模型参数更新不稳定；而梯度消失则相反，梯度值变得非常小，使得模型参数几乎不更新。通过特征标准化，可以将极端值限制在一个合理的范围内，从而有效避免这些数值问题，确保模型训练的顺利进行。</p>\n<h1 id=\"三、特征标准化的方法\"><a href=\"#三、特征标准化的方法\" class=\"headerlink\" title=\"三、特征标准化的方法\"></a>三、特征标准化的方法</h1><h2 id=\"Z-score标准化\"><a href=\"#Z-score标准化\" class=\"headerlink\" title=\"Z-score标准化\"></a>Z-score标准化</h2><p>Z-score标准化，也称为标准差标准化，是最常用的特征标准化方法之一。它通过计算每个特征值的Z分数（即该值与其均值的差除以标准差）来实现数据的标准化。具体计算过程如下：</p>\n<ol>\n<li><p><strong>计算均值</strong>：首先，对于每个特征，计算其所有样本值的均值（$\\mu$）。均值反映了该特征的中心趋势。</p>\n</li>\n<li><p><strong>计算标准差</strong>：接着，计算该特征的标准差（$\\sigma$）。标准差衡量了数据点相对于均值的离散程度，是数据分布宽度的一个度量。</p>\n</li>\n<li><p><strong>标准化处理</strong>：最后，使用Z-score公式将每个特征值转换为标准化后的值（$Z$）。公式为：</p>\n<p>$Z &#x3D; \\frac{X - \\mu}{\\sigma}$</p>\n<p>其中，$X$ 是原始特征值，$Z$ 是转换后的标准化值。</p>\n</li>\n</ol>\n<p>Z-score标准化的作用在于将数据转换为均值为0、标准差为1的分布，从而消除了不同特征之间的量纲差异，使得它们在模型训练中具有相同的权重。此外，Z-score标准化对异常值相对不敏感，因为它依赖于整个数据集的统计特性（均值和标准差）。Z-score标准化会改变原始数据的稀疏性（原来很多非零数据变为0）及分布，而且并不是归一化的。</p>\n<h2 id=\"MinMaxScaler标准化\"><a href=\"#MinMaxScaler标准化\" class=\"headerlink\" title=\"MinMaxScaler标准化\"></a>MinMaxScaler标准化</h2><p>虽然Min-Max标准化并不完全等同于特征标准化（因为它不改变数据的分布形状和稀疏性，只是进行了线性缩放），但它仍然是一种常用的数据缩放方法，并经常与特征标准化进行比较。Min-Max标准化的原理是将数据缩放到一个指定的最小值和最大值之间（通常是0和1），其计算公式为：</p>\n<p>$X_{\\text{norm}} &#x3D; {\\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}}\\times (range_{max} - range_{min}) + range_{min}$</p>\n<p>其中，$X_{\\text{norm}}$ 是归一化后的值，$X$ 是原始特征值，$X_{\\min}$ 和 $X_{\\max}$ 分别是该特征的最小值和最大值，$range_{max}$和$range_{min}$分别是缩放后的目标范围的最小值和最大值。</p>\n<p>Min-Max标准化适用于那些需要数据范围限制在特定区间的场景，比如某些神经网络的激活函数。然而，它对于新数据的加入比较敏感，因为新数据的最大值和最小值可能会改变整个数据集的缩放比例。</p>\n<h2 id=\"MaxAbsScaler\"><a href=\"#MaxAbsScaler\" class=\"headerlink\" title=\"MaxAbsScaler\"></a>MaxAbsScaler</h2><p>分别对每个特征进行缩放和平移，使得每个特征的最大绝对值为1，最终的值在[-1, 1]。该法不会导致数据整体形态发生大的变化，因此不破坏稀疏性（非零数据不会变为0，0还是0）。因此，可用于比较稀疏的数据。虽然会对分布造成一定的改变，但大致形态接近。</p>\n<p>$X_{\\text{norm}} &#x3D; \\frac{X}{\\left\\vert X_{max} \\right\\vert}$</p>\n<h2 id=\"保持分布的归一化缩放\"><a href=\"#保持分布的归一化缩放\" class=\"headerlink\" title=\"保持分布的归一化缩放\"></a>保持分布的归一化缩放</h2><p>若把一个样本用向量来表示，对于不全是0的向量，对其进行独立于其他样本的缩放，从而使其范数等于1，即将样本分别表转化为单位范数。</p>\n<p><code>L1范数</code>是向量中所有元素值的绝对值之和；<code>L2范数</code>是所有元素平方和的平方根；<code>inf范数</code>指所有元素最大绝对值。</p>\n<p><strong>L2范数标准化（也称为单位向量标准化）</strong>：将特征向量缩放为具有单位L2范数（即欧几里得距离）。这种方法常用于文本数据处理（文本分类、聚类）或图像处理中，以确保特征向量之间的比较是公平的。该缩放基本保持了原始数据的分布，并且对范围进行了归一化（[0, 1]）。</p>\n<p>$X_{\\text{norm}} &#x3D; \\frac{X}{\\left\\vert\\left\\vert X \\right\\vert\\right\\vert}$</p>\n<h2 id=\"缩放含离群值的特征（RobustScaler）\"><a href=\"#缩放含离群值的特征（RobustScaler）\" class=\"headerlink\" title=\"缩放含离群值的特征（RobustScaler）\"></a>缩放含离群值的特征（RobustScaler）</h2><p>若数据包含很多异常离群值，用Z-score方法效果不佳。可以使用RobustScaler替代。</p>\n<p><strong>原理</strong>：通过计算每个特征的中位数和四分位数范围来进行数据缩放。具体来说，它将每个特征的值减去该特征的中位数，然后再除以该特征的四分位数范围。这种方法可以有效地处理异常值的影响，因为四分位数范围对异常值不敏感，从而使得标准化后的数据更加稳定和可靠。</p>\n<p><strong>应用</strong>：RobustScaler在机器学习模型中的应用主要体现在提高模型的泛化能力和稳定性上。通过减少异常值对数据分析的影响，RobustScaler可以帮助模型更好地学习数据的内在规律，从而提高模型的预测准确性和稳定性。</p>\n<p>$X_{\\text{norm}} &#x3D; \\frac{X-Q1}{IQR}$</p>\n<p>其中 $X$ 是原始数据值 $Q1$ 是第一四分位数 $IQR$ 是四分位数间距，即 $Q3−Q1$。</p>\n<h2 id=\"其他标准化技术\"><a href=\"#其他标准化技术\" class=\"headerlink\" title=\"其他标准化技术\"></a>其他标准化技术</h2><p>除了上述标准化之外，还存在其他几种标准化方法，每种方法都有其特定的适用场景：</p>\n<ul>\n<li><p><strong>小数定标标准化（Decimal Scaling Normalization）</strong>：通过移动数据的小数点位置来进行标准化。具体移动多少位取决于数据的最大值。这种方法适用于数据范围已知且相对稳定的场景。</p>\n</li>\n<li><p><strong>对数标准化</strong>：对于某些具有长尾分布的数据，可以通过取对数来减少极端值的影响，并进行标准化处理。这种方法在经济学和金融数据分析中尤为常见。</p>\n</li>\n</ul>\n<p>每种标准化方法都有其独特的优势和局限性，因此在选择时应根据数据的特性、模型的需求以及业务场景来综合考虑。</p>\n<h1 id=\"四、特征标准化的实施步骤\"><a href=\"#四、特征标准化的实施步骤\" class=\"headerlink\" title=\"四、特征标准化的实施步骤\"></a>四、特征标准化的实施步骤</h1><h2 id=\"数据准备\"><a href=\"#数据准备\" class=\"headerlink\" title=\"数据准备\"></a>数据准备</h2><p>在进行特征标准化之前，数据准备是至关重要的一步。数据准备阶段主要包括数据清洗和预处理，以确保数据的质量和一致性，为后续的标准化过程打下坚实的基础。以下是数据准备阶段的关键步骤：</p>\n<ol>\n<li><p><strong>数据清洗</strong>：检查并处理数据中的缺失值、重复值、错误格式等问题。对于缺失值，可以采用填充（如均值填充、中位数填充、众数填充或插值法）或删除的方法进行处理。对于重复值，根据业务需求决定是保留还是删除。</p>\n</li>\n<li><p><strong>异常值处理</strong>：识别并处理数据中的异常值。异常值可能对标准化过程和模型训练产生不利影响。可以通过统计方法（如箱线图、IQR四分位距法）或基于模型的方法（如孤立森林）来检测异常值，并采取相应的处理措施（如删除、替换或标记）。</p>\n</li>\n<li><p><strong>数据划分</strong>：将数据集划分为训练集、验证集和测试集（如果可用）。虽然这一步不是直接针对特征标准化的，但它对于后续评估标准化效果至关重要。</p>\n</li>\n</ol>\n<h2 id=\"选择标准化方法\"><a href=\"#选择标准化方法\" class=\"headerlink\" title=\"选择标准化方法\"></a>选择标准化方法</h2><p>在选择标准化方法时，需要考虑数据类型、数据分布、模型需求以及业务场景等因素。对于大多数情况，Z-score标准化是一个安全且有效的选择，因为它能够消除量纲差异，并且对数据分布形状的影响较小。然而，在某些特定场景下，如需要限制数据范围到特定区间时，Min-Max标准化可能更为合适。此外，还可以根据数据的具体特性和业务需求探索其他标准化方法。</p>\n<h2 id=\"应用标准化\"><a href=\"#应用标准化\" class=\"headerlink\" title=\"应用标准化\"></a>应用标准化</h2><p>在确定了标准化方法后，可以使用编程工具来实现特征标准化。以下是一个使用Python的Pandas和Scikit-learn库进行Z-score标准化的示例：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 假设df是你的DataFrame，其中包含了需要标准化的特征</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用StandardScaler进行Z-score标准化</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">df_scaled = scaler.fit_transform(df)  <span class=\"comment\"># 注意：这会返回NumPy数组，而不是DataFrame</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果你需要DataFrame格式，可以这样做：</span></span><br><span class=\"line\">df_scaled = pd.DataFrame(df_scaled, columns=df.columns)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 或者，如果你只想标准化DataFrame中的某些列，可以这样做：</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">df[[<span class=\"string\">&#x27;feature1&#x27;</span>, <span class=\"string\">&#x27;feature2&#x27;</span>]] = scaler.fit_transform(df[[<span class=\"string\">&#x27;feature1&#x27;</span>, <span class=\"string\">&#x27;feature2&#x27;</span>]])</span><br></pre></td></tr></table></figure>\n\n<p>注意：在实际应用中，通常只在训练集上调用<code>fit_transform()</code>方法来拟合并转换数据，然后在验证集和测试集上调用<code>transform()</code>方法应用相同的转换规则，以确保数据的一致性。</p>\n<h2 id=\"验证效果\"><a href=\"#验证效果\" class=\"headerlink\" title=\"验证效果\"></a>验证效果</h2><p>验证标准化效果是评估特征标准化对模型性能影响的关键步骤。这通常通过比较标准化前后模型的性能指标（如准确率、召回率、F1分数等）来实现。此外，还可以观察模型训练过程中的收敛速度、损失函数的变化情况以及梯度更新的稳定性等指标来评估标准化的效果。如果标准化后模型的性能有所提升，且训练过程更加稳定，那么可以认为标准化是有效的。反之，则需要考虑调整标准化方法或进一步探索其他数据预处理技术。</p>\n<h1 id=\"五、特征标准化的应用场景\"><a href=\"#五、特征标准化的应用场景\" class=\"headerlink\" title=\"五、特征标准化的应用场景\"></a>五、特征标准化的应用场景</h1><h2 id=\"机器学习模型\"><a href=\"#机器学习模型\" class=\"headerlink\" title=\"机器学习模型\"></a>机器学习模型</h2><p>在机器学习领域，特征标准化是提升模型性能的重要手段之一。以下是几个常见机器学习任务中特征标准化的应用：</p>\n<ul>\n<li><p><strong>分类任务</strong>：在分类问题中，如文本分类、图像识别等，特征标准化可以帮助模型更好地学习数据中的真实模式，减少因量纲差异导致的偏差。通过标准化，模型可以更加公平地对待不同特征，从而提高分类的准确率。</p>\n</li>\n<li><p><strong>回归任务</strong>：在回归问题中，如房价预测、股票价格预测等，特征标准化同样重要。标准化后的数据能够加快梯度下降等优化算法的收敛速度，使模型更快找到最优解。此外，标准化还能提高模型的泛化能力，减少过拟合的风险。</p>\n</li>\n<li><p><strong>聚类任务</strong>：在聚类分析中，如K-means聚类、层次聚类等，特征标准化能够确保不同特征在聚类过程中具有相同的权重，从而得到更加合理和准确的聚类结果。未标准化的数据可能会导致某些特征在聚类过程中占据主导地位，影响聚类的有效性。</p>\n</li>\n</ul>\n<h2 id=\"深度学习\"><a href=\"#深度学习\" class=\"headerlink\" title=\"深度学习\"></a>深度学习</h2><p>在深度学习领域，神经网络训练过程中特征标准化的重要性更加凸显。神经网络通常包含多层非线性变换，如果输入数据未经过标准化处理，可能会导致梯度消失或梯度爆炸问题，严重影响模型的训练效果。通过特征标准化，可以确保输入数据在合理的范围内波动，有助于神经网络学习过程的稳定进行。此外，标准化后的数据还能加速神经网络的收敛速度，提高模型的训练效率。</p>\n<h2 id=\"金融分析\"><a href=\"#金融分析\" class=\"headerlink\" title=\"金融分析\"></a>金融分析</h2><p>在金融领域，特征标准化广泛应用于时间序列分析、风险评估等任务中。在金融时间序列分析中，不同金融指标（如股票价格、汇率、利率等）的量纲和波动范围差异较大，直接用于分析可能导致结果失真。通过特征标准化，可以消除这些差异，使得不同指标在模型中具有相同的权重和重要性。在风险评估领域，标准化后的数据有助于构建更加稳定和准确的风险评估模型，提高风险识别的准确性和及时性。</p>\n<h2 id=\"其他领域\"><a href=\"#其他领域\" class=\"headerlink\" title=\"其他领域\"></a>其他领域</h2><p>除了上述领域外，特征标准化还在生物信息学、图像处理等其他领域发挥着重要作用。在生物信息学中，基因表达数据、蛋白质结构数据等通常需要进行标准化处理，以便进行后续的生物信息学分析和挖掘。在图像处理中，像素值的标准化有助于减少光照变化、噪声等因素对图像质量的影响，提高图像识别和分析的准确性。</p>\n<h1 id=\"六、注意事项与挑战\"><a href=\"#六、注意事项与挑战\" class=\"headerlink\" title=\"六、注意事项与挑战\"></a>六、注意事项与挑战</h1><h2 id=\"数据泄露\"><a href=\"#数据泄露\" class=\"headerlink\" title=\"数据泄露\"></a>数据泄露</h2><p>在进行特征标准化时，需要特别注意数据泄露的问题。特别是在使用交叉验证或测试集来评估模型性能时，应避免在测试集上直接使用训练集计算得到的统计量（如均值和标准差）进行标准化。这样做会导致数据泄露问题，使得模型在测试集上的性能被高估。正确的做法是使用训练集计算统计量，并仅对训练集进行标准化处理；在评估模型性能时，应对测试集使用训练集统计量进行标准化，以确保评估的公正性和准确性。</p>\n<h2 id=\"异常值处理\"><a href=\"#异常值处理\" class=\"headerlink\" title=\"异常值处理\"></a>异常值处理</h2><p>异常值处理是特征标准化前的重要步骤之一。异常值可能会对统计量的计算产生显著影响，导致标准化后的数据出现偏差。因此，在进行特征标准化之前，应对数据进行全面的异常值检测和处理。常见的异常值处理方法包括删除、替换（如使用中位数或均值替换）、标记（如设置标志位）等。根据数据的特性和业务需求选择合适的异常值处理方法至关重要。</p>\n<h2 id=\"选择适合的标准化方法\"><a href=\"#选择适合的标准化方法\" class=\"headerlink\" title=\"选择适合的标准化方法\"></a>选择适合的标准化方法</h2><p>选择适合的标准化方法对于提高模型性能至关重要。不同的标准化方法适用于不同的数据类型和业务场景。例如，Z-score标准化适用于大多数连续型数据的标准化处理；Min-Max标准化适用于需要限制数据范围到特定区间的场景；小数定标标准化适用于数据范围已知且相对稳定的场景等。在选择标准化方法时，应充分考虑数据的特性、模型的需求以及业务场景的要求，以确保标准化过程的有效性和合理性。</p>\n<h1 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h1><p>在本文中，我们全面探讨了特征标准化的定义、重要性、方法、实施步骤以及广泛的应用场景。特征标准化作为数据预处理的关键步骤之一，通过调整数据的分布和量纲，使得不同特征在模型中具有相同的权重和重要性，从而提高了模型的训练效率和预测性能。</p>\n<p>首先，我们回顾了特征标准化的基本定义，即通过对数据进行缩放或平移操作，使其满足特定的统计特性（如均值为0，标准差为1）。接着，我们强调了特征标准化在机器学习、深度学习以及金融分析等领域中的重要性，指出它能够有效减少模型训练时间、提高模型泛化能力并优化模型性能。</p>\n<p>在方法部分，我们介绍了多种常见的特征标准化方法，包括Z-score标准化、Min-Max标准化以及小数定标标准化等，并讨论了它们各自的适用场景和优缺点。通过对比不同方法的特点和效果，读者可以根据实际需求选择最适合的标准化方法。</p>\n<p>在实施步骤方面，我们详细阐述了数据准备、选择标准化方法、应用标准化以及验证效果等关键步骤。这些步骤为读者提供了从数据清洗到模型评估的完整流程，有助于他们在实际项目中有效地应用特征标准化技术。</p>\n<p>此外，我们还探讨了特征标准化在多个领域的应用场景，包括机器学习模型中的分类、回归和聚类任务，深度学习中的神经网络训练，以及金融分析中的时间序列分析和风险评估等。这些应用案例不仅展示了特征标准化的广泛适用性，还进一步强调了其在解决实际问题中的重要作用。</p>\n<p>展望未来，随着数据科学和机器学习技术的不断发展，特征标准化技术也将迎来新的发展机遇。自动化标准化工具的发展将使得特征标准化的过程更加简便快捷，降低了技术门槛并提高了工作效率。同时，随着新算法和新模型的不断涌现，特征标准化的方法和应用场景也将不断拓展和深化。</p>\n<p>最后，我们鼓励读者在自己的项目中积极尝试应用特征标准化技术，并通过实践不断积累经验。通过分享实践经验和学习心得，我们可以共同推动特征标准化技术的发展和应用，为数据科学和机器学习领域的发展贡献自己的力量。</p>\n<h1 id=\"关注我\"><a href=\"#关注我\" class=\"headerlink\" title=\"关注我\"></a>关注我</h1><p>关注公众号“生信之巅”。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"IT","path":"api/categories/IT.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"特征工程","path":"api/tags/特征工程.json"}]}]}
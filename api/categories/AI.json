{"name":"AI","postlist":[{"title":"PyTorch实战-利用卷积神经网络完成手写数字识别","slug":"PyTorch实战-利用卷积神经网络完成手写数字识别","date":"2024-09-01T04:37:55.000Z","updated":"2024-09-01T04:57:42.611Z","comments":true,"path":"api/articles/PyTorch实战-利用卷积神经网络完成手写数字识别.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>卷积神经网络（Convolutional Neural Networks, CNNs）是一种特殊类型的神经网络，在图像和视频识别、推荐系统、图像分类、医学图像分析、自然语言处理等领域有着广泛的应用。它们能够自动从原始图像中提取特征，并通过多层网络结构学习这些特征的高级表示。本文通过手写数字识别项目带大家学习卷积神经网络。</p>\n<h1 id=\"卷积神经网络基本概念\"><a href=\"#卷积神经网络基本概念\" class=\"headerlink\" title=\"卷积神经网络基本概念\"></a>卷积神经网络基本概念</h1><h2 id=\"CNN结构\"><a href=\"#CNN结构\" class=\"headerlink\" title=\"CNN结构\"></a>CNN结构</h2><p>输入层（Input Layer）–&gt; {卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）–&gt; 卷积层（Convolutional Layer） –&gt; 池化层（Pooling Layer）}(重复) –&gt; 全连接层（Fully Connected Layer）</p>\n<h2 id=\"卷积层\"><a href=\"#卷积层\" class=\"headerlink\" title=\"卷积层\"></a>卷积层</h2><p>在深度学习和计算机视觉中，尤其是在处理卷积神经网络（CNN）时，计算输出尺寸（Output Size）的公式非常重要。您给出的公式是卷积层输出尺寸计算的一个基本公式，但通常我们会用更直观的符号来表示它。下面是该公式转换为常用表示方法的形式：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - K}{S} \\right\\rfloor + 1 $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），通常是输出特征图的高度或宽度（假设它们是相等的，即正方形特征图）。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度或宽度。</li>\n<li>$P$ 代表填充（Padding）的大小，即在输入特征图的边界上添加的零的层数。注意，这里的 $2P$ 表示在输入特征图的两侧（或上下两侧，取决于维度）都添加了 $P$ 层的零。</li>\n<li>$K$ 代表卷积核（Kernel Size）的大小，即卷积核的高度或宽度（在正方形卷积核的情况下）。</li>\n<li>$S$ 代表步长（Stride），即卷积核在输入特征图上移动的步数。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<p>这个公式适用于计算卷积层（包括标准卷积层和转置卷积层，但转置卷积层有额外的参数和复杂性）后的输出特征图尺寸。在实际应用中，了解如何根据这些参数调整网络结构以得到期望的输出尺寸是非常重要的。</p>\n<h2 id=\"激活函数（Activation-Function）\"><a href=\"#激活函数（Activation-Function）\" class=\"headerlink\" title=\"激活函数（Activation Function）\"></a>激活函数（Activation Function）</h2><p>激活函数用于在卷积层（以及其他类型的神经网络层）之后引入非线性。常见的激活函数包括ReLU（Rectified Linear Unit，修正线性单元）、sigmoid和tanh等。ReLU因其简单性和减少梯度消失问题的能力而在CNN中广泛使用。</p>\n<h2 id=\"池化层\"><a href=\"#池化层\" class=\"headerlink\" title=\"池化层\"></a>池化层</h2><p>池化层（Pooling Layer）在卷积神经网络（CNN）中扮演着重要的角色，主要用于特征融合和降维，以减少计算量和控制过拟合。池化层的输入尺寸和输出尺寸计算公式可以根据不同的参数设置而有所不同，但基本思路是相似的。</p>\n<h3 id=\"池化层的输出尺寸计算公式\"><a href=\"#池化层的输出尺寸计算公式\" class=\"headerlink\" title=\"池化层的输出尺寸计算公式\"></a>池化层的输出尺寸计算公式</h3><p>池化层的输出尺寸计算公式可以表示为：</p>\n<p>$$ O &#x3D; \\left\\lfloor \\frac{I + 2P - F}{S} + 1 \\right\\rfloor $$</p>\n<p>其中：</p>\n<ul>\n<li>$O$ 代表输出尺寸（Output Size），即池化层输出的特征图的高度和宽度。</li>\n<li>$I$ 代表输入尺寸（Input Size），即输入特征图的高度和宽度。</li>\n<li>$F$ 是池化窗口（Pooling Window）的大小，即池化操作覆盖的输入特征图的区域大小。</li>\n<li>$S$ 是步长（Stride），即池化窗口在输入特征图上移动的步数。</li>\n<li>$P$ 是填充（Padding），即在输入特征图的边界上添加的零的层数，用于控制输出尺寸。</li>\n<li>$\\left\\lfloor \\cdot \\right\\rfloor$ 表示向下取整操作，因为像素数必须是整数。</li>\n</ul>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><ul>\n<li>池化层通常不涉及权重和偏置参数，因此它们不会影响模型的学习能力，但对于减少计算量和控制过拟合非常有帮助。</li>\n<li>在实际应用中，池化窗口的大小$F$和步长$S$通常设置为相同的值，如2或3，这样可以更有效地降低特征图的维度。</li>\n<li>填充$P$的值可以是0（无填充），也可以是其他正整数（有填充），具体取决于需要保持输出特征图尺寸与输入特征图尺寸的比例关系。</li>\n</ul>\n<h2 id=\"全连接层（Fully-Connected-Layer-FC-Layer）\"><a href=\"#全连接层（Fully-Connected-Layer-FC-Layer）\" class=\"headerlink\" title=\"全连接层（Fully Connected Layer, FC Layer）\"></a>全连接层（Fully Connected Layer, FC Layer）</h2><p>在CNN的末端，通常会有几个全连接层。这些层中的每个神经元都与前一层的所有神经元相连接。全连接层的作用是将前面层学到的“分布式特征表示”映射到样本标记空间。在分类任务中，全连接层的输出可以传递给softmax函数来生成最终的类别概率分布。</p>\n<h2 id=\"参数共享（Parameter-Sharing）\"><a href=\"#参数共享（Parameter-Sharing）\" class=\"headerlink\" title=\"参数共享（Parameter Sharing）\"></a>参数共享（Parameter Sharing）</h2><p>在CNN中，卷积核的参数是在整个输入数据上共享的。这意味着无论数据中的哪个位置，卷积核都使用相同的权重和偏置参数进行卷积操作。这种参数共享机制减少了模型的参数量，并有助于模型学习到输入数据的空间层次结构。</p>\n<h2 id=\"局部连接（Local-Connectivity）\"><a href=\"#局部连接（Local-Connectivity）\" class=\"headerlink\" title=\"局部连接（Local Connectivity）\"></a>局部连接（Local Connectivity）</h2><p>在卷积层中，每个神经元仅与输入数据的一个局部区域（即感受野）相连接，而不是与整个输入数据相连接。这种局部连接机制使得CNN能够学习到数据的局部特征，这与人类视觉系统的处理机制相似。</p>\n<h2 id=\"反向传播（Backpropagation）\"><a href=\"#反向传播（Backpropagation）\" class=\"headerlink\" title=\"反向传播（Backpropagation）\"></a>反向传播（Backpropagation）</h2><p>反向传播算法是训练CNN（以及其他类型的神经网络）的关键算法。在训练过程中，通过计算损失函数关于网络参数的梯度，并利用梯度下降（或其变体）来更新网络参数，以最小化损失函数。反向传播算法通过链式法则在网络的每一层中传播梯度信息。</p>\n<h1 id=\"MNIST数据集介绍\"><a href=\"#MNIST数据集介绍\" class=\"headerlink\" title=\"MNIST数据集介绍\"></a>MNIST数据集介绍</h1><p>在探索机器学习领域的广阔天地时，手写数字识别作为一个经典且基础的任务，始终占据着重要的地位。而MNIST（Modified National Institute of Standards and Technology）数据集，正是这一任务中最常用、最经典的数据集之一。本文将首先介绍MNIST数据集，为后续的手写数字识别模型训练与测试打下坚实的基础。</p>\n<h2 id=\"MNIST数据集概述\"><a href=\"#MNIST数据集概述\" class=\"headerlink\" title=\"MNIST数据集概述\"></a>MNIST数据集概述</h2><p>MNIST数据集由Yann LeCun等人搜集整理，是一个大型的手写体数字数据库。该数据集最初来源于NIST（National Institute of Standards and Technology）的两个数据库：专用数据库1（Special Database 1）和特殊数据库3（Special Database 3）。通过精心筛选与预处理，MNIST最终成为了一个包含大量手写数字图像的标准数据集，广泛应用于各种图像处理系统和机器学习算法的训练与测试中。</p>\n<h2 id=\"数据集的构成\"><a href=\"#数据集的构成\" class=\"headerlink\" title=\"数据集的构成\"></a>数据集的构成</h2><p>MNIST数据集由60,000个训练样本和10,000个测试样本组成，每个样本都是一张<code>28x28</code>像素的灰度图像，表示一个手写数字（0-9）。这些图像均已被归一化，像素值范围在0到255之间，其中<code>0</code>代表黑色，<code>255</code>代表白色。数据集的图像由来自不同人群的手写体构成，包括高中生和美国人口普查局的工作人员，确保了数据的多样性和代表性。</p>\n<h2 id=\"数据集的特点\"><a href=\"#数据集的特点\" class=\"headerlink\" title=\"数据集的特点\"></a>数据集的特点</h2><ol>\n<li><p><strong>简单性</strong>：虽然MNIST数据集包含的手写数字种类繁多，但由于其图像尺寸小（28x28像素）、像素深度低（灰度图像），使得处理起来相对简单。这使其成为机器学习初学者练习图像识别、模式识别等任务的理想选择。</p>\n</li>\n<li><p><strong>代表性</strong>：MNIST数据集中的手写数字覆盖了各种书写风格和变体，使得训练出的模型能够较好地泛化到未知的手写数字上。因此，该数据集在评估机器学习算法性能时具有很高的参考价值。</p>\n</li>\n<li><p><strong>广泛应用</strong>：由于其简单性和代表性，MNIST数据集在机器学习领域得到了广泛应用。从简单的神经网络到复杂的深度学习模型，几乎所有的图像识别算法都会使用MNIST数据集进行训练和测试。</p>\n</li>\n</ol>\n<h2 id=\"数据集的下载与使用\"><a href=\"#数据集的下载与使用\" class=\"headerlink\" title=\"数据集的下载与使用\"></a>数据集的下载与使用</h2><p>MNIST数据集可以通过多种途径下载，其中最常用的方式是通过互联网直接下载。用户可以从Yann LeCun的官方网站（<a href=\"http://yann.lecun.com/exdb/mnist/%EF%BC%89%E6%88%96%E5%85%B6%E4%BB%96%E6%95%B0%E6%8D%AE%E5%85%B1%E4%BA%AB%E5%B9%B3%E5%8F%B0%E8%8E%B7%E5%8F%96%E8%AF%A5%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82%E4%B8%8B%E8%BD%BD%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E9%80%9A%E5%B8%B8%E5%8C%85%E5%90%AB%E5%9B%9B%E4%B8%AA%E6%96%87%E4%BB%B6%EF%BC%9A%E8%AE%AD%E7%BB%83%E9%9B%86%E5%9B%BE%E5%83%8F%E3%80%81%E8%AE%AD%E7%BB%83%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E5%9B%BE%E5%83%8F%E5%92%8C%E6%B5%8B%E8%AF%95%E9%9B%86%E6%A0%87%E7%AD%BE%E3%80%82%E8%BF%99%E4%BA%9B%E6%96%87%E4%BB%B6%E5%9D%87%E4%B8%BA%E5%8E%8B%E7%BC%A9%E6%A0%BC%E5%BC%8F%EF%BC%8C%E7%94%A8%E6%88%B7%E9%9C%80%E8%A6%81%E8%A7%A3%E5%8E%8B%E5%90%8E%E6%89%8D%E8%83%BD%E4%BD%BF%E7%94%A8%E3%80%82\">http://yann.lecun.com/exdb/mnist/）或其他数据共享平台获取该数据集。下载后的数据集通常包含四个文件：训练集图像、训练集标签、测试集图像和测试集标签。这些文件均为压缩格式，用户需要解压后才能使用。</a></p>\n<p>在使用MNIST数据集时，用户需要根据自己的需求进行预处理和加载操作。例如，可以使用Python的NumPy库或Pandas库来读取和处理数据集中的图像和标签信息；也可以使用深度学习框架（如TensorFlow、PyTorch等）中提供的数据加载工具来简化这一过程。</p>\n<h1 id=\"下载并导入数据集\"><a href=\"#下载并导入数据集\" class=\"headerlink\" title=\"下载并导入数据集\"></a>下载并导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入必要的库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn <span class=\"keyword\">as</span> nn</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.optim <span class=\"keyword\">as</span> optim</span><br><span class=\"line\"><span class=\"keyword\">from</span> torchvision <span class=\"keyword\">import</span> datasets, transforms</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch.utils.data <span class=\"keyword\">import</span> DataLoader</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.nn.functional <span class=\"keyword\">as</span> F</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 数据预处理</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Compose组合多个变换，这里将数据转换为张量并进行标准化</span></span><br><span class=\"line\">transform = transforms.Compose([</span><br><span class=\"line\">    transforms.ToTensor(),</span><br><span class=\"line\">    transforms.Normalize((<span class=\"number\">0.5</span>,), (<span class=\"number\">0.5</span>,))</span><br><span class=\"line\">])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 下载MNIST数据集并划分为训练集和测试集</span></span><br><span class=\"line\">train_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">True</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\">test_dataset = datasets.MNIST(root=<span class=\"string\">&#x27;./data&#x27;</span>, train=<span class=\"literal\">False</span>, download=<span class=\"literal\">True</span>, transform=transform)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>查看训练集属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看整个训练集的样本数量及单个样本的形状</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;训练集大小: <span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_dataset)&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 查看第一个样本的数据和标签</span></span><br><span class=\"line\">first_sample, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本数据形状: <span class=\"subst\">&#123;first_sample.shape&#125;</span>&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">f&quot;首个样本标签: <span class=\"subst\">&#123;first_label&#125;</span>&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果想查看前几个样本的具体数据内容，可以通过循环实现</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">5</span>):</span><br><span class=\"line\">    data, label = train_dataset[i]</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&quot;样本 <span class=\"subst\">&#123;i+<span class=\"number\">1</span>&#125;</span> 的数据:\\n<span class=\"subst\">&#123;data&#125;</span>\\n标签: <span class=\"subst\">&#123;label&#125;</span>\\n&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>训练集大小: 60000\n首个样本数据形状: torch.Size([1, 28, 28])\n首个样本标签: 5\n样本 1 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9765, -0.8588,\n          -0.8588, -0.8588, -0.0118,  0.0667,  0.3725, -0.7961,  0.3020,\n           1.0000,  0.9373, -0.0039, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7647, -0.7176, -0.2627,  0.2078,  0.3333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.7647,  0.3490,  0.9843,\n           0.8980,  0.5294, -0.4980, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6157,  0.8667,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.9686, -0.2706, -0.3569, -0.3569,\n          -0.5608, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8588,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.9843,\n           0.5529,  0.4275,  0.9373,  0.8902, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.3725,  0.2235, -0.1608,  0.9843,  0.9843,  0.6078,\n          -0.9137, -1.0000, -0.6627,  0.2078, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.8902, -0.9922,  0.2078,  0.9843, -0.2941,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.0902,  0.9843,  0.4902,\n          -0.9843, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9137,  0.4902,  0.9843,\n          -0.4510, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7255,  0.8902,\n           0.7647,  0.2549, -0.1529, -0.9922, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3647,\n           0.8824,  0.9843,  0.9843, -0.0667, -0.8039, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6471,  0.4588,  0.9843,  0.9843,  0.1765, -0.7882, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8745, -0.2706,  0.9765,  0.9843,  0.4667, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9529,  0.9843,  0.9529, -0.4980,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6392,  0.0196,  0.4353,  0.9843,  0.9843,  0.6235, -0.9843,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6941,  0.1608,\n           0.7961,  0.9843,  0.9843,  0.9843,  0.9608,  0.4275, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.8118, -0.1059,  0.7333,  0.9843,\n           0.9843,  0.9843,  0.9843,  0.5765, -0.3882, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8196, -0.4824,  0.6706,  0.9843,  0.9843,  0.9843,\n           0.9843,  0.5529, -0.3647, -0.9843, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8588,\n           0.3412,  0.7176,  0.9843,  0.9843,  0.9843,  0.9843,  0.5294,\n          -0.3725, -0.9294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.3490,  0.7725,\n           0.9843,  0.9843,  0.9843,  0.9843,  0.9137,  0.0431, -0.9137,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.0667,  0.9843,  0.9843,\n           0.9843,  0.6627,  0.0588,  0.0353, -0.8745, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 5\n\n样本 2 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6000,  0.2471,  0.9843,  0.2471, -0.6078, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.6235,  0.8667,  0.9765,  0.9765,  0.9765,  0.8588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5765,\n           0.7804,  0.9843,  0.9765,  0.8745,  0.8275,  0.9765, -0.5529,\n          -0.9529, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9216, -0.5294,  0.7569,\n           0.9765,  0.9843,  0.9765,  0.5843, -0.3412,  0.9765,  0.9843,\n          -0.0431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9765,  0.9765,\n           0.9765,  0.9843,  0.9765,  0.9765, -0.2471,  0.4824,  0.9843,\n           0.3098, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6000,  0.8667,  0.9843,  0.9843,\n           0.4902, -0.1059,  0.9843,  0.7882, -0.6314, -0.3804,  1.0000,\n           0.3176, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.8667,  0.9765,  0.9765,  0.4039,\n          -0.9059, -0.4118, -0.0510, -0.8353, -1.0000, -1.0000,  0.9843,\n           0.9059, -0.6078, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.7020,  0.2941,  0.9843,  0.8275,  0.6314, -0.3412,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.2941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.9451,  0.3961,  0.9765,  0.8824, -0.4431, -0.8510, -0.7804,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5529,  0.9765,  0.9765, -0.5059, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.5294, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.5529,  0.9843,  0.4902, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,\n           0.9843,  0.5373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4039,\n           0.9294,  0.9765, -0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9843,\n           0.9765,  0.1608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.8039, -0.8039, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.9451,  0.0588,  0.9843,\n           0.4588, -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.7490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.9451,  0.0275,  0.9765,  0.7647,\n          -0.4431, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.6235,  0.2941,  0.9765,  0.3569, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3255,\n           0.9843,  0.7647, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.1059,  0.8667,  0.9843,  0.2706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9529,  0.1451, -0.6235, -0.7725, -0.3333,  0.3961,\n           0.7647,  0.9843,  0.7490,  0.3098, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3333,\n           0.9765,  0.9765,  0.9765,  0.7961,  0.6863,  0.9765,  0.9765,\n           0.9765,  0.5373,  0.0196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7804,\n           0.5608,  0.9765,  0.9765,  0.9843,  0.9765,  0.9765,  0.8275,\n           0.1373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8039,  0.0039,  0.9765,  0.9843,  0.9765,  0.1059, -0.7098,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 0\n\n样本 3 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4745,\n           0.8196, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.5137, -0.3647, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.0588,\n           0.4118, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0118,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.9843,  0.2000,\n           0.6471, -0.6863, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7255,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7882,  0.9922,\n           0.2706, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000,  0.7412,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.4353,  0.9922,\n          -0.0196, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.6392,  0.9216,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5529,  0.9922,\n          -0.5608, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -0.0588,  0.9922,  0.2784, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.8196,  0.8118,  0.9922,\n          -0.7725, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.0588, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.2784,  0.9922,  0.6941,\n          -0.8745, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.4745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902, -0.3255,  0.3961,  0.9451,  0.9922, -0.2863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.2471,  0.9922, -0.3333, -1.0000,\n          -1.0000, -1.0000, -0.6314, -0.6157, -0.0902,  0.1294,  0.1765,\n           0.8902,  0.9059,  0.8353,  0.4039,  0.8902,  0.9765, -0.6863,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000,  0.1765,  0.9843,  0.8588,  0.6235,\n           0.6235,  0.6235,  0.9843,  0.9922,  0.9608,  0.8824,  0.5529,\n           0.1216, -0.2863, -0.7804, -0.9608,  0.8275,  0.9608, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -0.0667,  0.3882,  0.3882,\n           0.3882,  0.3882,  0.3882, -0.2314, -0.5608, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2000,  0.9922,  0.7255, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.0745, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.5529, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922, -0.2471, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.3255,  1.0000,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9922,  0.2000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 4\n\n样本 4 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.0275,  0.9843,  1.0000,\n          -0.5059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.2471,  0.9137,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.0039,  0.9686,  0.9686,  0.9843,\n          -0.5137, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.4667,  0.8510,  0.9686,  0.6549, -0.7569,\n          -0.9373, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.5294,  0.7882,  0.9686,  0.9686, -0.2627, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.2157,  0.9843,  0.9843,  0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8431,  0.9843,  0.9686,  0.8431, -0.4824, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,\n           0.6078,  0.9843,  0.9686, -0.0118, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1843,\n           0.9686,  0.9843,  0.4431, -0.8824, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.3725,  0.8824,\n           0.9686,  0.5137, -0.8196, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.7490,  0.9843,  0.9843,\n           0.9843,  0.2471, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000,  0.1843,  0.9686,  0.9686,\n           0.9686, -0.6941, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -0.6235,  0.7333,  0.9686,  0.9686,\n           0.3490, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.8353,  0.9686,  0.9686,  0.5373,\n          -0.9059, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000,  0.9843,  0.9686,  0.9686, -0.3020,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000,  0.2471,  1.0000,  0.9843,  0.9843, -0.7569,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.6235,  0.7882,  0.9843,  0.9373,  0.0980, -0.9373,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.4980,  0.9686,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8118,  0.5137,  0.9843,  0.7255, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 1\n\n样本 5 的数据:\ntensor([[[-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5686,  0.1608,\n           0.6471,  0.9843,  0.9843, -0.1137, -0.3176,  0.1608, -0.5686,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -0.3176,  0.8196,  0.9765,\n           0.9843,  0.4824,  0.6471,  0.9765,  0.9765,  0.9843,  0.3176,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.9686, -0.5529,  0.8980,  0.9765,  0.4902,\n          -0.4902, -0.9608, -0.9059,  0.4275,  0.9765,  0.9843, -0.0902,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -0.2471,  0.9765,  0.9765,  0.4353, -0.8902,\n          -1.0000, -1.0000, -0.2784,  0.9765,  0.9765,  0.7647, -0.8353,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000,  0.0353,  0.9843,  0.9765,  0.1451, -0.8902, -1.0000,\n          -1.0000, -1.0000,  0.6863,  0.9765,  0.9765, -0.3804, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.0118,  0.9843,  0.9373,  0.3804, -0.9294, -1.0000, -1.0000,\n          -0.9373, -0.3882,  0.9216,  0.9843,  0.0118, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8196,  0.9765,  0.3804, -1.0000, -1.0000, -1.0000, -0.7176,\n           0.5765,  0.9765,  0.9765,  0.3255, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8275,\n           0.9765,  0.9765, -0.7647, -0.8275, -0.0667,  0.5451,  0.8902,\n           0.9843,  0.9765,  0.9686, -0.3961, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.8745,\n           0.8118,  0.9765,  0.9843,  0.9765,  0.9765,  0.9765,  0.7725,\n           0.7804,  0.9765,  0.8118, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.5686,  0.8431,  0.9843,  0.7020,  0.0824, -0.6706, -0.8118,\n           0.5059,  0.9765,  0.1216, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.5137,\n           1.0000,  0.9843, -0.1451, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.4431,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.1686,\n           0.9843,  0.9765, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -0.6471,\n           1.0000,  0.9843, -0.8353, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n           0.7098,  0.9765, -0.5608, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.2471,  0.9765,  0.4824, -0.6706, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -0.8902,  0.4431,  0.9765,  0.3333, -0.9137, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -0.8902,  0.1529,  0.9765, -0.6706, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n         [-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n          -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000]]])\n标签: 9\n</code></pre>\n<p>展示训练集中的第一幅图片：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据和标签</span></span><br><span class=\"line\">first_image, first_label = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将图像数据从形状 (1, 28, 28) 转换为 (28, 28)，以便显示</span></span><br><span class=\"line\">first_image = first_image.squeeze().numpy()  <span class=\"comment\"># 去除通道维度，并转换为numpy数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图像</span></span><br><span class=\"line\">plt.imshow(first_image, cmap=<span class=\"string\">&#x27;gray&#x27;</span>)  <span class=\"comment\"># 使用灰度色阶显示图像</span></span><br><span class=\"line\">plt.title(<span class=\"string\">f&#x27;Label: <span class=\"subst\">&#123;first_label&#125;</span>&#x27;</span>)  <span class=\"comment\"># 显示图像标签作为标题</span></span><br><span class=\"line\">plt.axis(<span class=\"string\">&#x27;off&#x27;</span>)  <span class=\"comment\"># 不显示坐标轴</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/mnist.pytorchOK_files/mnist.pytorchOK_5_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 获取第一个样本的数据</span></span><br><span class=\"line\">first_image, _ = train_dataset[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将张量转换为numpy数组以便打印</span></span><br><span class=\"line\">first_image_np = first_image.numpy()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印原始的三维数组（包含通道维度）</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;原始图像数据（包含通道维度）:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_np)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>原始图像数据（包含通道维度）:\n[[[-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.9764706  -0.85882354 -0.85882354\n   -0.85882354 -0.01176471  0.06666672  0.37254906 -0.79607844\n    0.30196083  1.          0.9372549  -0.00392157 -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.7647059  -0.7176471\n   -0.26274508  0.20784318  0.33333337  0.9843137   0.9843137\n    0.9843137   0.9843137   0.9843137   0.7647059   0.34901965\n    0.9843137   0.8980392   0.5294118  -0.4980392  -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.6156863   0.8666667   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n    0.9843137   0.9843137   0.96862745 -0.27058822 -0.35686272\n   -0.35686272 -0.56078434 -0.69411767 -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.85882354  0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.9843137   0.5529412\n    0.427451    0.9372549   0.8901961  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.372549    0.22352946\n   -0.1607843   0.9843137   0.9843137   0.60784316 -0.9137255\n   -1.         -0.6627451   0.20784318 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.8901961\n   -0.99215686  0.20784318  0.9843137  -0.29411763 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.          0.09019613  0.9843137   0.4901961  -0.9843137\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -0.9137255   0.4901961   0.9843137  -0.45098037\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.7254902   0.8901961   0.7647059\n    0.254902   -0.15294117 -0.99215686 -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.36470586  0.88235295\n    0.9843137   0.9843137  -0.06666666 -0.8039216  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.64705884\n    0.45882356  0.9843137   0.9843137   0.17647064 -0.7882353\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8745098  -0.27058822  0.9764706   0.9843137   0.4666667\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.          0.9529412   0.9843137   0.9529412\n   -0.4980392  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -0.6392157\n    0.0196079   0.43529415  0.9843137   0.9843137   0.62352943\n   -0.9843137  -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -0.69411767  0.16078436  0.79607844\n    0.9843137   0.9843137   0.9843137   0.9607843   0.427451\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -0.8117647  -0.10588235  0.73333335  0.9843137   0.9843137\n    0.9843137   0.9843137   0.5764706  -0.38823527 -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -0.81960785 -0.4823529\n    0.67058825  0.9843137   0.9843137   0.9843137   0.9843137\n    0.5529412  -0.36470586 -0.9843137  -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -0.85882354  0.3411765   0.7176471   0.9843137\n    0.9843137   0.9843137   0.9843137   0.5294118  -0.372549\n   -0.92941177 -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -0.5686275\n    0.34901965  0.77254903  0.9843137   0.9843137   0.9843137\n    0.9843137   0.9137255   0.04313731 -0.9137255  -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.          0.06666672\n    0.9843137   0.9843137   0.9843137   0.6627451   0.05882359\n    0.03529418 -0.8745098  -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]\n  [-1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.         -1.         -1.\n   -1.         -1.         -1.        ]]]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 如果你想去掉通道维度，打印二维矩阵</span></span><br><span class=\"line\">first_image_2d = first_image_np.squeeze()  <span class=\"comment\"># 去掉通道维度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;\\n去掉通道后的二维矩阵:&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(first_image_2d)</span><br></pre></td></tr></table></figure>\n\n<pre><code>去掉通道后的二维矩阵:\n[[-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.9764706  -0.85882354 -0.85882354 -0.85882354 -0.01176471  0.06666672\n   0.37254906 -0.79607844  0.30196083  1.          0.9372549  -0.00392157\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.7647059  -0.7176471  -0.26274508  0.20784318\n   0.33333337  0.9843137   0.9843137   0.9843137   0.9843137   0.9843137\n   0.7647059   0.34901965  0.9843137   0.8980392   0.5294118  -0.4980392\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.6156863   0.8666667   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.9843137   0.9843137   0.9843137   0.96862745\n  -0.27058822 -0.35686272 -0.35686272 -0.56078434 -0.69411767 -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.85882354  0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.9843137   0.5529412   0.427451    0.9372549   0.8901961\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.372549    0.22352946 -0.1607843   0.9843137\n   0.9843137   0.60784316 -0.9137255  -1.         -0.6627451   0.20784318\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8901961  -0.99215686  0.20784318\n   0.9843137  -0.29411763 -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.09019613\n   0.9843137   0.4901961  -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -0.9137255\n   0.4901961   0.9843137  -0.45098037 -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.7254902   0.8901961   0.7647059   0.254902   -0.15294117 -0.99215686\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -0.36470586  0.88235295  0.9843137   0.9843137  -0.06666666\n  -0.8039216  -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.64705884  0.45882356  0.9843137   0.9843137\n   0.17647064 -0.7882353  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -0.8745098  -0.27058822  0.9764706\n   0.9843137   0.4666667  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.          0.9529412\n   0.9843137   0.9529412  -0.4980392  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.6392157   0.0196079   0.43529415  0.9843137\n   0.9843137   0.62352943 -0.9843137  -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -0.69411767  0.16078436  0.79607844  0.9843137   0.9843137   0.9843137\n   0.9607843   0.427451   -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -0.8117647  -0.10588235\n   0.73333335  0.9843137   0.9843137   0.9843137   0.9843137   0.5764706\n  -0.38823527 -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -0.81960785 -0.4823529   0.67058825  0.9843137\n   0.9843137   0.9843137   0.9843137   0.5529412  -0.36470586 -0.9843137\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -0.85882354  0.3411765   0.7176471   0.9843137   0.9843137   0.9843137\n   0.9843137   0.5294118  -0.372549   -0.92941177 -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -0.5686275   0.34901965\n   0.77254903  0.9843137   0.9843137   0.9843137   0.9843137   0.9137255\n   0.04313731 -0.9137255  -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.          0.06666672  0.9843137\n   0.9843137   0.9843137   0.6627451   0.05882359  0.03529418 -0.8745098\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]\n [-1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.         -1.         -1.\n  -1.         -1.         -1.         -1.        ]]\n</code></pre>\n<h1 id=\"定义模型、优化器、损失函数\"><a href=\"#定义模型、优化器、损失函数\" class=\"headerlink\" title=\"定义模型、优化器、损失函数\"></a>定义模型、优化器、损失函数</h1><p>进行2次卷积和2次池化，得到64<em>7</em>17，再进行2次全连接，得到10个输出。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 定义批次大小</span></span><br><span class=\"line\">batch_size = <span class=\"number\">64</span></span><br><span class=\"line\"><span class=\"comment\"># 使用DataLoader加载数据，以便在训练过程中更方便地迭代数据</span></span><br><span class=\"line\">train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">True</span>)</span><br><span class=\"line\">test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义模型</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Net</span>(nn.Module):</span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">        <span class=\"built_in\">super</span>(Net, self).__init__()</span><br><span class=\"line\">        <span class=\"comment\"># 初始化卷积层、池化层、全连接层和dropout层</span></span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个卷积层，用于提取特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为1（适用于灰度图像），输出通道数为32，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次卷积后生成的特征图大小为32*28*28</span></span><br><span class=\"line\">        self.conv1 = nn.Conv2d(<span class=\"number\">1</span>, <span class=\"number\">32</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义最大池化层，用于降低特征维度，减少计算量</span></span><br><span class=\"line\">        <span class=\"comment\"># 池化窗口大小为2x2，步长为2，无padding</span></span><br><span class=\"line\">        <span class=\"comment\"># 第一次池化后生成的特征图大小为32*14*14</span></span><br><span class=\"line\">        self.pool = nn.MaxPool2d(kernel_size=<span class=\"number\">2</span>, stride=<span class=\"number\">2</span>, padding=<span class=\"number\">0</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个卷积层，进一步提取和整合特征</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入通道数为32，输出通道数为64，卷积核大小为5x5，步长为1，padding为2</span></span><br><span class=\"line\">        <span class=\"comment\"># 第二次卷积后生成的特征图大小为64*14*14</span></span><br><span class=\"line\">        self.conv2 = nn.Conv2d(<span class=\"number\">32</span>, <span class=\"number\">64</span>, kernel_size=<span class=\"number\">5</span>, stride=<span class=\"number\">1</span>, padding=<span class=\"number\">2</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第一个全连接层，用于分类前的特征转换</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为64*7*7（这里的尺寸为64*7*7是因为在前向传播时对第二次卷积进行了池化操作），输出大小为1024</span></span><br><span class=\"line\">        self.fc1 = nn.Linear(<span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>, <span class=\"number\">1024</span>)</span><br><span class=\"line\">        </span><br><span class=\"line\">        <span class=\"comment\"># 定义第二个全连接层，用于最终的分类输出</span></span><br><span class=\"line\">        <span class=\"comment\"># 输入大小为1024，输出大小为10（假设分类任务有10个类别）</span></span><br><span class=\"line\">        self.fc2 = nn.Linear(<span class=\"number\">1024</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 定义Dropout层，用于训练过程中的正则化，防止过拟合</span></span><br><span class=\"line\">        <span class=\"comment\"># Dropout比例为0.5，即在训练过程中随机将50%的元素置为0</span></span><br><span class=\"line\">        self.dropout = nn.Dropout(p=<span class=\"number\">0.5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">def</span> <span class=\"title function_\">forward</span>(<span class=\"params\">self, x</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 定义前向传播过程</span></span><br><span class=\"line\">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class=\"line\">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class=\"line\">        x = x.view(-<span class=\"number\">1</span>, <span class=\"number\">64</span> * <span class=\"number\">7</span> * <span class=\"number\">7</span>)</span><br><span class=\"line\">        x = F.relu(self.fc1(x))</span><br><span class=\"line\">        x = self.dropout(x)</span><br><span class=\"line\">        x = self.fc2(x)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> x</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 实例化模型</span></span><br><span class=\"line\">model = Net()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义优化器</span></span><br><span class=\"line\"><span class=\"comment\"># 使用Adam优化器更新模型参数</span></span><br><span class=\"line\">optimizer = optim.Adam(model.parameters(), lr=<span class=\"number\">1e-4</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义损失函数</span></span><br><span class=\"line\"><span class=\"comment\"># 使用交叉熵损失函数进行分类任务</span></span><br><span class=\"line\">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"训练模型\"><a href=\"#训练模型\" class=\"headerlink\" title=\"训练模型\"></a>训练模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 将模型转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">device = torch.device(<span class=\"string\">&quot;cuda&quot;</span> <span class=\"keyword\">if</span> torch.cuda.is_available() <span class=\"keyword\">else</span> <span class=\"string\">&quot;cpu&quot;</span>)</span><br><span class=\"line\">model.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义训练轮数</span></span><br><span class=\"line\">num_epochs = <span class=\"number\">10</span></span><br><span class=\"line\"><span class=\"comment\"># 开始训练过程</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i, (images, labels) <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(train_loader):</span><br><span class=\"line\">        <span class=\"comment\"># 将数据转移到GPU设备上（如果可用）</span></span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 前向传播</span></span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        loss = criterion(outputs, labels)</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 反向传播和优化</span></span><br><span class=\"line\">        optimizer.zero_grad()</span><br><span class=\"line\">        loss.backward()</span><br><span class=\"line\">        optimizer.step()</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\"># 打印损失信息</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i + <span class=\"number\">1</span>) % <span class=\"number\">100</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">            <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Epoch [<span class=\"subst\">&#123;epoch + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;num_epochs&#125;</span>], Step [<span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>/<span class=\"subst\">&#123;<span class=\"built_in\">len</span>(train_loader)&#125;</span>], Loss: <span class=\"subst\">&#123;loss.item():<span class=\"number\">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保存模型参数到文件</span></span><br><span class=\"line\">torch.save(model.state_dict(), <span class=\"string\">&#x27;model.pth&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Epoch [1/10], Step [100/938], Loss: 0.0037\nEpoch [1/10], Step [200/938], Loss: 0.0029\nEpoch [1/10], Step [300/938], Loss: 0.0022\nEpoch [1/10], Step [400/938], Loss: 0.0651\nEpoch [1/10], Step [500/938], Loss: 0.0038\nEpoch [1/10], Step [600/938], Loss: 0.0022\nEpoch [1/10], Step [700/938], Loss: 0.0079\nEpoch [1/10], Step [800/938], Loss: 0.0019\nEpoch [1/10], Step [900/938], Loss: 0.0016\nEpoch [2/10], Step [100/938], Loss: 0.0016\nEpoch [2/10], Step [200/938], Loss: 0.0102\nEpoch [2/10], Step [300/938], Loss: 0.0341\nEpoch [2/10], Step [400/938], Loss: 0.0060\nEpoch [2/10], Step [500/938], Loss: 0.0257\nEpoch [2/10], Step [600/938], Loss: 0.0013\nEpoch [2/10], Step [700/938], Loss: 0.0767\nEpoch [2/10], Step [800/938], Loss: 0.0018\nEpoch [2/10], Step [900/938], Loss: 0.0343\nEpoch [3/10], Step [100/938], Loss: 0.0063\nEpoch [3/10], Step [200/938], Loss: 0.0096\nEpoch [3/10], Step [300/938], Loss: 0.0007\nEpoch [3/10], Step [400/938], Loss: 0.0002\nEpoch [3/10], Step [500/938], Loss: 0.0124\nEpoch [3/10], Step [600/938], Loss: 0.0109\nEpoch [3/10], Step [700/938], Loss: 0.0340\nEpoch [3/10], Step [800/938], Loss: 0.0004\nEpoch [3/10], Step [900/938], Loss: 0.0586\nEpoch [4/10], Step [100/938], Loss: 0.0002\nEpoch [4/10], Step [200/938], Loss: 0.0554\nEpoch [4/10], Step [300/938], Loss: 0.0008\nEpoch [4/10], Step [400/938], Loss: 0.0029\nEpoch [4/10], Step [500/938], Loss: 0.0036\nEpoch [4/10], Step [600/938], Loss: 0.0009\nEpoch [4/10], Step [700/938], Loss: 0.0281\nEpoch [4/10], Step [800/938], Loss: 0.0826\nEpoch [4/10], Step [900/938], Loss: 0.0003\nEpoch [5/10], Step [100/938], Loss: 0.0001\nEpoch [5/10], Step [200/938], Loss: 0.0240\nEpoch [5/10], Step [300/938], Loss: 0.0040\nEpoch [5/10], Step [400/938], Loss: 0.0003\nEpoch [5/10], Step [500/938], Loss: 0.0107\nEpoch [5/10], Step [600/938], Loss: 0.0019\nEpoch [5/10], Step [700/938], Loss: 0.0002\nEpoch [5/10], Step [800/938], Loss: 0.0006\nEpoch [5/10], Step [900/938], Loss: 0.0008\nEpoch [6/10], Step [100/938], Loss: 0.0003\nEpoch [6/10], Step [200/938], Loss: 0.0001\nEpoch [6/10], Step [300/938], Loss: 0.0003\nEpoch [6/10], Step [400/938], Loss: 0.0226\nEpoch [6/10], Step [500/938], Loss: 0.0024\nEpoch [6/10], Step [600/938], Loss: 0.0020\nEpoch [6/10], Step [700/938], Loss: 0.0005\nEpoch [6/10], Step [800/938], Loss: 0.0007\nEpoch [6/10], Step [900/938], Loss: 0.0188\nEpoch [7/10], Step [100/938], Loss: 0.0286\nEpoch [7/10], Step [200/938], Loss: 0.0007\nEpoch [7/10], Step [300/938], Loss: 0.0004\nEpoch [7/10], Step [400/938], Loss: 0.0008\nEpoch [7/10], Step [500/938], Loss: 0.0001\nEpoch [7/10], Step [600/938], Loss: 0.0006\nEpoch [7/10], Step [700/938], Loss: 0.0005\nEpoch [7/10], Step [800/938], Loss: 0.0007\nEpoch [7/10], Step [900/938], Loss: 0.0432\nEpoch [8/10], Step [100/938], Loss: 0.0005\nEpoch [8/10], Step [200/938], Loss: 0.0005\nEpoch [8/10], Step [300/938], Loss: 0.0000\nEpoch [8/10], Step [400/938], Loss: 0.0013\nEpoch [8/10], Step [500/938], Loss: 0.0005\nEpoch [8/10], Step [600/938], Loss: 0.0002\nEpoch [8/10], Step [700/938], Loss: 0.0004\nEpoch [8/10], Step [800/938], Loss: 0.0111\nEpoch [8/10], Step [900/938], Loss: 0.0001\nEpoch [9/10], Step [100/938], Loss: 0.0004\nEpoch [9/10], Step [200/938], Loss: 0.0693\nEpoch [9/10], Step [300/938], Loss: 0.0071\nEpoch [9/10], Step [400/938], Loss: 0.0000\nEpoch [9/10], Step [500/938], Loss: 0.0003\nEpoch [9/10], Step [600/938], Loss: 0.0003\nEpoch [9/10], Step [700/938], Loss: 0.0001\nEpoch [9/10], Step [800/938], Loss: 0.0000\nEpoch [9/10], Step [900/938], Loss: 0.0029\nEpoch [10/10], Step [100/938], Loss: 0.0008\nEpoch [10/10], Step [200/938], Loss: 0.0001\nEpoch [10/10], Step [300/938], Loss: 0.0000\nEpoch [10/10], Step [400/938], Loss: 0.0273\nEpoch [10/10], Step [500/938], Loss: 0.0001\nEpoch [10/10], Step [600/938], Loss: 0.0002\nEpoch [10/10], Step [700/938], Loss: 0.0010\nEpoch [10/10], Step [800/938], Loss: 0.0019\nEpoch [10/10], Step [900/938], Loss: 0.0000\n</code></pre>\n<h1 id=\"评估模型\"><a href=\"#评估模型\" class=\"headerlink\" title=\"评估模型\"></a>评估模型</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 实例化一个与原模型结构相同的模型</span></span><br><span class=\"line\">model = Net().to(device)  <span class=\"comment\"># 确保模型被放置在正确的设备上</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 加载模型参数</span></span><br><span class=\"line\">model.load_state_dict(torch.load(<span class=\"string\">&#x27;model.pth&#x27;</span>, map_location=device, weights_only=<span class=\"literal\">True</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 将模型设置为评估模式</span></span><br><span class=\"line\">model.<span class=\"built_in\">eval</span>()</span><br><span class=\"line\"><span class=\"comment\"># 禁用梯度计算以减少内存消耗</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    correct = <span class=\"number\">0</span></span><br><span class=\"line\">    total = <span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"comment\"># 在测试集上进行预测</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> images, labels <span class=\"keyword\">in</span> test_loader:</span><br><span class=\"line\">        images, labels = images.to(device), labels.to(device)</span><br><span class=\"line\">        outputs = model(images)</span><br><span class=\"line\">        _, predicted = torch.<span class=\"built_in\">max</span>(outputs.data, <span class=\"number\">1</span>)</span><br><span class=\"line\">        total += labels.size(<span class=\"number\">0</span>)</span><br><span class=\"line\">        correct += (predicted == labels).<span class=\"built_in\">sum</span>().item()</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\"># 打印测试集上的准确率</span></span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;Test Accuracy of the model on the <span class=\"subst\">&#123;total&#125;</span> test images: <span class=\"subst\">&#123;<span class=\"number\">100</span> * correct / total&#125;</span>%&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Test Accuracy of the model on the 10000 test images: 99.29%\n</code></pre>\n<h1 id=\"代码获取\"><a href=\"#代码获取\" class=\"headerlink\" title=\"代码获取\"></a>代码获取</h1><p>关注公众号“生信之巅”，聊天窗口回复“a7fe”获取完整版代码下载链接。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"深度学习","path":"api/tags/深度学习.json"},{"name":"PyTorch","path":"api/tags/PyTorch.json"},{"name":"卷积神经网络","path":"api/tags/卷积神经网络.json"}]},{"title":"Scikit-learn机器学习实战-HumanResourcesAnalytics","slug":"Scikit-learn机器学习实战-HumanResourcesAnalytics","date":"2024-08-30T14:24:05.000Z","updated":"2024-08-30T14:59:29.868Z","comments":true,"path":"api/articles/Scikit-learn机器学习实战-HumanResourcesAnalytics.json","excerpt":null,"keywords":null,"cover":"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>本文构建预测员工是否会离职的模型，并使用模型对员工进行预测。通过本文可以学习到：</p>\n<ul>\n<li>查看数据集的统计信息</li>\n<li>特征工程</li>\n<li>数据集的划分</li>\n<li>数据集的预处理</li>\n<li>数据集的可视化</li>\n<li>模型训练</li>\n<li>模型调参</li>\n<li>模型评估</li>\n<li>模型预测</li>\n</ul>\n<h1 id=\"查看数据集信息\"><a href=\"#查看数据集信息\" class=\"headerlink\" title=\"查看数据集信息\"></a>查看数据集信息</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 读入数据</span></span><br><span class=\"line\">url = <span class=\"string\">&#x27;https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/data/ML/HumanResourcesAnalytics/HR_comma_sep.csv&#x27;</span></span><br><span class=\"line\">df = pd.read_csv(url)</span><br><span class=\"line\"><span class=\"comment\">#df = pd.read_csv(&#x27;HR_comma_sep.csv&#x27;)</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(df.info()) <span class=\"comment\">#474241623</span></span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;\nRangeIndex: 14999 entries, 0 to 14998\nData columns (total 10 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   satisfaction_level     14999 non-null  float64\n 1   last_evaluation        14999 non-null  float64\n 2   number_project         14999 non-null  int64  \n 3   average_montly_hours   14999 non-null  int64  \n 4   time_spend_company     14999 non-null  int64  \n 5   Work_accident          14999 non-null  int64  \n 6   left                   14999 non-null  int64  \n 7   promotion_last_5years  14999 non-null  int64  \n 8   sales                  14999 non-null  object \n 9   salary                 14999 non-null  object \ndtypes: float64(2), int64(6), object(2)\nmemory usage: 1.1+ MB\nNone\n</code></pre>\n<div style=\"overflow-x: auto; width: 100%;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n</head>\n<body>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_montly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n      <th>sales</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<p><strong>header 信息</strong></p>\n<ul>\n<li>satisfaction_level\t员工满意度</li>\n<li>last_evaluation\t员工考核评分</li>\n<li>number_project\t员工参与的项目数</li>\n<li>average_montly_hours\t每个月均工作时长</li>\n<li>time_spend_company\t员工工作年限</li>\n<li>Work_accident\t是否发生过事故</li>\n<li>left\t员工是否离职</li>\n<li>promotion_last_5years\t过去5年中是否有升职</li>\n<li>sales\t员工岗位</li>\n<li>salary 员工薪资水平</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 更正列名</span></span><br><span class=\"line\">df.rename(columns=&#123;<span class=\"string\">&#x27;average_montly_hours&#x27;</span>:<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>, <span class=\"string\">&#x27;sales&#x27;</span>:<span class=\"string\">&#x27;department&#x27;</span>&#125;, </span><br><span class=\"line\">          inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n      <th>department</th>\n      <th>salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>sales</td>\n      <td>low</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 展示数据集的统计信息，仅展示数值列</span></span><br><span class=\"line\">df.describe()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n      <td>14999.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.612834</td>\n      <td>0.716102</td>\n      <td>3.803054</td>\n      <td>201.050337</td>\n      <td>3.498233</td>\n      <td>0.144610</td>\n      <td>0.238083</td>\n      <td>0.021268</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.248631</td>\n      <td>0.171169</td>\n      <td>1.232592</td>\n      <td>49.943099</td>\n      <td>1.460136</td>\n      <td>0.351719</td>\n      <td>0.425924</td>\n      <td>0.144281</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.090000</td>\n      <td>0.360000</td>\n      <td>2.000000</td>\n      <td>96.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.440000</td>\n      <td>0.560000</td>\n      <td>3.000000</td>\n      <td>156.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.640000</td>\n      <td>0.720000</td>\n      <td>4.000000</td>\n      <td>200.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.820000</td>\n      <td>0.870000</td>\n      <td>5.000000</td>\n      <td>245.000000</td>\n      <td>4.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>7.000000</td>\n      <td>310.000000</td>\n      <td>10.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看各元素的出现次数</span></span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;Departments:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (df[<span class=\"string\">&#x27;department&#x27;</span>].value_counts())</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;\\nSalary:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (df[<span class=\"string\">&#x27;salary&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure>\n\n<pre><code>Departments:\ndepartment\nsales          4140\ntechnical      2720\nsupport        2229\nIT             1227\nproduct_mng     902\nmarketing       858\nRandD           787\naccounting      767\nhr              739\nmanagement      630\nName: count, dtype: int64\n\nSalary:\nsalary\nlow       7316\nmedium    6446\nhigh      1237\nName: count, dtype: int64\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 记录各特征的类型和取值范围</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">satisfaction_level | Satisfaction level of employee based on survey | Continuous | [0.09, 1]</span></span><br><span class=\"line\"><span class=\"string\">last_evaluation | Score based on employee&#x27;s last evaluation | Continuous | [0.36, 1]</span></span><br><span class=\"line\"><span class=\"string\">number_project | Number of projects | Continuous | [2, 7]</span></span><br><span class=\"line\"><span class=\"string\">average_monthly_hours | Average monthly hours | Continuous | [96, 310]</span></span><br><span class=\"line\"><span class=\"string\">time_spend_company | Years at company | Continuous | [2, 10]</span></span><br><span class=\"line\"><span class=\"string\">Work_accident | Whether employee had a work accident | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">left | Whether employee had left (Outcome Variable) | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">promotion_last_5years | Whether employee had a promotion in the last 5 years | Categorical | &#123;0, 1&#125;</span></span><br><span class=\"line\"><span class=\"string\">department | Department employee worked in | Categorical | 10 departments</span></span><br><span class=\"line\"><span class=\"string\">salary | Level of employee&#x27;s salary | Categorical | &#123;low, medium, high&#125;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&quot;\\nsatisfaction_level | Satisfaction level of employee based on survey | Continuous | [0.09, 1]\\nlast_evaluation | Score based on employee&#x27;s last evaluation | Continuous | [0.36, 1]\\nnumber_project | Number of projects | Continuous | [2, 7]\\naverage_monthly_hours | Average monthly hours | Continuous | [96, 310]\\ntime_spend_company | Years at company | Continuous | [2, 10]\\nWork_accident | Whether employee had a work accident | Categorical | &#123;0, 1&#125;\\nleft | Whether employee had left (Outcome Variable) | Categorical | &#123;0, 1&#125;\\npromotion_last_5years | Whether employee had a promotion in the last 5 years | Categorical | &#123;0, 1&#125;\\ndepartment | Department employee worked in | Categorical | 10 departments\\nsalary | Level of employee&#x27;s salary | Categorical | &#123;low, medium, high&#125;\\n&quot;</span><br></pre></td></tr></table></figure>\n\n\n<h1 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h1><ul>\n<li>查找相关性大的特征，只保留其中的一个。</li>\n<li>也可查看与标签（left）相关性较大的特征，如此数据集中的<code>satisfaction_level</code>。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 筛选 DataFrame 中的所有数值列</span></span><br><span class=\"line\">numeric_df = df.select_dtypes(include=[np.number])</span><br><span class=\"line\"><span class=\"comment\"># 计算数值列之间的相关系数</span></span><br><span class=\"line\">numeric_df.corr()</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>left</th>\n      <th>promotion_last_5years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>satisfaction_level</th>\n      <td>1.000000</td>\n      <td>0.105021</td>\n      <td>-0.142970</td>\n      <td>-0.020048</td>\n      <td>-0.100866</td>\n      <td>0.058697</td>\n      <td>-0.388375</td>\n      <td>0.025605</td>\n    </tr>\n    <tr>\n      <th>last_evaluation</th>\n      <td>0.105021</td>\n      <td>1.000000</td>\n      <td>0.349333</td>\n      <td>0.339742</td>\n      <td>0.131591</td>\n      <td>-0.007104</td>\n      <td>0.006567</td>\n      <td>-0.008684</td>\n    </tr>\n    <tr>\n      <th>number_project</th>\n      <td>-0.142970</td>\n      <td>0.349333</td>\n      <td>1.000000</td>\n      <td>0.417211</td>\n      <td>0.196786</td>\n      <td>-0.004741</td>\n      <td>0.023787</td>\n      <td>-0.006064</td>\n    </tr>\n    <tr>\n      <th>average_monthly_hours</th>\n      <td>-0.020048</td>\n      <td>0.339742</td>\n      <td>0.417211</td>\n      <td>1.000000</td>\n      <td>0.127755</td>\n      <td>-0.010143</td>\n      <td>0.071287</td>\n      <td>-0.003544</td>\n    </tr>\n    <tr>\n      <th>time_spend_company</th>\n      <td>-0.100866</td>\n      <td>0.131591</td>\n      <td>0.196786</td>\n      <td>0.127755</td>\n      <td>1.000000</td>\n      <td>0.002120</td>\n      <td>0.144822</td>\n      <td>0.067433</td>\n    </tr>\n    <tr>\n      <th>Work_accident</th>\n      <td>0.058697</td>\n      <td>-0.007104</td>\n      <td>-0.004741</td>\n      <td>-0.010143</td>\n      <td>0.002120</td>\n      <td>1.000000</td>\n      <td>-0.154622</td>\n      <td>0.039245</td>\n    </tr>\n    <tr>\n      <th>left</th>\n      <td>-0.388375</td>\n      <td>0.006567</td>\n      <td>0.023787</td>\n      <td>0.071287</td>\n      <td>0.144822</td>\n      <td>-0.154622</td>\n      <td>1.000000</td>\n      <td>-0.061788</td>\n    </tr>\n    <tr>\n      <th>promotion_last_5years</th>\n      <td>0.025605</td>\n      <td>-0.008684</td>\n      <td>-0.006064</td>\n      <td>-0.003544</td>\n      <td>0.067433</td>\n      <td>0.039245</td>\n      <td>-0.061788</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 查看离职员工部门分布，发现HR离职员工最多</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;department&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df)</span><br><span class=\"line\">plot.set_xticklabels(rotation=<span class=\"number\">45</span>, horizontalalignment=<span class=\"string\">&#x27;right&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_10_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看工资水平和离职率的关系</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;salary&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_11_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_11_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看经理工资水平分布</span></span><br><span class=\"line\">df[df[<span class=\"string\">&#x27;department&#x27;</span>]==<span class=\"string\">&#x27;management&#x27;</span>][<span class=\"string\">&#x27;salary&#x27;</span>].value_counts().plot(kind=<span class=\"string\">&#x27;pie&#x27;</span>, title=<span class=\"string\">&#x27;Management salary level distribution&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_12_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_12_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 查看研发工资水平分布</span></span><br><span class=\"line\">df[df[<span class=\"string\">&#x27;department&#x27;</span>]==<span class=\"string\">&#x27;RandD&#x27;</span>][<span class=\"string\">&#x27;salary&#x27;</span>].value_counts().plot(kind=<span class=\"string\">&#x27;pie&#x27;</span>, title=<span class=\"string\">&#x27;R&amp;D dept salary level distribution&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_13_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_13_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制员工满意度分布的直方图，并分为两类员工：已离职和未离职</span></span><br><span class=\"line\"><span class=\"comment\"># 生成21个等间距的数值作为直方图的区间，范围从0.0001到1.0001</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">0.0001</span>, <span class=\"number\">1.0001</span>, <span class=\"number\">21</span>)</span><br><span class=\"line\"><span class=\"comment\"># 绘制直方图。首先筛选出已离职员工（df[&#x27;left&#x27;]==1）和未离职员工（df[&#x27;left&#x27;]==0）的满意度数据，使用指定的区间（bins）、透明度（alpha）和标签（label）进行绘制。</span></span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;satisfaction_level&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.7</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;satisfaction_level&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.5</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;satisfaction_level&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># 设置x轴的显示范围从0到1.05</span></span><br><span class=\"line\">plt.xlim((<span class=\"number\">0</span>,<span class=\"number\">1.05</span>))</span><br><span class=\"line\"><span class=\"comment\"># 在最合适的位置添加图例</span></span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_14_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_14_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>发现已离职员工对公司的满意度比较低（0~0.5），当然也存在满意度较高（0.8附近）的员工离职的情况。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Last evaluation</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">0.3501</span>, <span class=\"number\">1.0001</span>, <span class=\"number\">14</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;last_evaluation&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;last_evaluation&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;last_evaluation&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_16_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_16_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>公司评分高（0.8~1.0）的员工离职了很多，原因可能是这部分员工能力强，跳槽寻求更好的工作机会。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Number of projects </span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">1.5</span>, <span class=\"number\">7.5</span>, <span class=\"number\">7</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;number_project&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;number_project&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;number_project&#x27;</span>)</span><br><span class=\"line\">plt.grid(axis=<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_18_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_18_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>项目少时离职了，可能因为员工锻炼机会少。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Average monthly hours</span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">75</span>, <span class=\"number\">325</span>, <span class=\"number\">11</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;average_monthly_hours&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_20_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_20_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>工作时长少和多都容易离职。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Years at company </span></span><br><span class=\"line\">bins = np.linspace(<span class=\"number\">1.5</span>, <span class=\"number\">10.5</span>, <span class=\"number\">10</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">1</span>][<span class=\"string\">&#x27;time_spend_company&#x27;</span>], bins=bins, alpha=<span class=\"number\">1</span>, label=<span class=\"string\">&#x27;Employees Left&#x27;</span>)</span><br><span class=\"line\">plt.hist(df[df[<span class=\"string\">&#x27;left&#x27;</span>]==<span class=\"number\">0</span>][<span class=\"string\">&#x27;time_spend_company&#x27;</span>], bins=bins, alpha=<span class=\"number\">0.4</span>, label=<span class=\"string\">&#x27;Employees Stayed&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;time_spend_company&#x27;</span>)</span><br><span class=\"line\">plt.xlim((<span class=\"number\">1</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\">plt.grid(axis=<span class=\"string\">&#x27;x&#x27;</span>)</span><br><span class=\"line\">plt.xticks(np.arange(<span class=\"number\">2</span>,<span class=\"number\">11</span>))</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_22_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_22_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>工作年限3年，离职率最高。年限越长，离职率越低。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># whether employee had work accident</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;Work_accident&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_24_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_24_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>未发生工作事故的离职率较高，难以解释。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#whether employee had promotion in last 5 years</span></span><br><span class=\"line\">plot = sns.catplot(x=<span class=\"string\">&#x27;promotion_last_5years&#x27;</span>, y=<span class=\"string\">&#x27;left&#x27;</span>, kind=<span class=\"string\">&#x27;bar&#x27;</span>, data=df);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_26_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_26_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>不升职的离职率较高。</p>\n<h1 id=\"数据预处理\"><a href=\"#数据预处理\" class=\"headerlink\" title=\"数据预处理\"></a>数据预处理</h1><h2 id=\"独热编码替换分类数据\"><a href=\"#独热编码替换分类数据\" class=\"headerlink\" title=\"独热编码替换分类数据\"></a>独热编码替换分类数据</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 丢弃标签（left）列</span></span><br><span class=\"line\">X = df.drop(<span class=\"string\">&#x27;left&#x27;</span>, axis=<span class=\"number\">1</span>)</span><br><span class=\"line\"><span class=\"comment\"># 提取标签列</span></span><br><span class=\"line\">y = df[<span class=\"string\">&#x27;left&#x27;</span>]</span><br><span class=\"line\"><span class=\"comment\"># 删除部门与工资列，后面会通过独热编码将信息添加回来</span></span><br><span class=\"line\">X.drop([<span class=\"string\">&#x27;department&#x27;</span>,<span class=\"string\">&#x27;salary&#x27;</span>], axis=<span class=\"number\">1</span>, inplace=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># One-hot encoding</span></span><br><span class=\"line\"><span class=\"comment\"># 对工资进行独热编码</span></span><br><span class=\"line\">salary_dummy = pd.get_dummies(df[<span class=\"string\">&#x27;salary&#x27;</span>])</span><br><span class=\"line\"><span class=\"comment\"># 对部门进行独热编码</span></span><br><span class=\"line\">department_dummy = pd.get_dummies(df[<span class=\"string\">&#x27;department&#x27;</span>])</span><br><span class=\"line\">X = pd.concat([X, salary_dummy], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">X = pd.concat([X, department_dummy], axis=<span class=\"number\">1</span>)</span><br><span class=\"line\">X.head()</span><br></pre></td></tr></table></figure>\n\n<div style=\"overflow-x: auto; width: 100%;\">\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>satisfaction_level</th>\n      <th>last_evaluation</th>\n      <th>number_project</th>\n      <th>average_monthly_hours</th>\n      <th>time_spend_company</th>\n      <th>Work_accident</th>\n      <th>promotion_last_5years</th>\n      <th>high</th>\n      <th>low</th>\n      <th>medium</th>\n      <th>IT</th>\n      <th>RandD</th>\n      <th>accounting</th>\n      <th>hr</th>\n      <th>management</th>\n      <th>marketing</th>\n      <th>product_mng</th>\n      <th>sales</th>\n      <th>support</th>\n      <th>technical</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.38</td>\n      <td>0.53</td>\n      <td>2</td>\n      <td>157</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.80</td>\n      <td>0.86</td>\n      <td>5</td>\n      <td>262</td>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.11</td>\n      <td>0.88</td>\n      <td>7</td>\n      <td>272</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.72</td>\n      <td>0.87</td>\n      <td>5</td>\n      <td>223</td>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.37</td>\n      <td>0.52</td>\n      <td>2</td>\n      <td>159</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<h2 id=\"拆分训练集和测试集\"><a href=\"#拆分训练集和测试集\" class=\"headerlink\" title=\"拆分训练集和测试集\"></a>拆分训练集和测试集</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 划分训练集和测试集 (70%/30%)</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> train_test_split</span><br><span class=\"line\"></span><br><span class=\"line\">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class=\"number\">0.3</span>)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"数据标准化\"><a href=\"#数据标准化\" class=\"headerlink\" title=\"数据标准化\"></a>数据标准化</h2><ul>\n<li>比较大的数值，算法会认为其比较重要，导致结果不准确。</li>\n<li>数值差异比较大的话，模型收敛较慢。</li>\n<li>因此，需要将数据标准化。</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 数据标准化，这里是一个例子</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">stdsc = StandardScaler()</span><br><span class=\"line\">X_example = np.array([[ <span class=\"number\">10.</span>, -<span class=\"number\">2.</span>,  <span class=\"number\">23.</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">5.</span>,  <span class=\"number\">32.</span>,  <span class=\"number\">211.</span>],</span><br><span class=\"line\">                      [ <span class=\"number\">10.</span>,  <span class=\"number\">1.</span>, -<span class=\"number\">130.</span>]])</span><br><span class=\"line\">X_example = stdsc.fit_transform(X_example)</span><br><span class=\"line\">X_example = pd.DataFrame(X_example)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_example)</span><br><span class=\"line\">X_example.describe()</span><br></pre></td></tr></table></figure>\n\n<pre><code>          0         1         2\n0  0.707107 -0.802454 -0.083658\n1 -1.414214  1.409716  1.264429\n2  0.707107 -0.607262 -1.180771\n</code></pre>\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.000000e+00</td>\n      <td>3.000000e+00</td>\n      <td>3.000000e+00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-2.960595e-16</td>\n      <td>-1.110223e-16</td>\n      <td>7.401487e-17</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.224745e+00</td>\n      <td>1.224745e+00</td>\n      <td>1.224745e+00</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.414214e+00</td>\n      <td>-8.024539e-01</td>\n      <td>-1.180771e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-3.535534e-01</td>\n      <td>-7.048582e-01</td>\n      <td>-6.322145e-01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>7.071068e-01</td>\n      <td>-6.072624e-01</td>\n      <td>-8.365788e-02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.071068e-01</td>\n      <td>4.012270e-01</td>\n      <td>5.903856e-01</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.071068e-01</td>\n      <td>1.409716e+00</td>\n      <td>1.264429e+00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 分别对训练集和测试集进行标准化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"></span><br><span class=\"line\">stdsc = StandardScaler()</span><br><span class=\"line\"><span class=\"comment\"># transform our training features</span></span><br><span class=\"line\">X_train_std = stdsc.fit_transform(X_train)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_train_std[<span class=\"number\">0</span>])</span><br><span class=\"line\"><span class=\"comment\"># transform the testing features in the same way</span></span><br><span class=\"line\">X_test_std = stdsc.transform(X_test)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[ 1.40697692 -0.21068428 -0.65422416 -1.37529896 -1.02172591 -0.41080801\n -0.14595719 -0.30564365 -0.98084819  1.16499228 -0.2981308  -0.23781569\n -0.22665375 -0.23057496 -0.21332806 -0.24641294 -0.25073288  1.62416352\n -0.41712208 -0.47247431]\n</code></pre>\n<h1 id=\"构建模型\"><a href=\"#构建模型\" class=\"headerlink\" title=\"构建模型\"></a>构建模型</h1><h2 id=\"随机森林法\"><a href=\"#随机森林法\" class=\"headerlink\" title=\"随机森林法\"></a>随机森林法</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 交叉验证（Cross validation）</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> ShuffleSplit</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 进行20折交叉验证</span></span><br><span class=\"line\">cv = ShuffleSplit(n_splits=<span class=\"number\">20</span>, test_size=<span class=\"number\">0.3</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 构建随机森林模型</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.ensemble <span class=\"keyword\">import</span> RandomForestClassifier</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.model_selection <span class=\"keyword\">import</span> GridSearchCV</span><br><span class=\"line\">rf_model = RandomForestClassifier()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置RF模型，建立树的数量</span></span><br><span class=\"line\">rf_param = &#123;<span class=\"string\">&#x27;n_estimators&#x27;</span>: <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">11</span>)&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 探索模型参数（最佳树的个数）</span></span><br><span class=\"line\">rf_grid = GridSearchCV(rf_model, rf_param, cv=cv)</span><br><span class=\"line\">rf_grid.fit(X_train, y_train)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出最佳参数和最佳得分</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Parameter with best score:&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(rf_grid.best_params_)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Cross validation score:&#x27;</span>, rf_grid.best_score_)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Parameter with best score:\n&#123;&#39;n_estimators&#39;: 9&#125;\nCross validation score: 0.9835079365079364\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在测试集上评估模型</span></span><br><span class=\"line\">best_rf = rf_grid.best_estimator_</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Test score:&#x27;</span>, best_rf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>\n\n<pre><code>Test score: 0.9884444444444445\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 通过随机森林查看特征的重要性，原理是每次打乱一个特征（或添加噪音），然后看预测结果（错误率）是否发生变化，如果变化大，则该特征对预测结果有影响，否则没有影响</span></span><br><span class=\"line\">features = X.columns</span><br><span class=\"line\">feature_importances = best_rf.feature_importances_</span><br><span class=\"line\"></span><br><span class=\"line\">features_df = pd.DataFrame(&#123;<span class=\"string\">&#x27;Features&#x27;</span>: features, <span class=\"string\">&#x27;Importance Score&#x27;</span>: feature_importances&#125;)</span><br><span class=\"line\">features_df.sort_values(<span class=\"string\">&#x27;Importance Score&#x27;</span>, inplace=<span class=\"literal\">True</span>, ascending=<span class=\"literal\">False</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">features_df</span><br></pre></td></tr></table></figure>\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Features</th>\n      <th>Importance Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>satisfaction_level</td>\n      <td>0.260366</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>average_monthly_hours</td>\n      <td>0.186585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>number_project</td>\n      <td>0.179788</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>time_spend_company</td>\n      <td>0.179571</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>last_evaluation</td>\n      <td>0.144083</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Work_accident</td>\n      <td>0.011949</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>low</td>\n      <td>0.006395</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>high</td>\n      <td>0.005206</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>medium</td>\n      <td>0.003336</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>sales</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>support</td>\n      <td>0.003070</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>technical</td>\n      <td>0.003039</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>RandD</td>\n      <td>0.002143</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>IT</td>\n      <td>0.002048</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>accounting</td>\n      <td>0.001887</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>promotion_last_5years</td>\n      <td>0.001799</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>management</td>\n      <td>0.001755</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>hr</td>\n      <td>0.001425</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>product_mng</td>\n      <td>0.001182</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>marketing</td>\n      <td>0.001173</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算前五项特征的重要性之和</span></span><br><span class=\"line\">features_df[<span class=\"string\">&#x27;Importance Score&#x27;</span>][:<span class=\"number\">5</span>].<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n<pre><code>np.float64(0.9503925098929926)\n</code></pre>\n<h2 id=\"基于聚类模型的分析\"><a href=\"#基于聚类模型的分析\" class=\"headerlink\" title=\"基于聚类模型的分析\"></a>基于聚类模型的分析</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">import</span> seaborn <span class=\"keyword\">as</span> sns</span><br><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"></span><br><span class=\"line\">data = pd.read_csv(url)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize = (<span class=\"number\">8</span>,<span class=\"number\">8</span>))</span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">1</span>)</span><br><span class=\"line\">plt.plot(data.satisfaction_level[data.left == <span class=\"number\">1</span>],data.last_evaluation[data.left == <span class=\"number\">1</span>],<span class=\"string\">&#x27;o&#x27;</span>, alpha = <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Employees who left&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction level&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.subplot(<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">2</span>)</span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;Employees who stayed&#x27;</span>)</span><br><span class=\"line\">plt.plot(data.satisfaction_level[data.left == <span class=\"number\">0</span>],data.last_evaluation[data.left == <span class=\"number\">0</span>],<span class=\"string\">&#x27;o&#x27;</span>, alpha = <span class=\"number\">0.1</span>)</span><br><span class=\"line\">plt.xlim([<span class=\"number\">0.4</span>,<span class=\"number\">1</span>])</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction level&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Text(0.5, 0, &#39;Satisfaction level&#39;)\n</code></pre>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_43_1.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_43_1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 用KMeans聚类分析</span></span><br><span class=\"line\"><span class=\"comment\"># 导入KMeans聚类算法模块</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.cluster <span class=\"keyword\">import</span> KMeans</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 选取数据中已经离职的员工（left列为1），并从这些数据中删除特定的列</span></span><br><span class=\"line\"><span class=\"comment\"># 这里axis=1表示按列删除，这些列包括：项目数量、月平均工作小时、公司服务时间、工作事故、是否离职、过去5年是否晋升、销售部门和薪水等</span></span><br><span class=\"line\">kmeans_df =  data[data.left == <span class=\"number\">1</span>].drop([ <span class=\"string\">u&#x27;number_project&#x27;</span>,</span><br><span class=\"line\">       <span class=\"string\">u&#x27;average_montly_hours&#x27;</span>, <span class=\"string\">u&#x27;time_spend_company&#x27;</span>, <span class=\"string\">u&#x27;Work_accident&#x27;</span>,</span><br><span class=\"line\">       <span class=\"string\">u&#x27;left&#x27;</span>, <span class=\"string\">u&#x27;promotion_last_5years&#x27;</span>, <span class=\"string\">u&#x27;sales&#x27;</span>, <span class=\"string\">u&#x27;salary&#x27;</span>],axis = <span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用KMeans算法对处理后的数据进行聚类，设定聚类数为3，并设置随机种子为0以确保结果的可复现性</span></span><br><span class=\"line\"><span class=\"comment\"># 这里fit方法用于训练模型，使其学习数据的聚类结构</span></span><br><span class=\"line\">kmeans = KMeans(n_clusters = <span class=\"number\">3</span>, random_state = <span class=\"number\">0</span>).fit(kmeans_df)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 访问并输出每个聚类中心点的坐标，这些坐标表示了每个聚类的中心位置</span></span><br><span class=\"line\">kmeans.cluster_centers_</span><br></pre></td></tr></table></figure>\n\n<pre><code>array([[0.41014545, 0.51698182],\n       [0.80851586, 0.91170931],\n       [0.11115466, 0.86930085]])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 筛选出离职员工的数据</span></span><br><span class=\"line\">left = data[data.left == <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 使用布尔索引和 .loc 方法将 KMeans 聚类的标签分配给离职员工数据</span></span><br><span class=\"line\">left_labels = (data.left == <span class=\"number\">1</span>)</span><br><span class=\"line\">data.loc[left_labels, <span class=\"string\">&#x27;label&#x27;</span>] = kmeans.labels_</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 重新获取带有标签的离职员工数据</span></span><br><span class=\"line\">left = data[data.left == <span class=\"number\">1</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建一个新的图形窗口</span></span><br><span class=\"line\">plt.figure()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置 x 轴标签为满意度水平</span></span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Satisfaction Level&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置 y 轴标签为最后一次评估结果</span></span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Last Evaluation&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 设置图形标题为“离职员工的3个聚类”</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;3 Clusters of employees who left&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制不同聚类的离职员工的满意度水平和最后一次评估结果</span></span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">0</span>], left.last_evaluation[left.label==<span class=\"number\">0</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;r&#x27;</span>)</span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">1</span>], left.last_evaluation[left.label==<span class=\"number\">1</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;g&#x27;</span>)</span><br><span class=\"line\">plt.plot(left.satisfaction_level[left.label==<span class=\"number\">2</span>], left.last_evaluation[left.label==<span class=\"number\">2</span>], <span class=\"string\">&#x27;o&#x27;</span>, alpha=<span class=\"number\">0.2</span>, color=<span class=\"string\">&#x27;b&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加图例，解释不同聚类的含义，并设置图例的位置和字体大小</span></span><br><span class=\"line\">plt.legend([<span class=\"string\">&#x27;Winners&#x27;</span>, <span class=\"string\">&#x27;Frustrated&#x27;</span>, <span class=\"string\">&#x27;Bad Match&#x27;</span>], loc=<span class=\"number\">3</span>, fontsize=<span class=\"number\">15</span>, frameon=<span class=\"literal\">True</span>);</span><br></pre></td></tr></table></figure>\n<p><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_45_0.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/images/post/WhoWillLeave_files/WhoWillLeave_45_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h1><p>关注公众号“生信之巅”</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"Scikit-learn","path":"api/tags/Scikit-learn.json"}]},{"title":"Scikit-learn机器学习实战-PCA","slug":"Scikit-learn机器学习实战-PCA","date":"2024-09-01T08:38:58.000Z","updated":"2024-09-01T08:47:58.888Z","comments":true,"path":"api/articles/Scikit-learn机器学习实战-PCA.json","excerpt":null,"keywords":null,"cover":"/PCA_files/PCA_5_0.png","content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>PCA的全称是<strong>Principal Component Analysis</strong>，即主成分分析。这是一种常用的数据分析方法，主要用于数据降维。PCA的主要思想是将原始的高维特征空间通过线性变换投射到一个新的低维特征空间上，同时尽量保持原始数据的方差，使得在新的低维空间中数据的差异性得以保留。这一过程中，通过计算数据集的协方差矩阵，找到其特征值和特征向量，进而确定主成分的方向和贡献率，实现数据的有效降维。</p>\n<p>具体来说，PCA的计算过程包括以下几个步骤：</p>\n<ul>\n<li>数据标准化：将原始数据转换为均值为0，标准差为1的标准化数据，以消除不同量纲对分析结果的影响。</li>\n<li>计算协方差矩阵：标准化后的数据矩阵的协方差矩阵反映了各变量之间的相关性。</li>\n<li>计算特征值和特征向量：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征值的大小反映了对应主成分的重要性，而特征向量则指示了主成分的方向。</li>\n<li>选取主成分：根据特征值的大小，选取前k个主成分，使得这k个主成分的累计贡献率达到一定的阈值（如80%或90%）。</li>\n<li>转换数据到新的主成分空间：将原始数据转换到由选定的主成分构成的新空间中，得到降维后的数据。</li>\n</ul>\n<p>PCA在数据分析和机器学习领域有着广泛的应用，如特征提取、数据压缩、噪声消除、图像识别等。同时，PCA也是一种无监督学习的方法，它不需要数据的标签信息，就可以从数据中提取出有用的特征信息。</p>\n<p>本文通过鸢尾花数据集演示PCA方法，讲解如何确定降维采用的特征向量的数量，并完成降维。包括手写函数实现和通过SK-learn实现代码。</p>\n<h1 id=\"导入数据集\"><a href=\"#导入数据集\" class=\"headerlink\" title=\"导入数据集\"></a>导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;iris.data&#x27;</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>5.1</th>\n      <th>3.5</th>\n      <th>1.4</th>\n      <th>0.2</th>\n      <th>Iris-setosa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加列名，分别为 sepal_len, sepal_wid, petal_len, petal_wid, class</span></span><br><span class=\"line\"><span class=\"comment\"># 前四列为特征，最后一列为类别，共有3种类别</span></span><br><span class=\"line\">df.columns=[<span class=\"string\">&#x27;sepal_len&#x27;</span>, <span class=\"string\">&#x27;sepal_wid&#x27;</span>, <span class=\"string\">&#x27;petal_len&#x27;</span>, <span class=\"string\">&#x27;petal_wid&#x27;</span>, <span class=\"string\">&#x27;class&#x27;</span>]</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_len</th>\n      <th>sepal_wid</th>\n      <th>petal_len</th>\n      <th>petal_wid</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 拆分特征和标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = df.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">4</span>].values</span><br><span class=\"line\">y = df.iloc[:,<span class=\"number\">4</span>].values</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h1><h2 id=\"特征展示\"><a href=\"#特征展示\" class=\"headerlink\" title=\"特征展示\"></a>特征展示</h2><p>查看特征的取值范围以及其和类别的关系。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入matplotlib库中的pyplot模块，用于绘图</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 导入math库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的种类名称</span></span><br><span class=\"line\">label_dict = &#123;<span class=\"number\">1</span>: <span class=\"string\">&#x27;Iris-Setosa&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">2</span>: <span class=\"string\">&#x27;Iris-Versicolor&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">3</span>: <span class=\"string\">&#x27;Iris-Virgnica&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的特征名称</span></span><br><span class=\"line\">feature_dict = &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;sepal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">1</span>: <span class=\"string\">&#x27;sepal width [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">2</span>: <span class=\"string\">&#x27;petal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">3</span>: <span class=\"string\">&#x27;petal width [cm]&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建一个8x6英寸的图像</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"><span class=\"comment\"># 循环绘制4个子图，每个子图代表一个特征的直方图</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> cnt <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 在图像中创建2x2的子图，并指定当前子图的位置</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, cnt+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 遍历每种鸢尾花类别，绘制直方图</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> lab <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 绘制指定特征的直方图，使用不同颜色和透明度区分不同类别</span></span><br><span class=\"line\">        plt.hist(X[y==lab, cnt],</span><br><span class=\"line\">                 label=lab,</span><br><span class=\"line\">                 bins=<span class=\"number\">10</span>,</span><br><span class=\"line\">                 alpha=<span class=\"number\">0.3</span>,)</span><br><span class=\"line\">    <span class=\"comment\"># 设置子图的x轴标签</span></span><br><span class=\"line\">    plt.xlabel(feature_dict[cnt])</span><br><span class=\"line\">    <span class=\"comment\"># 添加图例，用于标识不同类别的鸢尾花</span></span><br><span class=\"line\">    plt.legend(loc=<span class=\"string\">&#x27;upper right&#x27;</span>, fancybox=<span class=\"literal\">True</span>, fontsize=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调整子图间的间距，使布局更加紧凑</span></span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\"><span class=\"comment\"># 显示绘制的图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_5_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_5_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<ul>\n<li>发现<code>花萼的长度</code>和<code>宽度</code>能够较好的区分3种类别。</li>\n<li>四个特征取值范围差异较大，需要进行标准化。</li>\n</ul>\n<h2 id=\"数据标准化\"><a href=\"#数据标准化\" class=\"headerlink\" title=\"数据标准化\"></a>数据标准化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 进行数据标准化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">X_std = StandardScaler().fit_transform(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_std)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[[-1.1483555  -0.11805969 -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.41098555 -1.32506301]\n [-1.51163569  0.11339944 -1.29694332 -1.32506301]\n [-1.02726211  1.27069504 -1.35396443 -1.32506301]\n [-0.54288852  1.9650724  -1.18290109 -1.0614657 ]\n [-1.51163569  0.8077768  -1.35396443 -1.19326436]\n [-1.02726211  0.8077768  -1.29694332 -1.32506301]\n [-1.75382249 -0.34951881 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-0.54288852  1.50215416 -1.29694332 -1.32506301]\n [-1.2694489   0.8077768  -1.23992221 -1.32506301]\n [-1.2694489  -0.11805969 -1.35396443 -1.45686167]\n [-1.87491588 -0.11805969 -1.52502777 -1.45686167]\n [-0.05851493  2.19653152 -1.46800666 -1.32506301]\n [-0.17960833  3.122368   -1.29694332 -1.0614657 ]\n [-0.54288852  1.9650724  -1.41098555 -1.0614657 ]\n [-0.90616871  1.03923592 -1.35396443 -1.19326436]\n [-0.17960833  1.73361328 -1.18290109 -1.19326436]\n [-0.90616871  1.73361328 -1.29694332 -1.19326436]\n [-0.54288852  0.8077768  -1.18290109 -1.32506301]\n [-0.90616871  1.50215416 -1.29694332 -1.0614657 ]\n [-1.51163569  1.27069504 -1.58204889 -1.32506301]\n [-0.90616871  0.57631768 -1.18290109 -0.92966704]\n [-1.2694489   0.8077768  -1.06885886 -1.32506301]\n [-1.02726211 -0.11805969 -1.23992221 -1.32506301]\n [-1.02726211  0.8077768  -1.23992221 -1.0614657 ]\n [-0.78507531  1.03923592 -1.29694332 -1.32506301]\n [-0.78507531  0.8077768  -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.23992221 -1.32506301]\n [-1.2694489   0.11339944 -1.23992221 -1.32506301]\n [-0.54288852  0.8077768  -1.29694332 -1.0614657 ]\n [-0.78507531  2.42799064 -1.29694332 -1.45686167]\n [-0.42179512  2.65944976 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.02726211  0.34485856 -1.46800666 -1.32506301]\n [-0.42179512  1.03923592 -1.41098555 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.75382249 -0.11805969 -1.41098555 -1.32506301]\n [-0.90616871  0.8077768  -1.29694332 -1.32506301]\n [-1.02726211  1.03923592 -1.41098555 -1.19326436]\n [-1.63272909 -1.73827353 -1.41098555 -1.19326436]\n [-1.75382249  0.34485856 -1.41098555 -1.32506301]\n [-1.02726211  1.03923592 -1.23992221 -0.79786838]\n [-0.90616871  1.73361328 -1.06885886 -1.0614657 ]\n [-1.2694489  -0.11805969 -1.35396443 -1.19326436]\n [-0.90616871  1.73361328 -1.23992221 -1.32506301]\n [-1.51163569  0.34485856 -1.35396443 -1.32506301]\n [-0.66398191  1.50215416 -1.29694332 -1.32506301]\n [-1.02726211  0.57631768 -1.35396443 -1.32506301]\n [ 1.39460583  0.34485856  0.52773232  0.25652088]\n [ 0.66804545  0.34485856  0.41369009  0.38831953]\n [ 1.27351244  0.11339944  0.64177455  0.38831953]\n [-0.42179512 -1.73827353  0.12858453  0.12472222]\n [ 0.78913885 -0.58097793  0.47071121  0.38831953]\n [-0.17960833 -0.58097793  0.41369009  0.12472222]\n [ 0.54695205  0.57631768  0.52773232  0.52011819]\n [-1.1483555  -1.50681441 -0.27056327 -0.27067375]\n [ 0.91023225 -0.34951881  0.47071121  0.12472222]\n [-0.78507531 -0.81243705  0.07156341  0.25652088]\n [-1.02726211 -2.43265089 -0.15652104 -0.27067375]\n [ 0.06257847 -0.11805969  0.24262675  0.38831953]\n [ 0.18367186 -1.96973265  0.12858453 -0.27067375]\n [ 0.30476526 -0.34951881  0.52773232  0.25652088]\n [-0.30070172 -0.34951881 -0.09949993  0.12472222]\n [ 1.03132564  0.11339944  0.35666898  0.25652088]\n [-0.30070172 -0.11805969  0.41369009  0.38831953]\n [-0.05851493 -0.81243705  0.18560564 -0.27067375]\n [ 0.42585866 -1.96973265  0.41369009  0.38831953]\n [-0.30070172 -1.27535529  0.07156341 -0.1388751 ]\n [ 0.06257847  0.34485856  0.58475344  0.78371551]\n [ 0.30476526 -0.58097793  0.12858453  0.12472222]\n [ 0.54695205 -1.27535529  0.64177455  0.38831953]\n [ 0.30476526 -0.58097793  0.52773232 -0.00707644]\n [ 0.66804545 -0.34951881  0.29964787  0.12472222]\n [ 0.91023225 -0.11805969  0.35666898  0.25652088]\n [ 1.15241904 -0.58097793  0.58475344  0.25652088]\n [ 1.03132564 -0.11805969  0.69879566  0.65191685]\n [ 0.18367186 -0.34951881  0.41369009  0.38831953]\n [-0.17960833 -1.04389617 -0.15652104 -0.27067375]\n [-0.42179512 -1.50681441  0.0145423  -0.1388751 ]\n [-0.42179512 -1.50681441 -0.04247882 -0.27067375]\n [-0.05851493 -0.81243705  0.07156341 -0.00707644]\n [ 0.18367186 -0.81243705  0.75581678  0.52011819]\n [-0.54288852 -0.11805969  0.41369009  0.38831953]\n [ 0.18367186  0.8077768   0.41369009  0.52011819]\n [ 1.03132564  0.11339944  0.52773232  0.38831953]\n [ 0.54695205 -1.73827353  0.35666898  0.12472222]\n [-0.30070172 -0.11805969  0.18560564  0.12472222]\n [-0.42179512 -1.27535529  0.12858453  0.12472222]\n [-0.42179512 -1.04389617  0.35666898 -0.00707644]\n [ 0.30476526 -0.11805969  0.47071121  0.25652088]\n [-0.05851493 -1.04389617  0.12858453 -0.00707644]\n [-1.02726211 -1.73827353 -0.27056327 -0.27067375]\n [-0.30070172 -0.81243705  0.24262675  0.12472222]\n [-0.17960833 -0.11805969  0.24262675 -0.00707644]\n [-0.17960833 -0.34951881  0.24262675  0.12472222]\n [ 0.42585866 -0.34951881  0.29964787  0.12472222]\n [-0.90616871 -1.27535529 -0.44162661 -0.1388751 ]\n [-0.17960833 -0.58097793  0.18560564  0.12472222]\n [ 0.54695205  0.57631768  1.2690068   1.70630611]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.51569923 -0.11805969  1.21198569  1.17911148]\n [ 0.54695205 -0.34951881  1.04092235  0.78371551]\n [ 0.78913885 -0.11805969  1.15496457  1.31091014]\n [ 2.12116622 -0.11805969  1.61113348  1.17911148]\n [-1.1483555  -1.27535529  0.41369009  0.65191685]\n [ 1.75788602 -0.34951881  1.44007014  0.78371551]\n [ 1.03132564 -1.27535529  1.15496457  0.78371551]\n [ 1.63679263  1.27069504  1.32602791  1.70630611]\n [ 0.78913885  0.34485856  0.75581678  1.04731282]\n [ 0.66804545 -0.81243705  0.869859    0.91551417]\n [ 1.15241904 -0.11805969  0.98390123  1.17911148]\n [-0.17960833 -1.27535529  0.69879566  1.04731282]\n [-0.05851493 -0.58097793  0.75581678  1.57450745]\n [ 0.66804545  0.34485856  0.869859    1.4427088 ]\n [ 0.78913885 -0.11805969  0.98390123  0.78371551]\n [ 2.24225961  1.73361328  1.6681546   1.31091014]\n [ 2.24225961 -1.04389617  1.78219682  1.4427088 ]\n [ 0.18367186 -1.96973265  0.69879566  0.38831953]\n [ 1.27351244  0.34485856  1.09794346  1.4427088 ]\n [-0.30070172 -0.58097793  0.64177455  1.04731282]\n [ 2.24225961 -0.58097793  1.6681546   1.04731282]\n [ 0.54695205 -0.81243705  0.64177455  0.78371551]\n [ 1.03132564  0.57631768  1.09794346  1.17911148]\n [ 1.63679263  0.34485856  1.2690068   0.78371551]\n [ 0.42585866 -0.58097793  0.58475344  0.78371551]\n [ 0.30476526 -0.11805969  0.64177455  0.78371551]\n [ 0.66804545 -0.58097793  1.04092235  1.17911148]\n [ 1.63679263 -0.11805969  1.15496457  0.52011819]\n [ 1.87897942 -0.58097793  1.32602791  0.91551417]\n [ 2.48444641  1.73361328  1.49709126  1.04731282]\n [ 0.66804545 -0.58097793  1.04092235  1.31091014]\n [ 0.54695205 -0.58097793  0.75581678  0.38831953]\n [ 0.30476526 -1.04389617  1.04092235  0.25652088]\n [ 2.24225961 -0.11805969  1.32602791  1.4427088 ]\n [ 0.54695205  0.8077768   1.04092235  1.57450745]\n [ 0.66804545  0.11339944  0.98390123  0.78371551]\n [ 0.18367186 -0.11805969  0.58475344  0.78371551]\n [ 1.27351244  0.11339944  0.92688012  1.17911148]\n [ 1.03132564  0.11339944  1.04092235  1.57450745]\n [ 1.27351244  0.11339944  0.75581678  1.4427088 ]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.15241904  0.34485856  1.21198569  1.4427088 ]\n [ 1.03132564  0.57631768  1.09794346  1.70630611]\n [ 1.03132564 -0.11805969  0.81283789  1.4427088 ]\n [ 0.54695205 -1.27535529  0.69879566  0.91551417]\n [ 0.78913885 -0.11805969  0.81283789  1.04731282]\n [ 0.42585866  0.8077768   0.92688012  1.4427088 ]\n [ 0.06257847 -0.11805969  0.75581678  0.78371551]]\n</code></pre>\n<h1 id=\"确定特征向量的数量\"><a href=\"#确定特征向量的数量\" class=\"headerlink\" title=\"确定特征向量的数量\"></a>确定特征向量的数量</h1><h2 id=\"协方差矩阵\"><a href=\"#协方差矩阵\" class=\"headerlink\" title=\"协方差矩阵\"></a>协方差矩阵</h2><p>协方差是衡量两个变量总体误差的期望，用于描述两个变量之间的线性关系程度和方向。两个特征间的协方差值越大，表明其相关性越强。</p>\n<p>对于两个随机变量$X$和$Y$，其协方差$Cov(X,Y)$的计算公式为：</p>\n<p>$$Cov(X,Y) &#x3D; E[(X - E[X])(Y - E[Y])]$$</p>\n<p>其中，$E[X]$和$E[Y]$分别是$X$和$Y$的期望值（即均值）。</p>\n<p>在实际应用中，当我们只有样本数据时，我们通常使用样本协方差来估计总体协方差。对于包含$n$个样本点的数据集，样本协方差$s_{xy}$的计算公式为：</p>\n<p>$$s_{xy} &#x3D; \\frac{1}{n-1} \\sum_{i&#x3D;1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$</p>\n<p>其中，$x_i$和$y_i$是样本点，$\\bar{x}$和$\\bar{y}$分别是$X$和$Y$的样本均值，计算公式为：</p>\n<p>$$\\bar{x} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} x_i$$</p>\n<p>$$\\bar{y} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} y_i$$</p>\n<p>注意，在计算样本协方差时，分母是$n-1$而不是$n$，这是为了得到总体协方差的无偏估计。</p>\n<p><strong>以下命令二选一</strong>：</p>\n<ul>\n<li>构造函数计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算特征向量的均值，用于中心化数据</span></span><br><span class=\"line\">mean_vec = np.mean(X_std, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算数据的协方差矩阵，用于理解特征之间的线性关系</span></span><br><span class=\"line\"><span class=\"comment\"># (X_std - mean_vec).T.dot((X_std - mean_vec) )计算的是(X_std - mean_vec)的矩阵乘法</span></span><br><span class=\"line\"><span class=\"comment\"># 除以(X_std.shape[0]-1)是为了获得一个无偏估计</span></span><br><span class=\"line\">cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出协方差矩阵，以便于后续分析和使用</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Covariance matrix \\n%s&#x27;</span> %cov_mat)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Covariance matrix \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<ul>\n<li>使用numpy计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;NumPy covariance matrix: \\n%s&#x27;</span> %np.cov(X_std.T))</span><br></pre></td></tr></table></figure>\n\n<pre><code>NumPy covariance matrix: \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<p>求协方差矩阵的特征向量和特征值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 求协方差矩阵</span></span><br><span class=\"line\">cov_mat = np.cov(X_std.T)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 求协方差矩阵的特征向量，返回特征向量（eig_vecs）和特征值（eig_vals）</span></span><br><span class=\"line\">eig_vals, eig_vecs = np.linalg.eig(cov_mat)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvectors \\n%s&#x27;</span> %eig_vecs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;\\nEigenvalues \\n%s&#x27;</span> %eig_vals)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Eigenvectors \n[[ 0.52308496 -0.36956962 -0.72154279  0.26301409]\n [-0.25956935 -0.92681168  0.2411952  -0.12437342]\n [ 0.58184289 -0.01912775  0.13962963 -0.80099722]\n [ 0.56609604 -0.06381646  0.63380158  0.52321917]]\n\nEigenvalues \n[2.92442837 0.93215233 0.14946373 0.02098259]\n</code></pre>\n<p>肉眼观察<code>Eigenvalues</code>，应当取前2个特征值对应的特征向量，因为其最重要。</p>\n<p>创建一个包含特征值和对应特征向量的元组列表。按特征值降序排序这些元组。打印排序后的特征值，确认排序正确。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Make a list of (eigenvalue, eigenvector) tuples</span></span><br><span class=\"line\">eig_pairs = [(np.<span class=\"built_in\">abs</span>(eig_vals[i]), eig_vecs[:,i]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(eig_vals))]</span><br><span class=\"line\"><span class=\"built_in\">print</span> (eig_pairs)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># Sort the (eigenvalue, eigenvector) tuples from high to low</span></span><br><span class=\"line\">eig_pairs.sort(key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">0</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Visually confirm that the list is correctly sorted by decreasing eigenvalues</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvalues in descending order:&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> eig_pairs:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[(np.float64(2.9244283691111117), array([ 0.52308496, -0.25956935,  0.58184289,  0.56609604])), (np.float64(0.9321523302535062), array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (np.float64(0.14946373489813364), array([-0.72154279,  0.2411952 ,  0.13962963,  0.63380158])), (np.float64(0.020982592764271016), array([ 0.26301409, -0.12437342, -0.80099722,  0.52321917]))]\n----------\nEigenvalues in descending order:\n2.9244283691111117\n0.9321523302535062\n0.14946373489813364\n0.020982592764271016\n</code></pre>\n<p>计算累计变异解释百分比，以评估前几个主成分能解释的总变异比例。最后输出累计变异解释百分比。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算所有特征值的总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(eig_vals)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步的目的是确定每个特征值在总变异中所占的百分比</span></span><br><span class=\"line\">var_exp = [(i / tot)*<span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(eig_vals, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步是为了了解前几个主成分可以解释多少总变异</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp</span><br></pre></td></tr></table></figure>\n\n<pre><code>[np.float64(72.62003332692032), np.float64(23.147406858644143), np.float64(3.711515564584531), np.float64(0.5210442498510259)]\n\n\n\n\n\narray([ 72.62003333,  95.76744019,  99.47895575, 100.        ])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 演示np.cumsum() 函数的用法</span></span><br><span class=\"line\">a = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span> (a)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;-----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (np.cumsum(a)) <span class=\"comment\"># 元素的累加和</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>[1 2 3 4]\n-----------\n[ 1  3  6 10]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制特征值的重要性</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.bar(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), var_exp, alpha=<span class=\"number\">0.5</span>, align=<span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            label=<span class=\"string\">&#x27;individual explained variance&#x27;</span>)</span><br><span class=\"line\">plt.step(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), cum_var_exp, where=<span class=\"string\">&#x27;mid&#x27;</span>,</span><br><span class=\"line\">             label=<span class=\"string\">&#x27;cumulative explained variance&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Explained variance ratio&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal components&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_20_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_20_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>根据上图可以决定取几个特征值进行降维，此例中前两个特征值已经足够（~95.77%）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 根据前两个主要成分构造转换矩阵W</span></span><br><span class=\"line\"><span class=\"comment\"># 选择主要成分分析（PCA）得到的前两个特征向量</span></span><br><span class=\"line\"><span class=\"comment\"># 将特征向量重塑为4x1的矩阵形式，以便进行矩阵操作</span></span><br><span class=\"line\"><span class=\"comment\"># 这两个特征向量代表了数据中方差最大的两个方向</span></span><br><span class=\"line\">matrix_w = np.hstack((eig_pairs[<span class=\"number\">0</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">                      eig_pairs[<span class=\"number\">1</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印转换矩阵W，以便于查看和验证结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Matrix W:\\n&#x27;</span>, matrix_w)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Matrix W:\n [[ 0.52308496 -0.36956962]\n [-0.25956935 -0.92681168]\n [ 0.58184289 -0.01912775]\n [ 0.56609604 -0.06381646]]\n</code></pre>\n<h1 id=\"矩阵降维\"><a href=\"#矩阵降维\" class=\"headerlink\" title=\"矩阵降维\"></a>矩阵降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 标准化数据X_std与权重矩阵matrix_w进行点积运算，用于特征提取和数据转换</span></span><br><span class=\"line\">Y = X_std.dot(matrix_w)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出转换后的数据Y</span></span><br><span class=\"line\">Y</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>array([[-2.10795032,  0.64427554],\n       [-2.38797131,  0.30583307],\n       [-2.32487909,  0.56292316],\n       [-2.40508635, -0.687591  ],\n       [-2.08320351, -1.53025171],\n       [-2.4636848 , -0.08795413],\n       [-2.25174963, -0.25964365],\n       [-2.3645813 ,  1.08255676],\n       [-2.20946338,  0.43707676],\n       [-2.17862017, -1.08221046],\n       [-2.34525657, -0.17122946],\n       [-2.24590315,  0.6974389 ],\n       [-2.66214582,  0.92447316],\n       [-2.2050227 , -1.90150522],\n       [-2.25993023, -2.73492274],\n       [-2.21591283, -1.52588897],\n       [-2.20705382, -0.52623535],\n       [-1.9077081 , -1.4415791 ],\n       [-2.35411558, -1.17088308],\n       [-1.93202643, -0.44083479],\n       [-2.21942518, -0.96477499],\n       [-2.79116421, -0.50421849],\n       [-1.83814105, -0.11729122],\n       [-2.24572458, -0.17450151],\n       [-1.97825353,  0.59734172],\n       [-2.06935091, -0.27755619],\n       [-2.18514506, -0.56366755],\n       [-2.15824269, -0.34805785],\n       [-2.28843932,  0.30256102],\n       [-2.16501749,  0.47232759],\n       [-1.8491597 , -0.45547527],\n       [-2.62023392, -1.84237072],\n       [-2.44885384, -2.1984673 ],\n       [-2.20946338,  0.43707676],\n       [-2.23112223,  0.17266644],\n       [-2.06147331, -0.6957435 ],\n       [-2.20946338,  0.43707676],\n       [-2.45783833,  0.86912843],\n       [-2.1884075 , -0.30439609],\n       [-2.30357329, -0.48039222],\n       [-1.89932763,  2.31759817],\n       [-2.57799771,  0.4400904 ],\n       [-1.98020921, -0.50889705],\n       [-2.14679556, -1.18365675],\n       [-2.09668176,  0.68061705],\n       [-2.39554894, -1.16356284],\n       [-2.41813611,  0.34949483],\n       [-2.24196231, -1.03745802],\n       [-2.22484727, -0.04403395],\n       [ 1.09225538, -0.86148748],\n       [ 0.72045861, -0.59920238],\n       [ 1.2299583 , -0.61280832],\n       [ 0.37598859,  1.756516  ],\n       [ 1.05729685,  0.21303055],\n       [ 0.36816104,  0.58896262],\n       [ 0.73800214, -0.77956125],\n       [-0.52021731,  1.84337921],\n       [ 0.9113379 , -0.02941906],\n       [-0.01292322,  1.02537703],\n       [-0.15020174,  2.65452146],\n       [ 0.42437533,  0.05686991],\n       [ 0.52894687,  1.77250558],\n       [ 0.70241525,  0.18484154],\n       [-0.05385675,  0.42901221],\n       [ 0.86277668, -0.50943908],\n       [ 0.33388091,  0.18785518],\n       [ 0.13504146,  0.7883247 ],\n       [ 1.19457128,  1.63549265],\n       [ 0.13677262,  1.30063807],\n       [ 0.72711201, -0.40394501],\n       [ 0.45564294,  0.41540628],\n       [ 1.21038365,  0.94282042],\n       [ 0.61327355,  0.4161824 ],\n       [ 0.68512164,  0.06335788],\n       [ 0.85951424, -0.25016762],\n       [ 1.23906722,  0.08500278],\n       [ 1.34575245, -0.32669695],\n       [ 0.64732915,  0.22336443],\n       [-0.06728496,  1.05414028],\n       [ 0.10033285,  1.56100021],\n       [-0.00745518,  1.57050182],\n       [ 0.2179082 ,  0.77368423],\n       [ 1.04116321,  0.63744742],\n       [ 0.20719664,  0.27736006],\n       [ 0.42154138, -0.85764157],\n       [ 1.03691937, -0.52112206],\n       [ 1.015435  ,  1.39413373],\n       [ 0.0519502 ,  0.20903977],\n       [ 0.25582921,  1.32747797],\n       [ 0.25384813,  1.11700714],\n       [ 0.60915822, -0.02858679],\n       [ 0.31116522,  0.98711256],\n       [-0.39679548,  2.01314578],\n       [ 0.26536661,  0.85150613],\n       [ 0.07385897,  0.17160757],\n       [ 0.20854936,  0.37771566],\n       [ 0.55843737,  0.15286277],\n       [-0.47853403,  1.53421644],\n       [ 0.23545172,  0.59332536],\n       [ 1.8408037 , -0.86943848],\n       [ 1.13831104,  0.70171953],\n       [ 2.19615974, -0.54916658],\n       [ 1.42613827,  0.05187679],\n       [ 1.8575403 , -0.28797217],\n       [ 2.74511173, -0.78056359],\n       [ 0.34010583,  1.5568955 ],\n       [ 2.29180093, -0.40328242],\n       [ 1.98618025,  0.72876171],\n       [ 2.26382116, -1.91685818],\n       [ 1.35591821, -0.69255356],\n       [ 1.58471851,  0.43102351],\n       [ 1.87342402, -0.41054652],\n       [ 1.23656166,  1.16818977],\n       [ 1.45128483,  0.4451459 ],\n       [ 1.58276283, -0.67521526],\n       [ 1.45956552, -0.25105642],\n       [ 2.43560434, -2.55096977],\n       [ 3.29752602,  0.01266612],\n       [ 1.23377366,  1.71954411],\n       [ 2.03218282, -0.90334021],\n       [ 0.95980311,  0.57047585],\n       [ 2.88717988, -0.38895776],\n       [ 1.31405636,  0.48854962],\n       [ 1.69619746, -1.01153249],\n       [ 1.94868773, -0.99881497],\n       [ 1.1574572 ,  0.31987373],\n       [ 1.007133  , -0.06550254],\n       [ 1.7733922 ,  0.19641059],\n       [ 1.85327106, -0.55077372],\n       [ 2.4234788 , -0.2397454 ],\n       [ 2.31353522, -2.62038074],\n       [ 1.84800289,  0.18799967],\n       [ 1.09649923,  0.29708201],\n       [ 1.1812503 ,  0.81858241],\n       [ 2.79178861, -0.83668445],\n       [ 1.57340399, -1.07118383],\n       [ 1.33614369, -0.420823  ],\n       [ 0.91061354, -0.01965942],\n       [ 1.84350913, -0.66872729],\n       [ 2.00701161, -0.60663655],\n       [ 1.89319854, -0.68227708],\n       [ 1.13831104,  0.70171953],\n       [ 2.03519535, -0.86076914],\n       [ 1.99464025, -1.04517619],\n       [ 1.85977129, -0.37934387],\n       [ 1.54200377,  0.90808604],\n       [ 1.50925493, -0.26460621],\n       [ 1.3690965 , -1.01583909],\n       [ 0.94680339,  0.02182097]])\n</code></pre>\n<p>查看降维前的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(X[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                X[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;sepal_len&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;sepal_wid&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_26_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_26_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>查看降维后的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(Y[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                Y[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower center&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_28_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_28_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"利用SK-LEARN-PCA进行降维\"><a href=\"#利用SK-LEARN-PCA进行降维\" class=\"headerlink\" title=\"利用SK-LEARN PCA进行降维\"></a>利用SK-LEARN PCA进行降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 加载鸢尾花数据集</span></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 数据标准化</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">X_std = scaler.fit_transform(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 应用PCA降维</span></span><br><span class=\"line\">pca = PCA()</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 计算特征值总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(pca.explained_variance_)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\">var_exp = [(i / tot) * <span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(pca.explained_variance_, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 6. 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;变异解释百分比:&quot;</span>, var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 7. 计算累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 8. 输出累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;累计变异解释百分比:&quot;</span>, cum_var_exp)</span><br></pre></td></tr></table></figure>\n\n<pre><code>变异解释百分比: [np.float64(72.96244541329987), np.float64(22.850761786701742), np.float64(3.668921889282886), np.float64(0.5178709107154782)]\n累计变异解释百分比: [ 72.96244541  95.8132072   99.48212909 100.        ]\n</code></pre>\n<p>由上述结果同样可见，前两个特征向量累计贡献的方差为95.81%，故取前2个特征向量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 应用PCA降维到2个主成分</span></span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>)</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 绘制PCA图</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为不同的类别绘制不同颜色的点</span></span><br><span class=\"line\">colors = [<span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>]</span><br><span class=\"line\">markers = [<span class=\"string\">&#x27;s&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;o&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> idx, color, marker <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"built_in\">range</span>(<span class=\"number\">3</span>), colors, markers):</span><br><span class=\"line\">    plt.scatter(x=X_pca[y == idx, <span class=\"number\">0</span>], </span><br><span class=\"line\">                y=X_pca[y == idx, <span class=\"number\">1</span>],</span><br><span class=\"line\">                c=color, </span><br><span class=\"line\">                marker=marker,</span><br><span class=\"line\">                label=iris.target_names[idx])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加标题和标签</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.grid(<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图形</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_32_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_32_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h1><p>关注公众号“生信之巅”，获取更多教程。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","raw":null,"categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"Scikit-learn","path":"api/tags/Scikit-learn.json"}]}]}
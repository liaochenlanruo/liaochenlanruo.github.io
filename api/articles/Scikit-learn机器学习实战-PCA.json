{"title":"Scikit-learn机器学习实战-PCA","slug":"Scikit-learn机器学习实战-PCA","date":"2024-09-01T08:38:58.000Z","updated":"2024-09-01T08:47:58.888Z","comments":true,"path":"api/articles/Scikit-learn机器学习实战-PCA.json","excerpt":null,"covers":["/PCA_files/PCA_5_0.png","/PCA_files/PCA_20_0.png","/PCA_files/PCA_26_0.png","/PCA_files/PCA_28_0.png","/PCA_files/PCA_32_0.png","https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg","https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png"],"content":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>PCA的全称是<strong>Principal Component Analysis</strong>，即主成分分析。这是一种常用的数据分析方法，主要用于数据降维。PCA的主要思想是将原始的高维特征空间通过线性变换投射到一个新的低维特征空间上，同时尽量保持原始数据的方差，使得在新的低维空间中数据的差异性得以保留。这一过程中，通过计算数据集的协方差矩阵，找到其特征值和特征向量，进而确定主成分的方向和贡献率，实现数据的有效降维。</p>\n<p>具体来说，PCA的计算过程包括以下几个步骤：</p>\n<ul>\n<li>数据标准化：将原始数据转换为均值为0，标准差为1的标准化数据，以消除不同量纲对分析结果的影响。</li>\n<li>计算协方差矩阵：标准化后的数据矩阵的协方差矩阵反映了各变量之间的相关性。</li>\n<li>计算特征值和特征向量：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征值的大小反映了对应主成分的重要性，而特征向量则指示了主成分的方向。</li>\n<li>选取主成分：根据特征值的大小，选取前k个主成分，使得这k个主成分的累计贡献率达到一定的阈值（如80%或90%）。</li>\n<li>转换数据到新的主成分空间：将原始数据转换到由选定的主成分构成的新空间中，得到降维后的数据。</li>\n</ul>\n<p>PCA在数据分析和机器学习领域有着广泛的应用，如特征提取、数据压缩、噪声消除、图像识别等。同时，PCA也是一种无监督学习的方法，它不需要数据的标签信息，就可以从数据中提取出有用的特征信息。</p>\n<p>本文通过鸢尾花数据集演示PCA方法，讲解如何确定降维采用的特征向量的数量，并完成降维。包括手写函数实现和通过SK-learn实现代码。</p>\n<h1 id=\"导入数据集\"><a href=\"#导入数据集\" class=\"headerlink\" title=\"导入数据集\"></a>导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;iris.data&#x27;</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>5.1</th>\n      <th>3.5</th>\n      <th>1.4</th>\n      <th>0.2</th>\n      <th>Iris-setosa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加列名，分别为 sepal_len, sepal_wid, petal_len, petal_wid, class</span></span><br><span class=\"line\"><span class=\"comment\"># 前四列为特征，最后一列为类别，共有3种类别</span></span><br><span class=\"line\">df.columns=[<span class=\"string\">&#x27;sepal_len&#x27;</span>, <span class=\"string\">&#x27;sepal_wid&#x27;</span>, <span class=\"string\">&#x27;petal_len&#x27;</span>, <span class=\"string\">&#x27;petal_wid&#x27;</span>, <span class=\"string\">&#x27;class&#x27;</span>]</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_len</th>\n      <th>sepal_wid</th>\n      <th>petal_len</th>\n      <th>petal_wid</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 拆分特征和标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = df.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">4</span>].values</span><br><span class=\"line\">y = df.iloc[:,<span class=\"number\">4</span>].values</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h1><h2 id=\"特征展示\"><a href=\"#特征展示\" class=\"headerlink\" title=\"特征展示\"></a>特征展示</h2><p>查看特征的取值范围以及其和类别的关系。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入matplotlib库中的pyplot模块，用于绘图</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 导入math库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的种类名称</span></span><br><span class=\"line\">label_dict = &#123;<span class=\"number\">1</span>: <span class=\"string\">&#x27;Iris-Setosa&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">2</span>: <span class=\"string\">&#x27;Iris-Versicolor&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">3</span>: <span class=\"string\">&#x27;Iris-Virgnica&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的特征名称</span></span><br><span class=\"line\">feature_dict = &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;sepal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">1</span>: <span class=\"string\">&#x27;sepal width [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">2</span>: <span class=\"string\">&#x27;petal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">3</span>: <span class=\"string\">&#x27;petal width [cm]&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建一个8x6英寸的图像</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"><span class=\"comment\"># 循环绘制4个子图，每个子图代表一个特征的直方图</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> cnt <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 在图像中创建2x2的子图，并指定当前子图的位置</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, cnt+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 遍历每种鸢尾花类别，绘制直方图</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> lab <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 绘制指定特征的直方图，使用不同颜色和透明度区分不同类别</span></span><br><span class=\"line\">        plt.hist(X[y==lab, cnt],</span><br><span class=\"line\">                 label=lab,</span><br><span class=\"line\">                 bins=<span class=\"number\">10</span>,</span><br><span class=\"line\">                 alpha=<span class=\"number\">0.3</span>,)</span><br><span class=\"line\">    <span class=\"comment\"># 设置子图的x轴标签</span></span><br><span class=\"line\">    plt.xlabel(feature_dict[cnt])</span><br><span class=\"line\">    <span class=\"comment\"># 添加图例，用于标识不同类别的鸢尾花</span></span><br><span class=\"line\">    plt.legend(loc=<span class=\"string\">&#x27;upper right&#x27;</span>, fancybox=<span class=\"literal\">True</span>, fontsize=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调整子图间的间距，使布局更加紧凑</span></span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\"><span class=\"comment\"># 显示绘制的图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_5_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_5_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<ul>\n<li>发现<code>花萼的长度</code>和<code>宽度</code>能够较好的区分3种类别。</li>\n<li>四个特征取值范围差异较大，需要进行标准化。</li>\n</ul>\n<h2 id=\"数据标准化\"><a href=\"#数据标准化\" class=\"headerlink\" title=\"数据标准化\"></a>数据标准化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 进行数据标准化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">X_std = StandardScaler().fit_transform(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_std)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[[-1.1483555  -0.11805969 -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.41098555 -1.32506301]\n [-1.51163569  0.11339944 -1.29694332 -1.32506301]\n [-1.02726211  1.27069504 -1.35396443 -1.32506301]\n [-0.54288852  1.9650724  -1.18290109 -1.0614657 ]\n [-1.51163569  0.8077768  -1.35396443 -1.19326436]\n [-1.02726211  0.8077768  -1.29694332 -1.32506301]\n [-1.75382249 -0.34951881 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-0.54288852  1.50215416 -1.29694332 -1.32506301]\n [-1.2694489   0.8077768  -1.23992221 -1.32506301]\n [-1.2694489  -0.11805969 -1.35396443 -1.45686167]\n [-1.87491588 -0.11805969 -1.52502777 -1.45686167]\n [-0.05851493  2.19653152 -1.46800666 -1.32506301]\n [-0.17960833  3.122368   -1.29694332 -1.0614657 ]\n [-0.54288852  1.9650724  -1.41098555 -1.0614657 ]\n [-0.90616871  1.03923592 -1.35396443 -1.19326436]\n [-0.17960833  1.73361328 -1.18290109 -1.19326436]\n [-0.90616871  1.73361328 -1.29694332 -1.19326436]\n [-0.54288852  0.8077768  -1.18290109 -1.32506301]\n [-0.90616871  1.50215416 -1.29694332 -1.0614657 ]\n [-1.51163569  1.27069504 -1.58204889 -1.32506301]\n [-0.90616871  0.57631768 -1.18290109 -0.92966704]\n [-1.2694489   0.8077768  -1.06885886 -1.32506301]\n [-1.02726211 -0.11805969 -1.23992221 -1.32506301]\n [-1.02726211  0.8077768  -1.23992221 -1.0614657 ]\n [-0.78507531  1.03923592 -1.29694332 -1.32506301]\n [-0.78507531  0.8077768  -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.23992221 -1.32506301]\n [-1.2694489   0.11339944 -1.23992221 -1.32506301]\n [-0.54288852  0.8077768  -1.29694332 -1.0614657 ]\n [-0.78507531  2.42799064 -1.29694332 -1.45686167]\n [-0.42179512  2.65944976 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.02726211  0.34485856 -1.46800666 -1.32506301]\n [-0.42179512  1.03923592 -1.41098555 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.75382249 -0.11805969 -1.41098555 -1.32506301]\n [-0.90616871  0.8077768  -1.29694332 -1.32506301]\n [-1.02726211  1.03923592 -1.41098555 -1.19326436]\n [-1.63272909 -1.73827353 -1.41098555 -1.19326436]\n [-1.75382249  0.34485856 -1.41098555 -1.32506301]\n [-1.02726211  1.03923592 -1.23992221 -0.79786838]\n [-0.90616871  1.73361328 -1.06885886 -1.0614657 ]\n [-1.2694489  -0.11805969 -1.35396443 -1.19326436]\n [-0.90616871  1.73361328 -1.23992221 -1.32506301]\n [-1.51163569  0.34485856 -1.35396443 -1.32506301]\n [-0.66398191  1.50215416 -1.29694332 -1.32506301]\n [-1.02726211  0.57631768 -1.35396443 -1.32506301]\n [ 1.39460583  0.34485856  0.52773232  0.25652088]\n [ 0.66804545  0.34485856  0.41369009  0.38831953]\n [ 1.27351244  0.11339944  0.64177455  0.38831953]\n [-0.42179512 -1.73827353  0.12858453  0.12472222]\n [ 0.78913885 -0.58097793  0.47071121  0.38831953]\n [-0.17960833 -0.58097793  0.41369009  0.12472222]\n [ 0.54695205  0.57631768  0.52773232  0.52011819]\n [-1.1483555  -1.50681441 -0.27056327 -0.27067375]\n [ 0.91023225 -0.34951881  0.47071121  0.12472222]\n [-0.78507531 -0.81243705  0.07156341  0.25652088]\n [-1.02726211 -2.43265089 -0.15652104 -0.27067375]\n [ 0.06257847 -0.11805969  0.24262675  0.38831953]\n [ 0.18367186 -1.96973265  0.12858453 -0.27067375]\n [ 0.30476526 -0.34951881  0.52773232  0.25652088]\n [-0.30070172 -0.34951881 -0.09949993  0.12472222]\n [ 1.03132564  0.11339944  0.35666898  0.25652088]\n [-0.30070172 -0.11805969  0.41369009  0.38831953]\n [-0.05851493 -0.81243705  0.18560564 -0.27067375]\n [ 0.42585866 -1.96973265  0.41369009  0.38831953]\n [-0.30070172 -1.27535529  0.07156341 -0.1388751 ]\n [ 0.06257847  0.34485856  0.58475344  0.78371551]\n [ 0.30476526 -0.58097793  0.12858453  0.12472222]\n [ 0.54695205 -1.27535529  0.64177455  0.38831953]\n [ 0.30476526 -0.58097793  0.52773232 -0.00707644]\n [ 0.66804545 -0.34951881  0.29964787  0.12472222]\n [ 0.91023225 -0.11805969  0.35666898  0.25652088]\n [ 1.15241904 -0.58097793  0.58475344  0.25652088]\n [ 1.03132564 -0.11805969  0.69879566  0.65191685]\n [ 0.18367186 -0.34951881  0.41369009  0.38831953]\n [-0.17960833 -1.04389617 -0.15652104 -0.27067375]\n [-0.42179512 -1.50681441  0.0145423  -0.1388751 ]\n [-0.42179512 -1.50681441 -0.04247882 -0.27067375]\n [-0.05851493 -0.81243705  0.07156341 -0.00707644]\n [ 0.18367186 -0.81243705  0.75581678  0.52011819]\n [-0.54288852 -0.11805969  0.41369009  0.38831953]\n [ 0.18367186  0.8077768   0.41369009  0.52011819]\n [ 1.03132564  0.11339944  0.52773232  0.38831953]\n [ 0.54695205 -1.73827353  0.35666898  0.12472222]\n [-0.30070172 -0.11805969  0.18560564  0.12472222]\n [-0.42179512 -1.27535529  0.12858453  0.12472222]\n [-0.42179512 -1.04389617  0.35666898 -0.00707644]\n [ 0.30476526 -0.11805969  0.47071121  0.25652088]\n [-0.05851493 -1.04389617  0.12858453 -0.00707644]\n [-1.02726211 -1.73827353 -0.27056327 -0.27067375]\n [-0.30070172 -0.81243705  0.24262675  0.12472222]\n [-0.17960833 -0.11805969  0.24262675 -0.00707644]\n [-0.17960833 -0.34951881  0.24262675  0.12472222]\n [ 0.42585866 -0.34951881  0.29964787  0.12472222]\n [-0.90616871 -1.27535529 -0.44162661 -0.1388751 ]\n [-0.17960833 -0.58097793  0.18560564  0.12472222]\n [ 0.54695205  0.57631768  1.2690068   1.70630611]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.51569923 -0.11805969  1.21198569  1.17911148]\n [ 0.54695205 -0.34951881  1.04092235  0.78371551]\n [ 0.78913885 -0.11805969  1.15496457  1.31091014]\n [ 2.12116622 -0.11805969  1.61113348  1.17911148]\n [-1.1483555  -1.27535529  0.41369009  0.65191685]\n [ 1.75788602 -0.34951881  1.44007014  0.78371551]\n [ 1.03132564 -1.27535529  1.15496457  0.78371551]\n [ 1.63679263  1.27069504  1.32602791  1.70630611]\n [ 0.78913885  0.34485856  0.75581678  1.04731282]\n [ 0.66804545 -0.81243705  0.869859    0.91551417]\n [ 1.15241904 -0.11805969  0.98390123  1.17911148]\n [-0.17960833 -1.27535529  0.69879566  1.04731282]\n [-0.05851493 -0.58097793  0.75581678  1.57450745]\n [ 0.66804545  0.34485856  0.869859    1.4427088 ]\n [ 0.78913885 -0.11805969  0.98390123  0.78371551]\n [ 2.24225961  1.73361328  1.6681546   1.31091014]\n [ 2.24225961 -1.04389617  1.78219682  1.4427088 ]\n [ 0.18367186 -1.96973265  0.69879566  0.38831953]\n [ 1.27351244  0.34485856  1.09794346  1.4427088 ]\n [-0.30070172 -0.58097793  0.64177455  1.04731282]\n [ 2.24225961 -0.58097793  1.6681546   1.04731282]\n [ 0.54695205 -0.81243705  0.64177455  0.78371551]\n [ 1.03132564  0.57631768  1.09794346  1.17911148]\n [ 1.63679263  0.34485856  1.2690068   0.78371551]\n [ 0.42585866 -0.58097793  0.58475344  0.78371551]\n [ 0.30476526 -0.11805969  0.64177455  0.78371551]\n [ 0.66804545 -0.58097793  1.04092235  1.17911148]\n [ 1.63679263 -0.11805969  1.15496457  0.52011819]\n [ 1.87897942 -0.58097793  1.32602791  0.91551417]\n [ 2.48444641  1.73361328  1.49709126  1.04731282]\n [ 0.66804545 -0.58097793  1.04092235  1.31091014]\n [ 0.54695205 -0.58097793  0.75581678  0.38831953]\n [ 0.30476526 -1.04389617  1.04092235  0.25652088]\n [ 2.24225961 -0.11805969  1.32602791  1.4427088 ]\n [ 0.54695205  0.8077768   1.04092235  1.57450745]\n [ 0.66804545  0.11339944  0.98390123  0.78371551]\n [ 0.18367186 -0.11805969  0.58475344  0.78371551]\n [ 1.27351244  0.11339944  0.92688012  1.17911148]\n [ 1.03132564  0.11339944  1.04092235  1.57450745]\n [ 1.27351244  0.11339944  0.75581678  1.4427088 ]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.15241904  0.34485856  1.21198569  1.4427088 ]\n [ 1.03132564  0.57631768  1.09794346  1.70630611]\n [ 1.03132564 -0.11805969  0.81283789  1.4427088 ]\n [ 0.54695205 -1.27535529  0.69879566  0.91551417]\n [ 0.78913885 -0.11805969  0.81283789  1.04731282]\n [ 0.42585866  0.8077768   0.92688012  1.4427088 ]\n [ 0.06257847 -0.11805969  0.75581678  0.78371551]]\n</code></pre>\n<h1 id=\"确定特征向量的数量\"><a href=\"#确定特征向量的数量\" class=\"headerlink\" title=\"确定特征向量的数量\"></a>确定特征向量的数量</h1><h2 id=\"协方差矩阵\"><a href=\"#协方差矩阵\" class=\"headerlink\" title=\"协方差矩阵\"></a>协方差矩阵</h2><p>协方差是衡量两个变量总体误差的期望，用于描述两个变量之间的线性关系程度和方向。两个特征间的协方差值越大，表明其相关性越强。</p>\n<p>对于两个随机变量$X$和$Y$，其协方差$Cov(X,Y)$的计算公式为：</p>\n<p>$$Cov(X,Y) &#x3D; E[(X - E[X])(Y - E[Y])]$$</p>\n<p>其中，$E[X]$和$E[Y]$分别是$X$和$Y$的期望值（即均值）。</p>\n<p>在实际应用中，当我们只有样本数据时，我们通常使用样本协方差来估计总体协方差。对于包含$n$个样本点的数据集，样本协方差$s_{xy}$的计算公式为：</p>\n<p>$$s_{xy} &#x3D; \\frac{1}{n-1} \\sum_{i&#x3D;1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$</p>\n<p>其中，$x_i$和$y_i$是样本点，$\\bar{x}$和$\\bar{y}$分别是$X$和$Y$的样本均值，计算公式为：</p>\n<p>$$\\bar{x} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} x_i$$</p>\n<p>$$\\bar{y} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} y_i$$</p>\n<p>注意，在计算样本协方差时，分母是$n-1$而不是$n$，这是为了得到总体协方差的无偏估计。</p>\n<p><strong>以下命令二选一</strong>：</p>\n<ul>\n<li>构造函数计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算特征向量的均值，用于中心化数据</span></span><br><span class=\"line\">mean_vec = np.mean(X_std, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算数据的协方差矩阵，用于理解特征之间的线性关系</span></span><br><span class=\"line\"><span class=\"comment\"># (X_std - mean_vec).T.dot((X_std - mean_vec) )计算的是(X_std - mean_vec)的矩阵乘法</span></span><br><span class=\"line\"><span class=\"comment\"># 除以(X_std.shape[0]-1)是为了获得一个无偏估计</span></span><br><span class=\"line\">cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出协方差矩阵，以便于后续分析和使用</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Covariance matrix \\n%s&#x27;</span> %cov_mat)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Covariance matrix \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<ul>\n<li>使用numpy计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;NumPy covariance matrix: \\n%s&#x27;</span> %np.cov(X_std.T))</span><br></pre></td></tr></table></figure>\n\n<pre><code>NumPy covariance matrix: \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<p>求协方差矩阵的特征向量和特征值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 求协方差矩阵</span></span><br><span class=\"line\">cov_mat = np.cov(X_std.T)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 求协方差矩阵的特征向量，返回特征向量（eig_vecs）和特征值（eig_vals）</span></span><br><span class=\"line\">eig_vals, eig_vecs = np.linalg.eig(cov_mat)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvectors \\n%s&#x27;</span> %eig_vecs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;\\nEigenvalues \\n%s&#x27;</span> %eig_vals)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Eigenvectors \n[[ 0.52308496 -0.36956962 -0.72154279  0.26301409]\n [-0.25956935 -0.92681168  0.2411952  -0.12437342]\n [ 0.58184289 -0.01912775  0.13962963 -0.80099722]\n [ 0.56609604 -0.06381646  0.63380158  0.52321917]]\n\nEigenvalues \n[2.92442837 0.93215233 0.14946373 0.02098259]\n</code></pre>\n<p>肉眼观察<code>Eigenvalues</code>，应当取前2个特征值对应的特征向量，因为其最重要。</p>\n<p>创建一个包含特征值和对应特征向量的元组列表。按特征值降序排序这些元组。打印排序后的特征值，确认排序正确。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Make a list of (eigenvalue, eigenvector) tuples</span></span><br><span class=\"line\">eig_pairs = [(np.<span class=\"built_in\">abs</span>(eig_vals[i]), eig_vecs[:,i]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(eig_vals))]</span><br><span class=\"line\"><span class=\"built_in\">print</span> (eig_pairs)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># Sort the (eigenvalue, eigenvector) tuples from high to low</span></span><br><span class=\"line\">eig_pairs.sort(key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">0</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Visually confirm that the list is correctly sorted by decreasing eigenvalues</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvalues in descending order:&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> eig_pairs:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[(np.float64(2.9244283691111117), array([ 0.52308496, -0.25956935,  0.58184289,  0.56609604])), (np.float64(0.9321523302535062), array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (np.float64(0.14946373489813364), array([-0.72154279,  0.2411952 ,  0.13962963,  0.63380158])), (np.float64(0.020982592764271016), array([ 0.26301409, -0.12437342, -0.80099722,  0.52321917]))]\n----------\nEigenvalues in descending order:\n2.9244283691111117\n0.9321523302535062\n0.14946373489813364\n0.020982592764271016\n</code></pre>\n<p>计算累计变异解释百分比，以评估前几个主成分能解释的总变异比例。最后输出累计变异解释百分比。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算所有特征值的总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(eig_vals)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步的目的是确定每个特征值在总变异中所占的百分比</span></span><br><span class=\"line\">var_exp = [(i / tot)*<span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(eig_vals, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步是为了了解前几个主成分可以解释多少总变异</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp</span><br></pre></td></tr></table></figure>\n\n<pre><code>[np.float64(72.62003332692032), np.float64(23.147406858644143), np.float64(3.711515564584531), np.float64(0.5210442498510259)]\n\n\n\n\n\narray([ 72.62003333,  95.76744019,  99.47895575, 100.        ])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 演示np.cumsum() 函数的用法</span></span><br><span class=\"line\">a = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span> (a)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;-----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (np.cumsum(a)) <span class=\"comment\"># 元素的累加和</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>[1 2 3 4]\n-----------\n[ 1  3  6 10]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制特征值的重要性</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.bar(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), var_exp, alpha=<span class=\"number\">0.5</span>, align=<span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            label=<span class=\"string\">&#x27;individual explained variance&#x27;</span>)</span><br><span class=\"line\">plt.step(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), cum_var_exp, where=<span class=\"string\">&#x27;mid&#x27;</span>,</span><br><span class=\"line\">             label=<span class=\"string\">&#x27;cumulative explained variance&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Explained variance ratio&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal components&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_20_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_20_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>根据上图可以决定取几个特征值进行降维，此例中前两个特征值已经足够（~95.77%）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 根据前两个主要成分构造转换矩阵W</span></span><br><span class=\"line\"><span class=\"comment\"># 选择主要成分分析（PCA）得到的前两个特征向量</span></span><br><span class=\"line\"><span class=\"comment\"># 将特征向量重塑为4x1的矩阵形式，以便进行矩阵操作</span></span><br><span class=\"line\"><span class=\"comment\"># 这两个特征向量代表了数据中方差最大的两个方向</span></span><br><span class=\"line\">matrix_w = np.hstack((eig_pairs[<span class=\"number\">0</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">                      eig_pairs[<span class=\"number\">1</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印转换矩阵W，以便于查看和验证结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Matrix W:\\n&#x27;</span>, matrix_w)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Matrix W:\n [[ 0.52308496 -0.36956962]\n [-0.25956935 -0.92681168]\n [ 0.58184289 -0.01912775]\n [ 0.56609604 -0.06381646]]\n</code></pre>\n<h1 id=\"矩阵降维\"><a href=\"#矩阵降维\" class=\"headerlink\" title=\"矩阵降维\"></a>矩阵降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 标准化数据X_std与权重矩阵matrix_w进行点积运算，用于特征提取和数据转换</span></span><br><span class=\"line\">Y = X_std.dot(matrix_w)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出转换后的数据Y</span></span><br><span class=\"line\">Y</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>array([[-2.10795032,  0.64427554],\n       [-2.38797131,  0.30583307],\n       [-2.32487909,  0.56292316],\n       [-2.40508635, -0.687591  ],\n       [-2.08320351, -1.53025171],\n       [-2.4636848 , -0.08795413],\n       [-2.25174963, -0.25964365],\n       [-2.3645813 ,  1.08255676],\n       [-2.20946338,  0.43707676],\n       [-2.17862017, -1.08221046],\n       [-2.34525657, -0.17122946],\n       [-2.24590315,  0.6974389 ],\n       [-2.66214582,  0.92447316],\n       [-2.2050227 , -1.90150522],\n       [-2.25993023, -2.73492274],\n       [-2.21591283, -1.52588897],\n       [-2.20705382, -0.52623535],\n       [-1.9077081 , -1.4415791 ],\n       [-2.35411558, -1.17088308],\n       [-1.93202643, -0.44083479],\n       [-2.21942518, -0.96477499],\n       [-2.79116421, -0.50421849],\n       [-1.83814105, -0.11729122],\n       [-2.24572458, -0.17450151],\n       [-1.97825353,  0.59734172],\n       [-2.06935091, -0.27755619],\n       [-2.18514506, -0.56366755],\n       [-2.15824269, -0.34805785],\n       [-2.28843932,  0.30256102],\n       [-2.16501749,  0.47232759],\n       [-1.8491597 , -0.45547527],\n       [-2.62023392, -1.84237072],\n       [-2.44885384, -2.1984673 ],\n       [-2.20946338,  0.43707676],\n       [-2.23112223,  0.17266644],\n       [-2.06147331, -0.6957435 ],\n       [-2.20946338,  0.43707676],\n       [-2.45783833,  0.86912843],\n       [-2.1884075 , -0.30439609],\n       [-2.30357329, -0.48039222],\n       [-1.89932763,  2.31759817],\n       [-2.57799771,  0.4400904 ],\n       [-1.98020921, -0.50889705],\n       [-2.14679556, -1.18365675],\n       [-2.09668176,  0.68061705],\n       [-2.39554894, -1.16356284],\n       [-2.41813611,  0.34949483],\n       [-2.24196231, -1.03745802],\n       [-2.22484727, -0.04403395],\n       [ 1.09225538, -0.86148748],\n       [ 0.72045861, -0.59920238],\n       [ 1.2299583 , -0.61280832],\n       [ 0.37598859,  1.756516  ],\n       [ 1.05729685,  0.21303055],\n       [ 0.36816104,  0.58896262],\n       [ 0.73800214, -0.77956125],\n       [-0.52021731,  1.84337921],\n       [ 0.9113379 , -0.02941906],\n       [-0.01292322,  1.02537703],\n       [-0.15020174,  2.65452146],\n       [ 0.42437533,  0.05686991],\n       [ 0.52894687,  1.77250558],\n       [ 0.70241525,  0.18484154],\n       [-0.05385675,  0.42901221],\n       [ 0.86277668, -0.50943908],\n       [ 0.33388091,  0.18785518],\n       [ 0.13504146,  0.7883247 ],\n       [ 1.19457128,  1.63549265],\n       [ 0.13677262,  1.30063807],\n       [ 0.72711201, -0.40394501],\n       [ 0.45564294,  0.41540628],\n       [ 1.21038365,  0.94282042],\n       [ 0.61327355,  0.4161824 ],\n       [ 0.68512164,  0.06335788],\n       [ 0.85951424, -0.25016762],\n       [ 1.23906722,  0.08500278],\n       [ 1.34575245, -0.32669695],\n       [ 0.64732915,  0.22336443],\n       [-0.06728496,  1.05414028],\n       [ 0.10033285,  1.56100021],\n       [-0.00745518,  1.57050182],\n       [ 0.2179082 ,  0.77368423],\n       [ 1.04116321,  0.63744742],\n       [ 0.20719664,  0.27736006],\n       [ 0.42154138, -0.85764157],\n       [ 1.03691937, -0.52112206],\n       [ 1.015435  ,  1.39413373],\n       [ 0.0519502 ,  0.20903977],\n       [ 0.25582921,  1.32747797],\n       [ 0.25384813,  1.11700714],\n       [ 0.60915822, -0.02858679],\n       [ 0.31116522,  0.98711256],\n       [-0.39679548,  2.01314578],\n       [ 0.26536661,  0.85150613],\n       [ 0.07385897,  0.17160757],\n       [ 0.20854936,  0.37771566],\n       [ 0.55843737,  0.15286277],\n       [-0.47853403,  1.53421644],\n       [ 0.23545172,  0.59332536],\n       [ 1.8408037 , -0.86943848],\n       [ 1.13831104,  0.70171953],\n       [ 2.19615974, -0.54916658],\n       [ 1.42613827,  0.05187679],\n       [ 1.8575403 , -0.28797217],\n       [ 2.74511173, -0.78056359],\n       [ 0.34010583,  1.5568955 ],\n       [ 2.29180093, -0.40328242],\n       [ 1.98618025,  0.72876171],\n       [ 2.26382116, -1.91685818],\n       [ 1.35591821, -0.69255356],\n       [ 1.58471851,  0.43102351],\n       [ 1.87342402, -0.41054652],\n       [ 1.23656166,  1.16818977],\n       [ 1.45128483,  0.4451459 ],\n       [ 1.58276283, -0.67521526],\n       [ 1.45956552, -0.25105642],\n       [ 2.43560434, -2.55096977],\n       [ 3.29752602,  0.01266612],\n       [ 1.23377366,  1.71954411],\n       [ 2.03218282, -0.90334021],\n       [ 0.95980311,  0.57047585],\n       [ 2.88717988, -0.38895776],\n       [ 1.31405636,  0.48854962],\n       [ 1.69619746, -1.01153249],\n       [ 1.94868773, -0.99881497],\n       [ 1.1574572 ,  0.31987373],\n       [ 1.007133  , -0.06550254],\n       [ 1.7733922 ,  0.19641059],\n       [ 1.85327106, -0.55077372],\n       [ 2.4234788 , -0.2397454 ],\n       [ 2.31353522, -2.62038074],\n       [ 1.84800289,  0.18799967],\n       [ 1.09649923,  0.29708201],\n       [ 1.1812503 ,  0.81858241],\n       [ 2.79178861, -0.83668445],\n       [ 1.57340399, -1.07118383],\n       [ 1.33614369, -0.420823  ],\n       [ 0.91061354, -0.01965942],\n       [ 1.84350913, -0.66872729],\n       [ 2.00701161, -0.60663655],\n       [ 1.89319854, -0.68227708],\n       [ 1.13831104,  0.70171953],\n       [ 2.03519535, -0.86076914],\n       [ 1.99464025, -1.04517619],\n       [ 1.85977129, -0.37934387],\n       [ 1.54200377,  0.90808604],\n       [ 1.50925493, -0.26460621],\n       [ 1.3690965 , -1.01583909],\n       [ 0.94680339,  0.02182097]])\n</code></pre>\n<p>查看降维前的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(X[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                X[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;sepal_len&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;sepal_wid&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_26_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_26_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<p>查看降维后的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(Y[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                Y[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower center&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_28_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_28_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"利用SK-LEARN-PCA进行降维\"><a href=\"#利用SK-LEARN-PCA进行降维\" class=\"headerlink\" title=\"利用SK-LEARN PCA进行降维\"></a>利用SK-LEARN PCA进行降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 加载鸢尾花数据集</span></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 数据标准化</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">X_std = scaler.fit_transform(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 应用PCA降维</span></span><br><span class=\"line\">pca = PCA()</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 计算特征值总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(pca.explained_variance_)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\">var_exp = [(i / tot) * <span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(pca.explained_variance_, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 6. 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;变异解释百分比:&quot;</span>, var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 7. 计算累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 8. 输出累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;累计变异解释百分比:&quot;</span>, cum_var_exp)</span><br></pre></td></tr></table></figure>\n\n<pre><code>变异解释百分比: [np.float64(72.96244541329987), np.float64(22.850761786701742), np.float64(3.668921889282886), np.float64(0.5178709107154782)]\n累计变异解释百分比: [ 72.96244541  95.8132072   99.48212909 100.        ]\n</code></pre>\n<p>由上述结果同样可见，前两个特征向量累计贡献的方差为95.81%，故取前2个特征向量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 应用PCA降维到2个主成分</span></span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>)</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 绘制PCA图</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为不同的类别绘制不同颜色的点</span></span><br><span class=\"line\">colors = [<span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>]</span><br><span class=\"line\">markers = [<span class=\"string\">&#x27;s&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;o&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> idx, color, marker <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"built_in\">range</span>(<span class=\"number\">3</span>), colors, markers):</span><br><span class=\"line\">    plt.scatter(x=X_pca[y == idx, <span class=\"number\">0</span>], </span><br><span class=\"line\">                y=X_pca[y == idx, <span class=\"number\">1</span>],</span><br><span class=\"line\">                c=color, </span><br><span class=\"line\">                marker=marker,</span><br><span class=\"line\">                label=iris.target_names[idx])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加标题和标签</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.grid(<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图形</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_32_0.png\" class=\"lazyload placeholder\" data-srcset=\"/PCA_files/PCA_32_0.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"png\"></p>\n<h1 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h1><p>关注公众号“生信之巅”，获取更多教程。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" class=\"lazyload placeholder\" data-srcset=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","more":"<h1 id=\"前言\"><a href=\"#前言\" class=\"headerlink\" title=\"前言\"></a>前言</h1><p>PCA的全称是<strong>Principal Component Analysis</strong>，即主成分分析。这是一种常用的数据分析方法，主要用于数据降维。PCA的主要思想是将原始的高维特征空间通过线性变换投射到一个新的低维特征空间上，同时尽量保持原始数据的方差，使得在新的低维空间中数据的差异性得以保留。这一过程中，通过计算数据集的协方差矩阵，找到其特征值和特征向量，进而确定主成分的方向和贡献率，实现数据的有效降维。</p>\n<p>具体来说，PCA的计算过程包括以下几个步骤：</p>\n<ul>\n<li>数据标准化：将原始数据转换为均值为0，标准差为1的标准化数据，以消除不同量纲对分析结果的影响。</li>\n<li>计算协方差矩阵：标准化后的数据矩阵的协方差矩阵反映了各变量之间的相关性。</li>\n<li>计算特征值和特征向量：对协方差矩阵进行特征值分解，得到特征值和对应的特征向量。特征值的大小反映了对应主成分的重要性，而特征向量则指示了主成分的方向。</li>\n<li>选取主成分：根据特征值的大小，选取前k个主成分，使得这k个主成分的累计贡献率达到一定的阈值（如80%或90%）。</li>\n<li>转换数据到新的主成分空间：将原始数据转换到由选定的主成分构成的新空间中，得到降维后的数据。</li>\n</ul>\n<p>PCA在数据分析和机器学习领域有着广泛的应用，如特征提取、数据压缩、噪声消除、图像识别等。同时，PCA也是一种无监督学习的方法，它不需要数据的标签信息，就可以从数据中提取出有用的特征信息。</p>\n<p>本文通过鸢尾花数据集演示PCA方法，讲解如何确定降维采用的特征向量的数量，并完成降维。包括手写函数实现和通过SK-learn实现代码。</p>\n<h1 id=\"导入数据集\"><a href=\"#导入数据集\" class=\"headerlink\" title=\"导入数据集\"></a>导入数据集</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> pandas <span class=\"keyword\">as</span> pd</span><br><span class=\"line\">df = pd.read_csv(<span class=\"string\">&#x27;iris.data&#x27;</span>)</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>5.1</th>\n      <th>3.5</th>\n      <th>1.4</th>\n      <th>0.2</th>\n      <th>Iris-setosa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加列名，分别为 sepal_len, sepal_wid, petal_len, petal_wid, class</span></span><br><span class=\"line\"><span class=\"comment\"># 前四列为特征，最后一列为类别，共有3种类别</span></span><br><span class=\"line\">df.columns=[<span class=\"string\">&#x27;sepal_len&#x27;</span>, <span class=\"string\">&#x27;sepal_wid&#x27;</span>, <span class=\"string\">&#x27;petal_len&#x27;</span>, <span class=\"string\">&#x27;petal_wid&#x27;</span>, <span class=\"string\">&#x27;class&#x27;</span>]</span><br><span class=\"line\">df.head()</span><br></pre></td></tr></table></figure>\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n<pre><code>.dataframe tbody tr th &#123;\n    vertical-align: top;\n&#125;\n\n.dataframe thead th &#123;\n    text-align: right;\n&#125;\n</code></pre>\n<p></style></p>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_len</th>\n      <th>sepal_wid</th>\n      <th>petal_len</th>\n      <th>petal_wid</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 拆分特征和标签</span></span><br><span class=\"line\"></span><br><span class=\"line\">X = df.iloc[:,<span class=\"number\">0</span>:<span class=\"number\">4</span>].values</span><br><span class=\"line\">y = df.iloc[:,<span class=\"number\">4</span>].values</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h1><h2 id=\"特征展示\"><a href=\"#特征展示\" class=\"headerlink\" title=\"特征展示\"></a>特征展示</h2><p>查看特征的取值范围以及其和类别的关系。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 导入matplotlib库中的pyplot模块，用于绘图</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> matplotlib <span class=\"keyword\">import</span> pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"comment\"># 导入math库</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的种类名称</span></span><br><span class=\"line\">label_dict = &#123;<span class=\"number\">1</span>: <span class=\"string\">&#x27;Iris-Setosa&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">2</span>: <span class=\"string\">&#x27;Iris-Versicolor&#x27;</span>,</span><br><span class=\"line\">              <span class=\"number\">3</span>: <span class=\"string\">&#x27;Iris-Virgnica&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 定义一个字典，映射数字到鸢尾花的特征名称</span></span><br><span class=\"line\">feature_dict = &#123;<span class=\"number\">0</span>: <span class=\"string\">&#x27;sepal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">1</span>: <span class=\"string\">&#x27;sepal width [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">2</span>: <span class=\"string\">&#x27;petal length [cm]&#x27;</span>,</span><br><span class=\"line\">                <span class=\"number\">3</span>: <span class=\"string\">&#x27;petal width [cm]&#x27;</span>&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 创建一个8x6英寸的图像</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"><span class=\"comment\"># 循环绘制4个子图，每个子图代表一个特征的直方图</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> cnt <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">4</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 在图像中创建2x2的子图，并指定当前子图的位置</span></span><br><span class=\"line\">    plt.subplot(<span class=\"number\">2</span>, <span class=\"number\">2</span>, cnt+<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"comment\"># 遍历每种鸢尾花类别，绘制直方图</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> lab <span class=\"keyword\">in</span> (<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>):</span><br><span class=\"line\">        <span class=\"comment\"># 绘制指定特征的直方图，使用不同颜色和透明度区分不同类别</span></span><br><span class=\"line\">        plt.hist(X[y==lab, cnt],</span><br><span class=\"line\">                 label=lab,</span><br><span class=\"line\">                 bins=<span class=\"number\">10</span>,</span><br><span class=\"line\">                 alpha=<span class=\"number\">0.3</span>,)</span><br><span class=\"line\">    <span class=\"comment\"># 设置子图的x轴标签</span></span><br><span class=\"line\">    plt.xlabel(feature_dict[cnt])</span><br><span class=\"line\">    <span class=\"comment\"># 添加图例，用于标识不同类别的鸢尾花</span></span><br><span class=\"line\">    plt.legend(loc=<span class=\"string\">&#x27;upper right&#x27;</span>, fancybox=<span class=\"literal\">True</span>, fontsize=<span class=\"number\">8</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 调整子图间的间距，使布局更加紧凑</span></span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\"><span class=\"comment\"># 显示绘制的图像</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_5_0.png\" alt=\"png\"></p>\n<ul>\n<li>发现<code>花萼的长度</code>和<code>宽度</code>能够较好的区分3种类别。</li>\n<li>四个特征取值范围差异较大，需要进行标准化。</li>\n</ul>\n<h2 id=\"数据标准化\"><a href=\"#数据标准化\" class=\"headerlink\" title=\"数据标准化\"></a>数据标准化</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 进行数据标准化</span></span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\">X_std = StandardScaler().fit_transform(X)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (X_std)</span><br></pre></td></tr></table></figure>\n\n<pre><code>[[-1.1483555  -0.11805969 -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.41098555 -1.32506301]\n [-1.51163569  0.11339944 -1.29694332 -1.32506301]\n [-1.02726211  1.27069504 -1.35396443 -1.32506301]\n [-0.54288852  1.9650724  -1.18290109 -1.0614657 ]\n [-1.51163569  0.8077768  -1.35396443 -1.19326436]\n [-1.02726211  0.8077768  -1.29694332 -1.32506301]\n [-1.75382249 -0.34951881 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-0.54288852  1.50215416 -1.29694332 -1.32506301]\n [-1.2694489   0.8077768  -1.23992221 -1.32506301]\n [-1.2694489  -0.11805969 -1.35396443 -1.45686167]\n [-1.87491588 -0.11805969 -1.52502777 -1.45686167]\n [-0.05851493  2.19653152 -1.46800666 -1.32506301]\n [-0.17960833  3.122368   -1.29694332 -1.0614657 ]\n [-0.54288852  1.9650724  -1.41098555 -1.0614657 ]\n [-0.90616871  1.03923592 -1.35396443 -1.19326436]\n [-0.17960833  1.73361328 -1.18290109 -1.19326436]\n [-0.90616871  1.73361328 -1.29694332 -1.19326436]\n [-0.54288852  0.8077768  -1.18290109 -1.32506301]\n [-0.90616871  1.50215416 -1.29694332 -1.0614657 ]\n [-1.51163569  1.27069504 -1.58204889 -1.32506301]\n [-0.90616871  0.57631768 -1.18290109 -0.92966704]\n [-1.2694489   0.8077768  -1.06885886 -1.32506301]\n [-1.02726211 -0.11805969 -1.23992221 -1.32506301]\n [-1.02726211  0.8077768  -1.23992221 -1.0614657 ]\n [-0.78507531  1.03923592 -1.29694332 -1.32506301]\n [-0.78507531  0.8077768  -1.35396443 -1.32506301]\n [-1.3905423   0.34485856 -1.23992221 -1.32506301]\n [-1.2694489   0.11339944 -1.23992221 -1.32506301]\n [-0.54288852  0.8077768  -1.29694332 -1.0614657 ]\n [-0.78507531  2.42799064 -1.29694332 -1.45686167]\n [-0.42179512  2.65944976 -1.35396443 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.02726211  0.34485856 -1.46800666 -1.32506301]\n [-0.42179512  1.03923592 -1.41098555 -1.32506301]\n [-1.1483555   0.11339944 -1.29694332 -1.45686167]\n [-1.75382249 -0.11805969 -1.41098555 -1.32506301]\n [-0.90616871  0.8077768  -1.29694332 -1.32506301]\n [-1.02726211  1.03923592 -1.41098555 -1.19326436]\n [-1.63272909 -1.73827353 -1.41098555 -1.19326436]\n [-1.75382249  0.34485856 -1.41098555 -1.32506301]\n [-1.02726211  1.03923592 -1.23992221 -0.79786838]\n [-0.90616871  1.73361328 -1.06885886 -1.0614657 ]\n [-1.2694489  -0.11805969 -1.35396443 -1.19326436]\n [-0.90616871  1.73361328 -1.23992221 -1.32506301]\n [-1.51163569  0.34485856 -1.35396443 -1.32506301]\n [-0.66398191  1.50215416 -1.29694332 -1.32506301]\n [-1.02726211  0.57631768 -1.35396443 -1.32506301]\n [ 1.39460583  0.34485856  0.52773232  0.25652088]\n [ 0.66804545  0.34485856  0.41369009  0.38831953]\n [ 1.27351244  0.11339944  0.64177455  0.38831953]\n [-0.42179512 -1.73827353  0.12858453  0.12472222]\n [ 0.78913885 -0.58097793  0.47071121  0.38831953]\n [-0.17960833 -0.58097793  0.41369009  0.12472222]\n [ 0.54695205  0.57631768  0.52773232  0.52011819]\n [-1.1483555  -1.50681441 -0.27056327 -0.27067375]\n [ 0.91023225 -0.34951881  0.47071121  0.12472222]\n [-0.78507531 -0.81243705  0.07156341  0.25652088]\n [-1.02726211 -2.43265089 -0.15652104 -0.27067375]\n [ 0.06257847 -0.11805969  0.24262675  0.38831953]\n [ 0.18367186 -1.96973265  0.12858453 -0.27067375]\n [ 0.30476526 -0.34951881  0.52773232  0.25652088]\n [-0.30070172 -0.34951881 -0.09949993  0.12472222]\n [ 1.03132564  0.11339944  0.35666898  0.25652088]\n [-0.30070172 -0.11805969  0.41369009  0.38831953]\n [-0.05851493 -0.81243705  0.18560564 -0.27067375]\n [ 0.42585866 -1.96973265  0.41369009  0.38831953]\n [-0.30070172 -1.27535529  0.07156341 -0.1388751 ]\n [ 0.06257847  0.34485856  0.58475344  0.78371551]\n [ 0.30476526 -0.58097793  0.12858453  0.12472222]\n [ 0.54695205 -1.27535529  0.64177455  0.38831953]\n [ 0.30476526 -0.58097793  0.52773232 -0.00707644]\n [ 0.66804545 -0.34951881  0.29964787  0.12472222]\n [ 0.91023225 -0.11805969  0.35666898  0.25652088]\n [ 1.15241904 -0.58097793  0.58475344  0.25652088]\n [ 1.03132564 -0.11805969  0.69879566  0.65191685]\n [ 0.18367186 -0.34951881  0.41369009  0.38831953]\n [-0.17960833 -1.04389617 -0.15652104 -0.27067375]\n [-0.42179512 -1.50681441  0.0145423  -0.1388751 ]\n [-0.42179512 -1.50681441 -0.04247882 -0.27067375]\n [-0.05851493 -0.81243705  0.07156341 -0.00707644]\n [ 0.18367186 -0.81243705  0.75581678  0.52011819]\n [-0.54288852 -0.11805969  0.41369009  0.38831953]\n [ 0.18367186  0.8077768   0.41369009  0.52011819]\n [ 1.03132564  0.11339944  0.52773232  0.38831953]\n [ 0.54695205 -1.73827353  0.35666898  0.12472222]\n [-0.30070172 -0.11805969  0.18560564  0.12472222]\n [-0.42179512 -1.27535529  0.12858453  0.12472222]\n [-0.42179512 -1.04389617  0.35666898 -0.00707644]\n [ 0.30476526 -0.11805969  0.47071121  0.25652088]\n [-0.05851493 -1.04389617  0.12858453 -0.00707644]\n [-1.02726211 -1.73827353 -0.27056327 -0.27067375]\n [-0.30070172 -0.81243705  0.24262675  0.12472222]\n [-0.17960833 -0.11805969  0.24262675 -0.00707644]\n [-0.17960833 -0.34951881  0.24262675  0.12472222]\n [ 0.42585866 -0.34951881  0.29964787  0.12472222]\n [-0.90616871 -1.27535529 -0.44162661 -0.1388751 ]\n [-0.17960833 -0.58097793  0.18560564  0.12472222]\n [ 0.54695205  0.57631768  1.2690068   1.70630611]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.51569923 -0.11805969  1.21198569  1.17911148]\n [ 0.54695205 -0.34951881  1.04092235  0.78371551]\n [ 0.78913885 -0.11805969  1.15496457  1.31091014]\n [ 2.12116622 -0.11805969  1.61113348  1.17911148]\n [-1.1483555  -1.27535529  0.41369009  0.65191685]\n [ 1.75788602 -0.34951881  1.44007014  0.78371551]\n [ 1.03132564 -1.27535529  1.15496457  0.78371551]\n [ 1.63679263  1.27069504  1.32602791  1.70630611]\n [ 0.78913885  0.34485856  0.75581678  1.04731282]\n [ 0.66804545 -0.81243705  0.869859    0.91551417]\n [ 1.15241904 -0.11805969  0.98390123  1.17911148]\n [-0.17960833 -1.27535529  0.69879566  1.04731282]\n [-0.05851493 -0.58097793  0.75581678  1.57450745]\n [ 0.66804545  0.34485856  0.869859    1.4427088 ]\n [ 0.78913885 -0.11805969  0.98390123  0.78371551]\n [ 2.24225961  1.73361328  1.6681546   1.31091014]\n [ 2.24225961 -1.04389617  1.78219682  1.4427088 ]\n [ 0.18367186 -1.96973265  0.69879566  0.38831953]\n [ 1.27351244  0.34485856  1.09794346  1.4427088 ]\n [-0.30070172 -0.58097793  0.64177455  1.04731282]\n [ 2.24225961 -0.58097793  1.6681546   1.04731282]\n [ 0.54695205 -0.81243705  0.64177455  0.78371551]\n [ 1.03132564  0.57631768  1.09794346  1.17911148]\n [ 1.63679263  0.34485856  1.2690068   0.78371551]\n [ 0.42585866 -0.58097793  0.58475344  0.78371551]\n [ 0.30476526 -0.11805969  0.64177455  0.78371551]\n [ 0.66804545 -0.58097793  1.04092235  1.17911148]\n [ 1.63679263 -0.11805969  1.15496457  0.52011819]\n [ 1.87897942 -0.58097793  1.32602791  0.91551417]\n [ 2.48444641  1.73361328  1.49709126  1.04731282]\n [ 0.66804545 -0.58097793  1.04092235  1.31091014]\n [ 0.54695205 -0.58097793  0.75581678  0.38831953]\n [ 0.30476526 -1.04389617  1.04092235  0.25652088]\n [ 2.24225961 -0.11805969  1.32602791  1.4427088 ]\n [ 0.54695205  0.8077768   1.04092235  1.57450745]\n [ 0.66804545  0.11339944  0.98390123  0.78371551]\n [ 0.18367186 -0.11805969  0.58475344  0.78371551]\n [ 1.27351244  0.11339944  0.92688012  1.17911148]\n [ 1.03132564  0.11339944  1.04092235  1.57450745]\n [ 1.27351244  0.11339944  0.75581678  1.4427088 ]\n [-0.05851493 -0.81243705  0.75581678  0.91551417]\n [ 1.15241904  0.34485856  1.21198569  1.4427088 ]\n [ 1.03132564  0.57631768  1.09794346  1.70630611]\n [ 1.03132564 -0.11805969  0.81283789  1.4427088 ]\n [ 0.54695205 -1.27535529  0.69879566  0.91551417]\n [ 0.78913885 -0.11805969  0.81283789  1.04731282]\n [ 0.42585866  0.8077768   0.92688012  1.4427088 ]\n [ 0.06257847 -0.11805969  0.75581678  0.78371551]]\n</code></pre>\n<h1 id=\"确定特征向量的数量\"><a href=\"#确定特征向量的数量\" class=\"headerlink\" title=\"确定特征向量的数量\"></a>确定特征向量的数量</h1><h2 id=\"协方差矩阵\"><a href=\"#协方差矩阵\" class=\"headerlink\" title=\"协方差矩阵\"></a>协方差矩阵</h2><p>协方差是衡量两个变量总体误差的期望，用于描述两个变量之间的线性关系程度和方向。两个特征间的协方差值越大，表明其相关性越强。</p>\n<p>对于两个随机变量$X$和$Y$，其协方差$Cov(X,Y)$的计算公式为：</p>\n<p>$$Cov(X,Y) &#x3D; E[(X - E[X])(Y - E[Y])]$$</p>\n<p>其中，$E[X]$和$E[Y]$分别是$X$和$Y$的期望值（即均值）。</p>\n<p>在实际应用中，当我们只有样本数据时，我们通常使用样本协方差来估计总体协方差。对于包含$n$个样本点的数据集，样本协方差$s_{xy}$的计算公式为：</p>\n<p>$$s_{xy} &#x3D; \\frac{1}{n-1} \\sum_{i&#x3D;1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$$</p>\n<p>其中，$x_i$和$y_i$是样本点，$\\bar{x}$和$\\bar{y}$分别是$X$和$Y$的样本均值，计算公式为：</p>\n<p>$$\\bar{x} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} x_i$$</p>\n<p>$$\\bar{y} &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} y_i$$</p>\n<p>注意，在计算样本协方差时，分母是$n-1$而不是$n$，这是为了得到总体协方差的无偏估计。</p>\n<p><strong>以下命令二选一</strong>：</p>\n<ul>\n<li>构造函数计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算特征向量的均值，用于中心化数据</span></span><br><span class=\"line\">mean_vec = np.mean(X_std, axis=<span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算数据的协方差矩阵，用于理解特征之间的线性关系</span></span><br><span class=\"line\"><span class=\"comment\"># (X_std - mean_vec).T.dot((X_std - mean_vec) )计算的是(X_std - mean_vec)的矩阵乘法</span></span><br><span class=\"line\"><span class=\"comment\"># 除以(X_std.shape[0]-1)是为了获得一个无偏估计</span></span><br><span class=\"line\">cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[<span class=\"number\">0</span>]-<span class=\"number\">1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出协方差矩阵，以便于后续分析和使用</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Covariance matrix \\n%s&#x27;</span> %cov_mat)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Covariance matrix \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<ul>\n<li>使用numpy计算协方差矩阵</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;NumPy covariance matrix: \\n%s&#x27;</span> %np.cov(X_std.T))</span><br></pre></td></tr></table></figure>\n\n<pre><code>NumPy covariance matrix: \n[[ 1.00675676 -0.10448539  0.87716999  0.82249094]\n [-0.10448539  1.00675676 -0.41802325 -0.35310295]\n [ 0.87716999 -0.41802325  1.00675676  0.96881642]\n [ 0.82249094 -0.35310295  0.96881642  1.00675676]]\n</code></pre>\n<p>求协方差矩阵的特征向量和特征值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 求协方差矩阵</span></span><br><span class=\"line\">cov_mat = np.cov(X_std.T)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 求协方差矩阵的特征向量，返回特征向量（eig_vecs）和特征值（eig_vals）</span></span><br><span class=\"line\">eig_vals, eig_vecs = np.linalg.eig(cov_mat)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvectors \\n%s&#x27;</span> %eig_vecs)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;\\nEigenvalues \\n%s&#x27;</span> %eig_vals)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Eigenvectors \n[[ 0.52308496 -0.36956962 -0.72154279  0.26301409]\n [-0.25956935 -0.92681168  0.2411952  -0.12437342]\n [ 0.58184289 -0.01912775  0.13962963 -0.80099722]\n [ 0.56609604 -0.06381646  0.63380158  0.52321917]]\n\nEigenvalues \n[2.92442837 0.93215233 0.14946373 0.02098259]\n</code></pre>\n<p>肉眼观察<code>Eigenvalues</code>，应当取前2个特征值对应的特征向量，因为其最重要。</p>\n<p>创建一个包含特征值和对应特征向量的元组列表。按特征值降序排序这些元组。打印排序后的特征值，确认排序正确。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># Make a list of (eigenvalue, eigenvector) tuples</span></span><br><span class=\"line\">eig_pairs = [(np.<span class=\"built_in\">abs</span>(eig_vals[i]), eig_vecs[:,i]) <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"built_in\">len</span>(eig_vals))]</span><br><span class=\"line\"><span class=\"built_in\">print</span> (eig_pairs)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"comment\"># Sort the (eigenvalue, eigenvector) tuples from high to low</span></span><br><span class=\"line\">eig_pairs.sort(key=<span class=\"keyword\">lambda</span> x: x[<span class=\"number\">0</span>], reverse=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Visually confirm that the list is correctly sorted by decreasing eigenvalues</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Eigenvalues in descending order:&#x27;</span>)</span><br><span class=\"line\"><span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> eig_pairs:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(i[<span class=\"number\">0</span>])</span><br></pre></td></tr></table></figure>\n\n<pre><code>[(np.float64(2.9244283691111117), array([ 0.52308496, -0.25956935,  0.58184289,  0.56609604])), (np.float64(0.9321523302535062), array([-0.36956962, -0.92681168, -0.01912775, -0.06381646])), (np.float64(0.14946373489813364), array([-0.72154279,  0.2411952 ,  0.13962963,  0.63380158])), (np.float64(0.020982592764271016), array([ 0.26301409, -0.12437342, -0.80099722,  0.52321917]))]\n----------\nEigenvalues in descending order:\n2.9244283691111117\n0.9321523302535062\n0.14946373489813364\n0.020982592764271016\n</code></pre>\n<p>计算累计变异解释百分比，以评估前几个主成分能解释的总变异比例。最后输出累计变异解释百分比。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 计算所有特征值的总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(eig_vals)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步的目的是确定每个特征值在总变异中所占的百分比</span></span><br><span class=\"line\">var_exp = [(i / tot)*<span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(eig_vals, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 计算累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"comment\"># 这一步是为了了解前几个主成分可以解释多少总变异</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp</span><br></pre></td></tr></table></figure>\n\n<pre><code>[np.float64(72.62003332692032), np.float64(23.147406858644143), np.float64(3.711515564584531), np.float64(0.5210442498510259)]\n\n\n\n\n\narray([ 72.62003333,  95.76744019,  99.47895575, 100.        ])\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 演示np.cumsum() 函数的用法</span></span><br><span class=\"line\">a = np.array([<span class=\"number\">1</span>,<span class=\"number\">2</span>,<span class=\"number\">3</span>,<span class=\"number\">4</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span> (a)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (<span class=\"string\">&#x27;-----------&#x27;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span> (np.cumsum(a)) <span class=\"comment\"># 元素的累加和</span></span><br></pre></td></tr></table></figure>\n\n<pre><code>[1 2 3 4]\n-----------\n[ 1  3  6 10]\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 绘制特征值的重要性</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"></span><br><span class=\"line\">plt.bar(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), var_exp, alpha=<span class=\"number\">0.5</span>, align=<span class=\"string\">&#x27;center&#x27;</span>,</span><br><span class=\"line\">            label=<span class=\"string\">&#x27;individual explained variance&#x27;</span>)</span><br><span class=\"line\">plt.step(<span class=\"built_in\">range</span>(<span class=\"number\">4</span>), cum_var_exp, where=<span class=\"string\">&#x27;mid&#x27;</span>,</span><br><span class=\"line\">             label=<span class=\"string\">&#x27;cumulative explained variance&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Explained variance ratio&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal components&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_20_0.png\" alt=\"png\"></p>\n<p>根据上图可以决定取几个特征值进行降维，此例中前两个特征值已经足够（~95.77%）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 根据前两个主要成分构造转换矩阵W</span></span><br><span class=\"line\"><span class=\"comment\"># 选择主要成分分析（PCA）得到的前两个特征向量</span></span><br><span class=\"line\"><span class=\"comment\"># 将特征向量重塑为4x1的矩阵形式，以便进行矩阵操作</span></span><br><span class=\"line\"><span class=\"comment\"># 这两个特征向量代表了数据中方差最大的两个方向</span></span><br><span class=\"line\">matrix_w = np.hstack((eig_pairs[<span class=\"number\">0</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>),</span><br><span class=\"line\">                      eig_pairs[<span class=\"number\">1</span>][<span class=\"number\">1</span>].reshape(<span class=\"number\">4</span>,<span class=\"number\">1</span>)))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 打印转换矩阵W，以便于查看和验证结果</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Matrix W:\\n&#x27;</span>, matrix_w)</span><br></pre></td></tr></table></figure>\n\n<pre><code>Matrix W:\n [[ 0.52308496 -0.36956962]\n [-0.25956935 -0.92681168]\n [ 0.58184289 -0.01912775]\n [ 0.56609604 -0.06381646]]\n</code></pre>\n<h1 id=\"矩阵降维\"><a href=\"#矩阵降维\" class=\"headerlink\" title=\"矩阵降维\"></a>矩阵降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 标准化数据X_std与权重矩阵matrix_w进行点积运算，用于特征提取和数据转换</span></span><br><span class=\"line\">Y = X_std.dot(matrix_w)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 输出转换后的数据Y</span></span><br><span class=\"line\">Y</span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>array([[-2.10795032,  0.64427554],\n       [-2.38797131,  0.30583307],\n       [-2.32487909,  0.56292316],\n       [-2.40508635, -0.687591  ],\n       [-2.08320351, -1.53025171],\n       [-2.4636848 , -0.08795413],\n       [-2.25174963, -0.25964365],\n       [-2.3645813 ,  1.08255676],\n       [-2.20946338,  0.43707676],\n       [-2.17862017, -1.08221046],\n       [-2.34525657, -0.17122946],\n       [-2.24590315,  0.6974389 ],\n       [-2.66214582,  0.92447316],\n       [-2.2050227 , -1.90150522],\n       [-2.25993023, -2.73492274],\n       [-2.21591283, -1.52588897],\n       [-2.20705382, -0.52623535],\n       [-1.9077081 , -1.4415791 ],\n       [-2.35411558, -1.17088308],\n       [-1.93202643, -0.44083479],\n       [-2.21942518, -0.96477499],\n       [-2.79116421, -0.50421849],\n       [-1.83814105, -0.11729122],\n       [-2.24572458, -0.17450151],\n       [-1.97825353,  0.59734172],\n       [-2.06935091, -0.27755619],\n       [-2.18514506, -0.56366755],\n       [-2.15824269, -0.34805785],\n       [-2.28843932,  0.30256102],\n       [-2.16501749,  0.47232759],\n       [-1.8491597 , -0.45547527],\n       [-2.62023392, -1.84237072],\n       [-2.44885384, -2.1984673 ],\n       [-2.20946338,  0.43707676],\n       [-2.23112223,  0.17266644],\n       [-2.06147331, -0.6957435 ],\n       [-2.20946338,  0.43707676],\n       [-2.45783833,  0.86912843],\n       [-2.1884075 , -0.30439609],\n       [-2.30357329, -0.48039222],\n       [-1.89932763,  2.31759817],\n       [-2.57799771,  0.4400904 ],\n       [-1.98020921, -0.50889705],\n       [-2.14679556, -1.18365675],\n       [-2.09668176,  0.68061705],\n       [-2.39554894, -1.16356284],\n       [-2.41813611,  0.34949483],\n       [-2.24196231, -1.03745802],\n       [-2.22484727, -0.04403395],\n       [ 1.09225538, -0.86148748],\n       [ 0.72045861, -0.59920238],\n       [ 1.2299583 , -0.61280832],\n       [ 0.37598859,  1.756516  ],\n       [ 1.05729685,  0.21303055],\n       [ 0.36816104,  0.58896262],\n       [ 0.73800214, -0.77956125],\n       [-0.52021731,  1.84337921],\n       [ 0.9113379 , -0.02941906],\n       [-0.01292322,  1.02537703],\n       [-0.15020174,  2.65452146],\n       [ 0.42437533,  0.05686991],\n       [ 0.52894687,  1.77250558],\n       [ 0.70241525,  0.18484154],\n       [-0.05385675,  0.42901221],\n       [ 0.86277668, -0.50943908],\n       [ 0.33388091,  0.18785518],\n       [ 0.13504146,  0.7883247 ],\n       [ 1.19457128,  1.63549265],\n       [ 0.13677262,  1.30063807],\n       [ 0.72711201, -0.40394501],\n       [ 0.45564294,  0.41540628],\n       [ 1.21038365,  0.94282042],\n       [ 0.61327355,  0.4161824 ],\n       [ 0.68512164,  0.06335788],\n       [ 0.85951424, -0.25016762],\n       [ 1.23906722,  0.08500278],\n       [ 1.34575245, -0.32669695],\n       [ 0.64732915,  0.22336443],\n       [-0.06728496,  1.05414028],\n       [ 0.10033285,  1.56100021],\n       [-0.00745518,  1.57050182],\n       [ 0.2179082 ,  0.77368423],\n       [ 1.04116321,  0.63744742],\n       [ 0.20719664,  0.27736006],\n       [ 0.42154138, -0.85764157],\n       [ 1.03691937, -0.52112206],\n       [ 1.015435  ,  1.39413373],\n       [ 0.0519502 ,  0.20903977],\n       [ 0.25582921,  1.32747797],\n       [ 0.25384813,  1.11700714],\n       [ 0.60915822, -0.02858679],\n       [ 0.31116522,  0.98711256],\n       [-0.39679548,  2.01314578],\n       [ 0.26536661,  0.85150613],\n       [ 0.07385897,  0.17160757],\n       [ 0.20854936,  0.37771566],\n       [ 0.55843737,  0.15286277],\n       [-0.47853403,  1.53421644],\n       [ 0.23545172,  0.59332536],\n       [ 1.8408037 , -0.86943848],\n       [ 1.13831104,  0.70171953],\n       [ 2.19615974, -0.54916658],\n       [ 1.42613827,  0.05187679],\n       [ 1.8575403 , -0.28797217],\n       [ 2.74511173, -0.78056359],\n       [ 0.34010583,  1.5568955 ],\n       [ 2.29180093, -0.40328242],\n       [ 1.98618025,  0.72876171],\n       [ 2.26382116, -1.91685818],\n       [ 1.35591821, -0.69255356],\n       [ 1.58471851,  0.43102351],\n       [ 1.87342402, -0.41054652],\n       [ 1.23656166,  1.16818977],\n       [ 1.45128483,  0.4451459 ],\n       [ 1.58276283, -0.67521526],\n       [ 1.45956552, -0.25105642],\n       [ 2.43560434, -2.55096977],\n       [ 3.29752602,  0.01266612],\n       [ 1.23377366,  1.71954411],\n       [ 2.03218282, -0.90334021],\n       [ 0.95980311,  0.57047585],\n       [ 2.88717988, -0.38895776],\n       [ 1.31405636,  0.48854962],\n       [ 1.69619746, -1.01153249],\n       [ 1.94868773, -0.99881497],\n       [ 1.1574572 ,  0.31987373],\n       [ 1.007133  , -0.06550254],\n       [ 1.7733922 ,  0.19641059],\n       [ 1.85327106, -0.55077372],\n       [ 2.4234788 , -0.2397454 ],\n       [ 2.31353522, -2.62038074],\n       [ 1.84800289,  0.18799967],\n       [ 1.09649923,  0.29708201],\n       [ 1.1812503 ,  0.81858241],\n       [ 2.79178861, -0.83668445],\n       [ 1.57340399, -1.07118383],\n       [ 1.33614369, -0.420823  ],\n       [ 0.91061354, -0.01965942],\n       [ 1.84350913, -0.66872729],\n       [ 2.00701161, -0.60663655],\n       [ 1.89319854, -0.68227708],\n       [ 1.13831104,  0.70171953],\n       [ 2.03519535, -0.86076914],\n       [ 1.99464025, -1.04517619],\n       [ 1.85977129, -0.37934387],\n       [ 1.54200377,  0.90808604],\n       [ 1.50925493, -0.26460621],\n       [ 1.3690965 , -1.01583909],\n       [ 0.94680339,  0.02182097]])\n</code></pre>\n<p>查看降维前的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(X[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                X[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;sepal_len&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;sepal_wid&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;best&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_26_0.png\" alt=\"png\"></p>\n<p>查看降维后的区分表现。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">plt.figure(figsize=(<span class=\"number\">6</span>, <span class=\"number\">4</span>))</span><br><span class=\"line\"><span class=\"keyword\">for</span> lab, col <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>((<span class=\"string\">&#x27;Iris-setosa&#x27;</span>, <span class=\"string\">&#x27;Iris-versicolor&#x27;</span>, <span class=\"string\">&#x27;Iris-virginica&#x27;</span>),</span><br><span class=\"line\">                        (<span class=\"string\">&#x27;blue&#x27;</span>, <span class=\"string\">&#x27;red&#x27;</span>, <span class=\"string\">&#x27;green&#x27;</span>)):</span><br><span class=\"line\">     plt.scatter(Y[y==lab, <span class=\"number\">0</span>],</span><br><span class=\"line\">                Y[y==lab, <span class=\"number\">1</span>],</span><br><span class=\"line\">                label=lab,</span><br><span class=\"line\">                c=col)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend(loc=<span class=\"string\">&#x27;lower center&#x27;</span>)</span><br><span class=\"line\">plt.tight_layout()</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_28_0.png\" alt=\"png\"></p>\n<h1 id=\"利用SK-LEARN-PCA进行降维\"><a href=\"#利用SK-LEARN-PCA进行降维\" class=\"headerlink\" title=\"利用SK-LEARN PCA进行降维\"></a>利用SK-LEARN PCA进行降维</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> matplotlib.pyplot <span class=\"keyword\">as</span> plt</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn <span class=\"keyword\">import</span> datasets</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.preprocessing <span class=\"keyword\">import</span> StandardScaler</span><br><span class=\"line\"><span class=\"keyword\">from</span> sklearn.decomposition <span class=\"keyword\">import</span> PCA</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 1. 加载鸢尾花数据集</span></span><br><span class=\"line\">iris = datasets.load_iris()</span><br><span class=\"line\">X = iris.data</span><br><span class=\"line\">y = iris.target</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 2. 数据标准化</span></span><br><span class=\"line\">scaler = StandardScaler()</span><br><span class=\"line\">X_std = scaler.fit_transform(X)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 3. 应用PCA降维</span></span><br><span class=\"line\">pca = PCA()</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 计算特征值总和</span></span><br><span class=\"line\">tot = <span class=\"built_in\">sum</span>(pca.explained_variance_)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 5. 计算每个特征值占总和的百分比，并按降序排序</span></span><br><span class=\"line\">var_exp = [(i / tot) * <span class=\"number\">100</span> <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(pca.explained_variance_, reverse=<span class=\"literal\">True</span>)]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 6. 输出每个特征值的变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;变异解释百分比:&quot;</span>, var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 7. 计算累计变异解释百分比</span></span><br><span class=\"line\">cum_var_exp = np.cumsum(var_exp)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 8. 输出累计变异解释百分比</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;累计变异解释百分比:&quot;</span>, cum_var_exp)</span><br></pre></td></tr></table></figure>\n\n<pre><code>变异解释百分比: [np.float64(72.96244541329987), np.float64(22.850761786701742), np.float64(3.668921889282886), np.float64(0.5178709107154782)]\n累计变异解释百分比: [ 72.96244541  95.8132072   99.48212909 100.        ]\n</code></pre>\n<p>由上述结果同样可见，前两个特征向量累计贡献的方差为95.81%，故取前2个特征向量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 应用PCA降维到2个主成分</span></span><br><span class=\"line\">pca = PCA(n_components=<span class=\"number\">2</span>)</span><br><span class=\"line\">X_pca = pca.fit_transform(X_std)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 4. 绘制PCA图</span></span><br><span class=\"line\">plt.figure(figsize=(<span class=\"number\">8</span>, <span class=\"number\">6</span>))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 为不同的类别绘制不同颜色的点</span></span><br><span class=\"line\">colors = [<span class=\"string\">&#x27;r&#x27;</span>, <span class=\"string\">&#x27;g&#x27;</span>, <span class=\"string\">&#x27;b&#x27;</span>]</span><br><span class=\"line\">markers = [<span class=\"string\">&#x27;s&#x27;</span>, <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;o&#x27;</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> idx, color, marker <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(<span class=\"built_in\">range</span>(<span class=\"number\">3</span>), colors, markers):</span><br><span class=\"line\">    plt.scatter(x=X_pca[y == idx, <span class=\"number\">0</span>], </span><br><span class=\"line\">                y=X_pca[y == idx, <span class=\"number\">1</span>],</span><br><span class=\"line\">                c=color, </span><br><span class=\"line\">                marker=marker,</span><br><span class=\"line\">                label=iris.target_names[idx])</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 添加标题和标签</span></span><br><span class=\"line\">plt.title(<span class=\"string\">&#x27;PCA of Iris Dataset&#x27;</span>)</span><br><span class=\"line\">plt.xlabel(<span class=\"string\">&#x27;Principal Component 1&#x27;</span>)</span><br><span class=\"line\">plt.ylabel(<span class=\"string\">&#x27;Principal Component 2&#x27;</span>)</span><br><span class=\"line\">plt.legend()</span><br><span class=\"line\">plt.grid(<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 显示图形</span></span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/PCA_files/PCA_32_0.png\" alt=\"png\"></p>\n<h1 id=\"加关注\"><a href=\"#加关注\" class=\"headerlink\" title=\"加关注\"></a>加关注</h1><p>关注公众号“生信之巅”，获取更多教程。</p>\n<table align=\"center\"><tr>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/生信之巅公众号.jpg\" alt=\"生信之巅微信公众号\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-right: 0px;margin-bottom: 5px;align: center;\"></td>\n  <td align=\"center\"><img src=\"https://cdn.jsdelivr.net/gh/liaochenlanruo/cdn@master/img/social/小程序码.png\" alt=\"生信之巅小程序码\" style=\"width: 100px;height: 100px;vertical-align: -20px;border-radius: 0%;margin-left: 0px;margin-bottom: 5px;align: center\"></td>\n</tr></table>\n\n\n<p><font color=\"#FF0000\"><ruby><b>敬告</b>：使用文中脚本请引用本文网址，请尊重本人的劳动成果，谢谢！<rt><b>Notice</b>: When you use the scripts in this article, please cite the link of this webpage. Thank you!</rt></ruby></font></p>\n","categories":[{"name":"AI","path":"api/categories/AI.json"}],"tags":[{"name":"机器学习","path":"api/tags/机器学习.json"},{"name":"Scikit-learn","path":"api/tags/Scikit-learn.json"}]}